{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1745167480676,
     "user": {
      "displayName": "HAO WANG",
      "userId": "10010733232250767181"
     },
     "user_tz": -480
    },
    "id": "-yTJWFI3hocH",
    "outputId": "465afb7f-27f1-4dab-e639-d90223b1aacd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'QBQTC' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/CLUEbenchmark/QBQTC.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9w_MkQGF8wL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7535,
     "status": "ok",
     "timestamp": 1745167489404,
     "user": {
      "displayName": "HAO WANG",
      "userId": "10010733232250767181"
     },
     "user_tz": -480
    },
    "id": "p8Ige9d-lguR",
    "outputId": "aae858ea-4fb9-4cb4-ea0f-a645853994ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kW8r20yjT1b"
   },
   "outputs": [],
   "source": [
    "dataset_path = '/content/QBQTC/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1745167494996,
     "user": {
      "displayName": "HAO WANG",
      "userId": "10010733232250767181"
     },
     "user_tz": -480
    },
    "id": "JZ5ur93KlUbq",
    "outputId": "591af6f2-51b1-4ce4-a02e-30e9c1d5e332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train.json，include 180000 \n",
      "read dev.json，include 20000 \n",
      "read test.json，include 10000 \n",
      "read test_public.json，include 5000 \n",
      "   id            query                           title label\n",
      "0   0            应届生实习                    实习生招聘-应届生求职网     1\n",
      "1   1  ln1+x-ln1+y=x-y  已知函数fx=1lnx+1-x则y=fx的图像高考吧百度贴吧     0\n",
      "2   2         大秦之悍卒189                   起点中文网阅文集团旗下网站     0\n",
      "3   3             出门经咒                     快快乐乐出门咒-豆丁网     1\n",
      "4   4           盖中盖广告词              谁知道盖中盖所有的广告词急用百度知道     1\n",
      "   id             query                                              title  \\\n",
      "0   0            小孩咳嗽感冒                              小孩感冒过后久咳嗽该吃什么药育儿问答宝宝树   \n",
      "1   1      前列腺癌根治术后能活多久                    前列腺癌转移能活多久前列腺癌治疗方法盘点-家庭医生在线肿瘤频道   \n",
      "2   2          英雄大作战022               英雄大作战v0.65无敌版英雄大作战v0.65无敌版小游戏4399小游戏   \n",
      "3   3  如何将一个文件复制到另一个文件里                           怎么把布局里的图纸复制到另外一个文件中去百度文库   \n",
      "4   4        gilneasart  gilneas-pictures&charactersart-worldofwarcraft...   \n",
      "\n",
      "  label  \n",
      "0     1  \n",
      "1     1  \n",
      "2     1  \n",
      "3     0  \n",
      "4     1  \n",
      "      id                query  \\\n",
      "0  13475  离婚申请之日起任何一方不同意离婚可以向   \n",
      "1  19170       淘宝里面图片轮播怎么上图片啊   \n",
      "2  15378               tpu和pc   \n",
      "3  11256             做完人流小腹刺痛   \n",
      "4   3189        央财金融专硕分数线2021   \n",
      "\n",
      "                                               title label  \n",
      "0  依据《中华人民共和国民法典》规定自婚姻登记机关收到离婚登记申请之日起任何一方不愿意离婚的可以...     1  \n",
      "1                             淘宝店铺里的轮播图片宝贝怎么弄上去啊百度知道     1  \n",
      "2                                 软硬大比拼硅胶tpu和pc材质对比-     1  \n",
      "3                    做完人流后左下腹伴有疼痛是怎么回事即问即答家庭医生在线即问即答     1  \n",
      "4                          中央财经大学2021年硕士研究生招生考试复试分数线     1  \n",
      "   id                        query                   title\n",
      "0   0  chapterobjectivesdefinition       chapterobjectives\n",
      "1   1                        怀孕痣激光  怀孕了可以做激光去痣吗热门健康问答有问必答网\n",
      "2   2                  饱和蒸汽压力温度对照表         饱和蒸汽压力温度对照表-豆丁网\n",
      "3   3                        我想租房子            我想租房子要注意什么事情\n",
      "4   4                     fl贸易方式简写          航运专业术语缩写全集百度文库\n"
     ]
    }
   ],
   "source": [
    "def read_json_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line.strip())\n",
    "                data.append(item)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing line: {line}\")\n",
    "    return data\n",
    "\n",
    "files = ['train.json', 'dev.json', 'test.json', 'test_public.json']\n",
    "data_dict = {}\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        data = read_json_file(file_path)\n",
    "        data_dict[file.split('.')[0]] = data\n",
    "        print(f\"read {file}，include {len(data)} \")\n",
    "\n",
    "train_df = pd.DataFrame(data_dict['train'])\n",
    "dev_df = pd.DataFrame(data_dict['dev'])\n",
    "test_public_df = pd.DataFrame(data_dict['test_public'])\n",
    "test_df = pd.DataFrame(data_dict['test'])\n",
    "print(train_df.head())\n",
    "print(dev_df.head())\n",
    "print(test_public_df.head())\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz9l3wc0oSpw"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    if 'label' in df.columns:\n",
    "        df['label'] = df['label'].astype(int)\n",
    "\n",
    "    df['query'] = df['query'].astype(str)\n",
    "    df['title'] = df['title'].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = preprocess_data(train_df)\n",
    "dev_df = preprocess_data(dev_df)\n",
    "test_public_df = preprocess_data(test_public_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XJ0mpIMOzqs"
   },
   "outputs": [],
   "source": [
    "class TextMatchingDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=256):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.has_label = 'label' in df.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = str(self.df.iloc[idx]['query'])\n",
    "        title = str(self.df.iloc[idx]['title'])\n",
    "\n",
    "        # tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            query,\n",
    "            title,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # [1, seq_len]---->[seq_len]\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        token_type_ids = encoding['token_type_ids'].squeeze(0)\n",
    "\n",
    "        if self.has_label:\n",
    "            label = torch.tensor(self.df.iloc[idx]['label'])\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'token_type_ids': token_type_ids,\n",
    "                'label': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'token_type_ids': token_type_ids,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9CnNbkVO8uz"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # BertForSequenceClassification，the output is an object containing loss and logits\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    return total_loss / len(train_dataloader)\n",
    "\n",
    "# evaluate\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    report = classification_report(true_labels, predictions, digits=4)\n",
    "\n",
    "    return accuracy, f1, report, predictions\n",
    "\n",
    "# predict\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLnzeqgPn1DI"
   },
   "outputs": [],
   "source": [
    "class TransformerMatchingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=8, num_layers=3, num_classes=3, max_length=256):\n",
    "        super(TransformerMatchingModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        word_embeddings = self.embedding(input_ids)\n",
    "        position_embeddings = self.position_embedding(positions)\n",
    "        embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "        # src_key_padding_mask: shape [batch_size, seq_len], where True indicates padding\n",
    "        src_key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "\n",
    "        transformer_output = self.transformer(embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "        cls_output = transformer_output[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fl79sZv0PCY6"
   },
   "outputs": [],
   "source": [
    "def train_transformer_model():\n",
    "    all_text = list(train_df['query']) + list(train_df['title'])\n",
    "    all_text = [str(text) for text in all_text]\n",
    "\n",
    "    all_tokens = []\n",
    "    for text in all_text:\n",
    "        tokens = list(text)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    vocab = {token: idx + 1 for idx, token in enumerate(set(all_tokens))}\n",
    "    vocab['<PAD>'] = 0  # pad\n",
    "    vocab['<UNK>'] = len(vocab)  # unkown\n",
    "\n",
    "    class SimpleTokenizer:\n",
    "        def __init__(self, vocab, max_length=128):\n",
    "            self.vocab = vocab\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __call__(self, query, title, add_special_tokens=True, max_length=None, padding='max_length', truncation=True, return_tensors='pt'):\n",
    "            if max_length is None:\n",
    "                max_length = self.max_length\n",
    "\n",
    "            query_tokens = list(query)\n",
    "            title_tokens = list(title)\n",
    "\n",
    "            query_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in query_tokens]\n",
    "            title_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in title_tokens]\n",
    "\n",
    "            if add_special_tokens:\n",
    "                input_ids = [self.vocab['<UNK>']] + query_ids + [self.vocab['<UNK>']] + title_ids + [self.vocab['<UNK>']]\n",
    "                token_type_ids = [0] * (len(query_ids) + 2) + [1] * (len(title_ids) + 1)\n",
    "            else:\n",
    "                input_ids = query_ids + title_ids\n",
    "                token_type_ids = [0] * len(query_ids) + [1] * len(title_ids)\n",
    "\n",
    "            if truncation and len(input_ids) > max_length:\n",
    "                input_ids = input_ids[:max_length]\n",
    "                token_type_ids = token_type_ids[:max_length]\n",
    "\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            if padding == 'max_length':\n",
    "                pad_length = max_length - len(input_ids)\n",
    "                input_ids = input_ids + [self.vocab['<PAD>']] * pad_length\n",
    "                attention_mask = attention_mask + [0] * pad_length\n",
    "                token_type_ids = token_type_ids + [0] * pad_length\n",
    "\n",
    "            if return_tensors == 'pt':\n",
    "                return {\n",
    "                    'input_ids': torch.tensor([input_ids]),\n",
    "                    'attention_mask': torch.tensor([attention_mask]),\n",
    "                    'token_type_ids': torch.tensor([token_type_ids])\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'token_type_ids': token_type_ids\n",
    "                }\n",
    "\n",
    "    tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "    model = TransformerMatchingModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=128,\n",
    "        num_heads=8,\n",
    "        num_layers=3,\n",
    "        num_classes=3  # 0, 1, 2\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset = TextMatchingDataset(train_df, tokenizer)\n",
    "    dev_dataset = TextMatchingDataset(dev_df, tokenizer)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    epochs = 5\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.1 * total_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        train_loss = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"training loss: {train_loss:.4f}\")\n",
    "\n",
    "        accuracy, f1, report, _ = evaluate(model, dev_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_transformer_model.pt')\n",
    "            print(\"save best model\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_transformer_model.pt'))\n",
    "\n",
    "    test_public_dataset = TextMatchingDataset(test_public_df, tokenizer)\n",
    "    test_public_dataloader = DataLoader(test_public_dataset, batch_size=batch_size)\n",
    "\n",
    "    accuracy, f1, report, _ = evaluate(model, test_public_dataloader, device)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "    print(report)\n",
    "\n",
    "    # predict\n",
    "    # test_dataset = TextMatchingDataset(test_df, tokenizer)\n",
    "    # test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # predictions = predict(model, test_dataloader, device)\n",
    "\n",
    "    # test_df['predicted_label'] = predictions\n",
    "    # test_df[['id', 'predicted_label']].to_csv('transformer_predictions.csv', index=False)\n",
    "    # print(\"result: transformer_predictions.csv\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 600449,
     "status": "ok",
     "timestamp": 1745172268314,
     "user": {
      "displayName": "HAO WANG",
      "userId": "10010733232250767181"
     },
     "user_tz": -480
    },
    "id": "pASaAznulV7p",
    "outputId": "ef631218-beb2-44b1-8ea7-7d26bf5f2075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Transformer model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:50<00:00, 25.52it/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.8529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6457\n",
      "Validation F1 Score: 0.5975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5540    0.2432    0.3380      4894\n",
      "           1     0.6693    0.8945    0.7657     12592\n",
      "           2     0.4498    0.1834    0.2605      2514\n",
      "\n",
      "    accuracy                         0.6457     20000\n",
      "   macro avg     0.5577    0.4403    0.4547     20000\n",
      "weighted avg     0.6135    0.6457    0.5975     20000\n",
      "\n",
      "save best model\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:52<00:00, 24.92it/s, loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.8035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6544\n",
      "Validation F1 Score: 0.5901\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5815    0.2303    0.3299      4894\n",
      "           1     0.6643    0.9299    0.7749     12592\n",
      "           2     0.5793    0.1002    0.1709      2514\n",
      "\n",
      "    accuracy                         0.6544     20000\n",
      "   macro avg     0.6084    0.4201    0.4253     20000\n",
      "weighted avg     0.6333    0.6544    0.5901     20000\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:50<00:00, 25.38it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6579\n",
      "Validation F1 Score: 0.6009\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5783    0.2460    0.3452      4894\n",
      "           1     0.6702    0.9221    0.7762     12592\n",
      "           2     0.5758    0.1360    0.2201      2514\n",
      "\n",
      "    accuracy                         0.6579     20000\n",
      "   macro avg     0.6081    0.4347    0.4472     20000\n",
      "weighted avg     0.6359    0.6579    0.6009     20000\n",
      "\n",
      "save best model\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:53<00:00, 24.88it/s, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6604\n",
      "Validation F1 Score: 0.6255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5624    0.3269    0.4135      4894\n",
      "           1     0.6883    0.8781    0.7717     12592\n",
      "           2     0.5050    0.2192    0.3057      2514\n",
      "\n",
      "    accuracy                         0.6604     20000\n",
      "   macro avg     0.5852    0.4747    0.4970     20000\n",
      "weighted avg     0.6345    0.6604    0.6255     20000\n",
      "\n",
      "save best model\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:49<00:00, 25.69it/s, loss=0.933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6609\n",
      "Validation F1 Score: 0.6268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5642    0.3259    0.4132      4894\n",
      "           1     0.6895    0.8770    0.7720     12592\n",
      "           2     0.5009    0.2303    0.3155      2514\n",
      "\n",
      "    accuracy                         0.6609     20000\n",
      "   macro avg     0.5848    0.4777    0.5002     20000\n",
      "weighted avg     0.6351    0.6609    0.6268     20000\n",
      "\n",
      "save best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:02<00:00, 38.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.6588\n",
      "Test Set F1 Score: 0.6277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5637    0.3366    0.4215      1209\n",
      "           1     0.6912    0.8664    0.7689      3159\n",
      "           2     0.4717    0.2373    0.3158       632\n",
      "\n",
      "    accuracy                         0.6588      5000\n",
      "   macro avg     0.5755    0.4801    0.5021      5000\n",
      "weighted avg     0.6326    0.6588    0.6277      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ntrain Transformer model...\")\n",
    "transformer_model = train_transformer_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3sWldIpuLc0"
   },
   "source": [
    "## bge + transformer\n",
    "用 BGE 得到 [query_tokens] 和 [title_tokens] 的 embedding， 拼接之后送入 Transformer 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQObD7OICphJ"
   },
   "outputs": [],
   "source": [
    "class BGETransformerMatchingModel(nn.Module):\n",
    "    def __init__(self, num_heads=8, num_layers=3, num_classes=3, freeze_bge=True):\n",
    "        super(BGETransformerMatchingModel, self).__init__()\n",
    "\n",
    "        self.bge_model_name = \"BAAI/bge-base-zh-v1.5\"\n",
    "        self.bge_model = AutoModel.from_pretrained(self.bge_model_name)\n",
    "\n",
    "        # (option)\n",
    "        if freeze_bge:\n",
    "            for param in self.bge_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        embed_dim = self.bge_model.config.hidden_size\n",
    "\n",
    "        self.projection = nn.Linear(embed_dim, 128)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=128*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "\n",
    "        bge_outputs = self.bge_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        token_embeddings = bge_outputs.last_hidden_state  # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        projected_embeddings = self.projection(token_embeddings)  # [batch_size, seq_len, 128]\n",
    "\n",
    "        src_key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "\n",
    "        transformer_output = self.transformer(projected_embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        cls_output = transformer_output[:, 0, :]\n",
    "\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n",
    "\n",
    "def train_bge_transformer_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-zh-v1.5\")\n",
    "\n",
    "    model = BGETransformerMatchingModel(\n",
    "        num_heads=8,\n",
    "        num_layers=3,\n",
    "        num_classes=3,  # 0, 1, 2\n",
    "        freeze_bge=True\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset = TextMatchingDataset(train_df, tokenizer)\n",
    "    dev_dataset = TextMatchingDataset(dev_df, tokenizer)\n",
    "\n",
    "    batch_size = 32\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    if model.bge_model.parameters().__next__().requires_grad:\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': model.bge_model.parameters(), 'lr': 1e-5},  # smaller learning rate for bge\n",
    "            {'params': model.projection.parameters()},\n",
    "            {'params': model.transformer.parameters()},\n",
    "            {'params': model.classifier.parameters()}\n",
    "        ], lr=5e-4)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': model.projection.parameters()},\n",
    "            {'params': model.transformer.parameters()},\n",
    "            {'params': model.classifier.parameters()}\n",
    "        ], lr=5e-4)\n",
    "\n",
    "    epochs = 5\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.1 * total_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        train_loss = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"training loss: {train_loss:.4f}\")\n",
    "\n",
    "        accuracy, f1, report, _ = evaluate(model, dev_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_bge_transformer_model.pt')\n",
    "            print(\"save best model\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_bge_transformer_model.pt'))\n",
    "\n",
    "    test_public_dataset = TextMatchingDataset(test_public_df, tokenizer)\n",
    "    test_public_dataloader = DataLoader(test_public_dataset, batch_size=batch_size)\n",
    "\n",
    "    accuracy, f1, report, _ = evaluate(model, test_public_dataloader, device)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "    print(report)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3860332,
     "status": "ok",
     "timestamp": 1745171609372,
     "user": {
      "displayName": "HAO WANG",
      "userId": "10010733232250767181"
     },
     "user_tz": -480
    },
    "id": "HMeWmicZuGLa",
    "outputId": "b12cd1b6-eda1-4263-ac38-3dc07aebe5d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Transformer_bge model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:36<00:00,  8.08it/s, loss=0.621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:13<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6681\n",
      "Validation F1 Score: 0.5845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7288    0.2372    0.3579      4894\n",
      "           1     0.6626    0.9676    0.7866     12592\n",
      "           2     0.8500    0.0068    0.0134      2514\n",
      "\n",
      "    accuracy                         0.6681     20000\n",
      "   macro avg     0.7472    0.4039    0.3860     20000\n",
      "weighted avg     0.7024    0.6681    0.5845     20000\n",
      "\n",
      "save best model\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:35<00:00,  8.09it/s, loss=0.618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:12<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6916\n",
      "Validation F1 Score: 0.6571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6631    0.4142    0.5099      4894\n",
      "           1     0.7041    0.9009    0.7904     12592\n",
      "           2     0.5548    0.1834    0.2756      2514\n",
      "\n",
      "    accuracy                         0.6916     20000\n",
      "   macro avg     0.6406    0.4995    0.5253     20000\n",
      "weighted avg     0.6753    0.6916    0.6571     20000\n",
      "\n",
      "save best model\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:33<00:00,  8.11it/s, loss=0.737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:12<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6890\n",
      "Validation F1 Score: 0.6535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6723    0.3756    0.4819      4894\n",
      "           1     0.7004    0.9064    0.7902     12592\n",
      "           2     0.5443    0.2100    0.3031      2514\n",
      "\n",
      "    accuracy                         0.6890     20000\n",
      "   macro avg     0.6390    0.4973    0.5251     20000\n",
      "weighted avg     0.6739    0.6890    0.6535     20000\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:33<00:00,  8.11it/s, loss=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:13<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6955\n",
      "Validation F1 Score: 0.6668\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7067    0.3796    0.4940      4894\n",
      "           1     0.7077    0.8989    0.7919     12592\n",
      "           2     0.5320    0.2912    0.3763      2514\n",
      "\n",
      "    accuracy                         0.6955     20000\n",
      "   macro avg     0.6488    0.5232    0.5541     20000\n",
      "weighted avg     0.6853    0.6955    0.6668     20000\n",
      "\n",
      "save best model\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:35<00:00,  8.09it/s, loss=0.734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:12<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6979\n",
      "Validation F1 Score: 0.6786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6958    0.4285    0.5303      4894\n",
      "           1     0.7210    0.8722    0.7894     12592\n",
      "           2     0.5017    0.3496    0.4121      2514\n",
      "\n",
      "    accuracy                         0.6979     20000\n",
      "   macro avg     0.6395    0.5501    0.5773     20000\n",
      "weighted avg     0.6872    0.6979    0.6786     20000\n",
      "\n",
      "save best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:18<00:00,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.7000\n",
      "Test Set F1 Score: 0.6812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7020    0.4326    0.5353      1209\n",
      "           1     0.7245    0.8724    0.7916      3159\n",
      "           2     0.4900    0.3497    0.4081       632\n",
      "\n",
      "    accuracy                         0.7000      5000\n",
      "   macro avg     0.6388    0.5516    0.5784      5000\n",
      "weighted avg     0.6894    0.7000    0.6812      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ntrain Transformer_bge model...\")\n",
    "transformer_model = train_bge_transformer_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpBM3KsNuGhh"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPKDYF9BiJsH"
   },
   "source": [
    "bert单塔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFmxrOAWO_VB"
   },
   "outputs": [],
   "source": [
    "def train_bert_model():\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-chinese',\n",
    "        num_labels=3  # 0, 1, 2\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset = TextMatchingDataset(train_df, tokenizer)\n",
    "    dev_dataset = TextMatchingDataset(dev_df, tokenizer)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    epochs = 3\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.1 * total_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        train_loss = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"training loss: {train_loss:.4f}\")\n",
    "\n",
    "        # val\n",
    "        accuracy, f1, report, _ = evaluate(model, dev_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_bert_model.pt')\n",
    "            print(\"save best model\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_bert_model.pt'))\n",
    "\n",
    "    # test_public\n",
    "    test_public_dataset = TextMatchingDataset(test_public_df, tokenizer)\n",
    "    test_public_dataloader = DataLoader(test_public_dataset, batch_size=batch_size)\n",
    "\n",
    "    accuracy, f1, report, _ = evaluate(model, test_public_dataloader, device)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "    print(report)\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5261549,
     "status": "ok",
     "timestamp": 1745177945648,
     "user": {
      "displayName": "HAO WANG",
      "userId": "10010733232250767181"
     },
     "user_tz": -480
    },
    "id": "5KGARK0rp2HX",
    "outputId": "96441a22-303e-480e-bdc6-1aed675f3ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [27:56<00:00,  1.68it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [01:09<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7220\n",
      "Validation F1 Score: 0.7145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6961    0.5922    0.6399      4894\n",
      "           1     0.7594    0.8362    0.7959     12592\n",
      "           2     0.5137    0.4029    0.4516      2514\n",
      "\n",
      "    accuracy                         0.7220     20000\n",
      "   macro avg     0.6564    0.6104    0.6292     20000\n",
      "weighted avg     0.7130    0.7220    0.7145     20000\n",
      "\n",
      "save best model\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [27:57<00:00,  1.68it/s, loss=0.593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.5784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [01:09<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7331\n",
      "Validation F1 Score: 0.7275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7228    0.5936    0.6519      4894\n",
      "           1     0.7710    0.8420    0.8049     12592\n",
      "           2     0.5184    0.4594    0.4871      2514\n",
      "\n",
      "    accuracy                         0.7331     20000\n",
      "   macro avg     0.6707    0.6317    0.6480     20000\n",
      "weighted avg     0.7274    0.7331    0.7275     20000\n",
      "\n",
      "save best model\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [27:57<00:00,  1.68it/s, loss=0.493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [01:09<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7308\n",
      "Validation F1 Score: 0.7274\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7166    0.5979    0.6519      4894\n",
      "           1     0.7759    0.8289    0.8015     12592\n",
      "           2     0.5077    0.4980    0.5028      2514\n",
      "\n",
      "    accuracy                         0.7308     20000\n",
      "   macro avg     0.6668    0.6416    0.6521     20000\n",
      "weighted avg     0.7277    0.7308    0.7274     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:17<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.7408\n",
      "Test Set F1 Score: 0.7345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7227    0.5906    0.6500      1209\n",
      "           1     0.7780    0.8531    0.8138      3159\n",
      "           2     0.5383    0.4668    0.5000       632\n",
      "\n",
      "    accuracy                         0.7408      5000\n",
      "   macro avg     0.6797    0.6368    0.6546      5000\n",
      "weighted avg     0.7343    0.7408    0.7345      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"train BERT model...\")\n",
    "bert_model = train_bert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5HBT903ycMo"
   },
   "source": [
    "双塔bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYbBbamcwL8H"
   },
   "outputs": [],
   "source": [
    "class TextMatchingDataset_Dual(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.queries = df['query'].tolist()\n",
    "        self.titles = df['title'].tolist()\n",
    "        self.labels = df['label'].tolist() if 'label' in df.columns else [0] * len(df)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.queries[idx]\n",
    "        title = self.titles[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        query_enc = self.tokenizer(query, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        title_enc = self.tokenizer(title, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids_1': query_enc['input_ids'].squeeze(0),\n",
    "            'attention_mask_1': query_enc['attention_mask'].squeeze(0),\n",
    "            'input_ids_2': title_enc['input_ids'].squeeze(0),\n",
    "            'attention_mask_2': title_enc['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSYcXwyT0Boc"
   },
   "outputs": [],
   "source": [
    "class DualTowerBERTModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-chinese', hidden_size=768, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        output1 = self.bert(input_ids=input_ids_1, attention_mask=attention_mask_1).pooler_output\n",
    "        output2 = self.bert(input_ids=input_ids_2, attention_mask=attention_mask_2).pooler_output\n",
    "        combined = torch.cat([output1, output2], dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUJ0WR1i0OYB"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids_1 = batch['input_ids_1'].to(device)\n",
    "        attention_mask_1 = batch['attention_mask_1'].to(device)\n",
    "        input_ids_2 = batch['input_ids_2'].to(device)\n",
    "        attention_mask_2 = batch['attention_mask_2'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo2PhrRB1n1t"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids_1 = batch['input_ids_1'].to(device)\n",
    "            attention_mask_1 = batch['attention_mask_1'].to(device)\n",
    "            input_ids_2 = batch['input_ids_2'].to(device)\n",
    "            attention_mask_2 = batch['attention_mask_2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    report = classification_report(all_labels, all_preds, digits=4)\n",
    "    return acc, f1, report, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4ktmIJH1qlh"
   },
   "outputs": [],
   "source": [
    "def train_bert_model_Dual():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    model = DualTowerBERTModel().to(device)\n",
    "\n",
    "    train_dataset = TextMatchingDataset_Dual(train_df, tokenizer)\n",
    "    dev_dataset = TextMatchingDataset_Dual(dev_df, tokenizer)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    epochs = 3\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "        print(f\"training loss: {train_loss:.4f}\")\n",
    "\n",
    "        acc, f1, report, _ = evaluate(model, dev_loader, device)\n",
    "        print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "        print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_dualtower_model.pt')\n",
    "            print(\"save best model\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_dualtower_model.pt'))\n",
    "\n",
    "    test_public_dataset = TextMatchingDataset_Dual(test_public_df, tokenizer)\n",
    "    test_public_dataloader = DataLoader(test_public_dataset, batch_size=batch_size)\n",
    "\n",
    "    accuracy, f1, report, _ = evaluate(model, test_public_dataloader, device)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "    print(report)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5225006,
     "status": "ok",
     "timestamp": 1745183217658,
     "user": {
      "displayName": "HAO WANG",
      "userId": "10010733232250767181"
     },
     "user_tz": -480
    },
    "id": "8rn28vYstn7h",
    "outputId": "ffca4adc-7def-46d6-fb72-90db1d2e78b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [27:44<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [01:09<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6807\n",
      "Validation F1 Score: 0.5578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6525    0.3964    0.4932      4894\n",
      "           1     0.7183    0.8548    0.7806     12592\n",
      "           2     0.4459    0.3620    0.3996      2514\n",
      "\n",
      "    accuracy                         0.6807     20000\n",
      "   macro avg     0.6056    0.5377    0.5578     20000\n",
      "weighted avg     0.6679    0.6807    0.6624     20000\n",
      "\n",
      "save best model\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [27:45<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [01:09<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6970\n",
      "Validation F1 Score: 0.5695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6751    0.4219    0.5193      4894\n",
      "           1     0.7200    0.8784    0.7914     12592\n",
      "           2     0.5155    0.3238    0.3978      2514\n",
      "\n",
      "    accuracy                         0.6970     20000\n",
      "   macro avg     0.6369    0.5414    0.5695     20000\n",
      "weighted avg     0.6833    0.6970    0.6753     20000\n",
      "\n",
      "save best model\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [27:44<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [01:09<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6975\n",
      "Validation F1 Score: 0.5903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6701    0.4585    0.5445      4894\n",
      "           1     0.7349    0.8492    0.7879     12592\n",
      "           2     0.4819    0.4025    0.4387      2514\n",
      "\n",
      "    accuracy                         0.6975     20000\n",
      "   macro avg     0.6289    0.5701    0.5903     20000\n",
      "weighted avg     0.6872    0.6975    0.6844     20000\n",
      "\n",
      "save best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:17<00:00,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.6966\n",
      "Test Set F1 Score: 0.5907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6518    0.4582    0.5381      1209\n",
      "           1     0.7389    0.8446    0.7882      3159\n",
      "           2     0.4842    0.4130    0.4458       632\n",
      "\n",
      "    accuracy                         0.6966      5000\n",
      "   macro avg     0.6249    0.5719    0.5907      5000\n",
      "weighted avg     0.6856    0.6966    0.6844      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = train_bert_model_Dual()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOXRlttmiwonu9cV+wYJ0Im",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
