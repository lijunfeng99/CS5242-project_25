{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "## Table of Contents\n",
    "* [0. Project Description](#0)\n",
    "* [1. Libraries](#1)\n",
    "* [2. Data Collection](#2)\n",
    "    - [2.1 Data Loader](#2.1)\n",
    "    - [2.2 An Overview of the Data](#2.2)\n",
    "* [3. Exploratory Data Analysis](#3) \n",
    "    - [3.1 Statistics](#3.1)\n",
    "        - [3.1.1 Distribution of Labels](#3.1.1)\n",
    "        - [3.1.2 Distribution of Document Lengths](#3.1.2)\n",
    "        - [3.1.3 Overlab between query and title](#3.1.3)\n",
    "    - [3.2 Summary of EDA and Futer steps](#3.2)\n",
    "* [4. Data Module](#4)\n",
    "    - [4.1 Pre-processing](#4.1)\n",
    "        - [4.1.1 Flitering out the Outliers](#4.1.1)\n",
    "        - [4.1.2 Upsampling of the training data via LLMs](#4.1.2)\n",
    "    - [4.2 Overview of the New Dataset](#4.2)\n",
    "* [5. Model Module](#5)\n",
    "* [6. Results](#6)\n",
    "* [7. Conclusion & Future Development](#7)\n",
    "\n",
    "The source code and basic project setup are available at https://github.com/lijunfeng99/CS5242-project_25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  0. <a name='0'></a>Project Description\n",
    "[Back to Table of Contents](#top)\n",
    "\n",
    "### Exploring Language Models for Query-Document Relevance Prediction on the QBQTC Dataset\n",
    "\n",
    "In this project, we aim to explore the effectiveness of various deep learning-based language models in predicting query-document relevance using the QBQTC dataset. Specifically, we will experiment with three prominent architectures: Recurrent Neural Networks (RNN), Transformer models, and BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "The QBQTC (QQ Browser Query Title Corpus) dataset is a large-scale Learning to Rank (LTR) dataset developed by the QQ Browser search engine team. It is designed for general web search scenarios and incorporates multi-dimensional annotations, including relevance, authority, content quality, and timeliness. For this project, we focus on the relevance labels, which indicate how well a given document title matches a user query:\n",
    "\n",
    "- 0: Poor relevance\n",
    "- 1: Moderately relevant\n",
    "- 2: Highly relevant\n",
    "\n",
    "The dataset is split into the following subsets:\n",
    "\n",
    "Training set: 180,000 samples\n",
    "Development set: 20,000 samples\n",
    "Public test set: 5,000 samples\n",
    "Our goal is to build and compare models that can effectively predict the relevance score between a query and a document title. The performance of each model will be evaluated on the development and test sets using appropriate ranking and classification metrics.\n",
    "\n",
    "Through this project, we aim to understand the relative strengths and limitations of different neural language model architectures in the context of information retrieval and learning to rank tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. <a name='1'></a>Libraries\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib_inline import backend_inline\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from tqdm.auto import tqdm as tqdm_auto\n",
    "import jieba\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import ollama\n",
    "from IPython import display\n",
    "\n",
    "sys.path.append(\"/content/drive/MyDrive/CS5242/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. <a name='2'></a>Data Collection\n",
    "[Back to Table of Contents](#top)\n",
    "\n",
    "For data collection, we first collected about 6000 reviews data for the first time. However, after data analysis, we found that the distribution and proportion of the data were seriously uneven.The main problem is that the negative and neutral data are seriously insufficient, and the second is that the data of some aspects is also seriously insufficient. So we scoured multiple websites, controlling the data tags for each crawl, and finally got our current dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 <a name='2.1'></a> Data Loader\n",
    "\n",
    "To prepare the QBQTC dataset for model training and evaluation, we implemented a custom data loader in Python to handle the JSON-formatted data files. The dataset is stored in three separate files: `train.json`, `dev.json`, and `test.json`, located in the `./dataset/` directory.\n",
    "\n",
    "We defined a function read_json_file to read each file line by line and parse each line as a JSON object. This approach ensures robustness against potential formatting issues, as it skips any lines that cannot be properly parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train.json，include 180000 \n",
      "read dev.json，include 20000 \n",
      "read test.json，include 5000 \n",
      "   id            query                           title label\n",
      "0   0            应届生实习                    实习生招聘-应届生求职网     1\n",
      "1   1  ln1+x-ln1+y=x-y  已知函数fx=1lnx+1-x则y=fx的图像高考吧百度贴吧     0\n",
      "2   2         大秦之悍卒189                   起点中文网阅文集团旗下网站     0\n",
      "3   3             出门经咒                     快快乐乐出门咒-豆丁网     1\n",
      "4   4           盖中盖广告词              谁知道盖中盖所有的广告词急用百度知道     1\n",
      "   id             query                                              title  \\\n",
      "0   0            小孩咳嗽感冒                              小孩感冒过后久咳嗽该吃什么药育儿问答宝宝树   \n",
      "1   1      前列腺癌根治术后能活多久                    前列腺癌转移能活多久前列腺癌治疗方法盘点-家庭医生在线肿瘤频道   \n",
      "2   2          英雄大作战022               英雄大作战v0.65无敌版英雄大作战v0.65无敌版小游戏4399小游戏   \n",
      "3   3  如何将一个文件复制到另一个文件里                           怎么把布局里的图纸复制到另外一个文件中去百度文库   \n",
      "4   4        gilneasart  gilneas-pictures&charactersart-worldofwarcraft...   \n",
      "\n",
      "  label  \n",
      "0     1  \n",
      "1     1  \n",
      "2     1  \n",
      "3     0  \n",
      "4     1  \n",
      "      id                query  \\\n",
      "0  13475  离婚申请之日起任何一方不同意离婚可以向   \n",
      "1  19170       淘宝里面图片轮播怎么上图片啊   \n",
      "2  15378               tpu和pc   \n",
      "3  11256             做完人流小腹刺痛   \n",
      "4   3189        央财金融专硕分数线2021   \n",
      "\n",
      "                                               title label  \n",
      "0  依据《中华人民共和国民法典》规定自婚姻登记机关收到离婚登记申请之日起任何一方不愿意离婚的可以...     1  \n",
      "1                             淘宝店铺里的轮播图片宝贝怎么弄上去啊百度知道     1  \n",
      "2                                 软硬大比拼硅胶tpu和pc材质对比-     1  \n",
      "3                    做完人流后左下腹伴有疼痛是怎么回事即问即答家庭医生在线即问即答     1  \n",
      "4                          中央财经大学2021年硕士研究生招生考试复试分数线     1  \n"
     ]
    }
   ],
   "source": [
    "dataset_path = './dataset/'\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line.strip())\n",
    "                data.append(item)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing line: {line}\")\n",
    "    return data\n",
    "\n",
    "files = ['train.json', 'dev.json', 'test.json']\n",
    "data_dict = {}\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        data = read_json_file(file_path)\n",
    "        data_dict[file.split('.')[0]] = data\n",
    "        print(f\"read {file}，include {len(data)} \")\n",
    "\n",
    "train_df = pd.DataFrame(data_dict['train'])\n",
    "dev_df = pd.DataFrame(data_dict['dev'])\n",
    "test_df = pd.DataFrame(data_dict['test'])\n",
    "print(train_df.head())\n",
    "print(dev_df.head())\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2 <a name='2.2'></a> An Overview of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#texts: 205000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 205000 entries, 0 to 204999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      205000 non-null  int64 \n",
      " 1   query   205000 non-null  object\n",
      " 2   title   205000 non-null  object\n",
      " 3   label   205000 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 6.3+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>应届生实习</td>\n",
       "      <td>实习生招聘-应届生求职网</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ln1+x-ln1+y=x-y</td>\n",
       "      <td>已知函数fx=1lnx+1-x则y=fx的图像高考吧百度贴吧</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>大秦之悍卒189</td>\n",
       "      <td>起点中文网阅文集团旗下网站</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>出门经咒</td>\n",
       "      <td>快快乐乐出门咒-豆丁网</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>盖中盖广告词</td>\n",
       "      <td>谁知道盖中盖所有的广告词急用百度知道</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            query                           title  label\n",
       "0   0            应届生实习                    实习生招聘-应届生求职网      1\n",
       "1   1  ln1+x-ln1+y=x-y  已知函数fx=1lnx+1-x则y=fx的图像高考吧百度贴吧      0\n",
       "2   2         大秦之悍卒189                   起点中文网阅文集团旗下网站      0\n",
       "3   3             出门经咒                     快快乐乐出门咒-豆丁网      1\n",
       "4   4           盖中盖广告词              谁知道盖中盖所有的广告词急用百度知道      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# merge all data into one dataframe\n",
    "records = []\n",
    "for name, data in data_dict.items():\n",
    "    records.extend(data)\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(f'#texts: {len(df)}')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. <a name='3'></a>Exploratory Data Analysis\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1 <a name='3.1'></a> Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Distribution of Labels\n",
    "According to the distribution of Labels in the dataset, we can see that the dataset is imbalanced. The number of *Lable1* is much larger than the number of *Label0*. This will lead to a bias in the model training. We need to handle this issue by using techniques such as oversampling or undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='label', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Distribution of Document Lengths\n",
    "\n",
    "EDA shows that query lengths are relatively short, averaging around 9.6 words, while titles are longer with a mean of 25.4 words. Both distributions have long tails, with maximum lengths of 246 for queries and 165 for titles, indicating some extreme outliers. The majority of queries fall between 6 and 11 words, and most titles between 15 and 31 words.\n",
    "\n",
    "For preprocessing, consider capping overly long inputs and truncating or padding as needed for model input consistency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query length description: count    205000.000000\n",
      "mean          9.599380\n",
      "std           7.293327\n",
      "min           1.000000\n",
      "25%           6.000000\n",
      "50%           8.000000\n",
      "75%          11.000000\n",
      "max         246.000000\n",
      "Name: query_len, dtype: float64\n",
      "Title length description: count    205000.000000\n",
      "mean         25.418395\n",
      "std          14.729485\n",
      "min           1.000000\n",
      "25%          15.000000\n",
      "50%          23.000000\n",
      "75%          31.000000\n",
      "max         165.000000\n",
      "Name: title_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['query_len'] = df['query'].apply(len)\n",
    "df['title_len'] = df['title'].apply(len)\n",
    "\n",
    "sns.histplot(df['query_len'], bins=50, kde=True, legend=True, label=\"Query\")\n",
    "sns.histplot(df['title_len'], bins=50, kde=True, legend=True, label=\"Title\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Length\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Query length description: {df['query_len'].describe()}\")\n",
    "print(f\"Title length description: {df['title_len'].describe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Overlap between query and title\n",
    "\n",
    "To measure the textual similarity between queries and titles, we calculated the Jaccard similarity at the word level using Chinese word segmentation (jieba). The overall distribution of Jaccard similarity shows a relatively wide spread, indicating varying degrees of lexical overlap across samples.\n",
    "\n",
    "When visualizing the similarity distributions by relevance labels, we observed no strong or consistent patterns. While higher relevance scores (label 2) tend to have slightly higher average similarity, the distributions of all three labels (0, 1, 2) overlap significantly. This suggests that simple word-level overlap is not a reliable standalone indicator of semantic relevance. In other words, queries and titles can be relevant even with low lexical overlap, possibly due to paraphrasing, synonym usage, or implicit relationships that word-level comparison fails to capture.ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/mg/t_zxv0bx41x3ty06pvsqh1280000gn/T/jieba.cache\n",
      "Loading model cost 0.370 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calc_Jaccard(row):\n",
    "    query_words = set(jieba.lcut(row['query']))\n",
    "    title_words = set(jieba.lcut(row['title']))\n",
    "    return len(query_words & title_words) / len(query_words | title_words)\n",
    "df[\"Jaccard_similarity\"] = df.apply(calc_Jaccard, axis=1)\n",
    "\n",
    "sns.histplot(df['Jaccard_similarity'], bins=50, kde=True, legend=True, label='Jaccard similarity')\n",
    "sns.histplot(\n",
    "    df[df['label'] == 0]['Jaccard_similarity'],\n",
    "    bins=50,\n",
    "    kde=True,\n",
    "    legend=True,\n",
    "    label='Poor Relevance'\n",
    ")\n",
    "sns.histplot(\n",
    "    df[df['label'] == 1]['Jaccard_similarity'],\n",
    "    bins=50,\n",
    "    kde=True,\n",
    "    legend=True,\n",
    "    label='Moderately Relevance'\n",
    ")\n",
    "sns.histplot(\n",
    "    df[df['label'] == 2]['Jaccard_similarity'],\n",
    "    bins=50,\n",
    "    kde=True,\n",
    "    legend=True,\n",
    "    label='Highly Relevance'\n",
    ")\n",
    "plt.xlabel('Jaccard similarity')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  <a name='3.2'></a> Summary of EDA and Futher steps\n",
    "#### Summary:\n",
    "- The dataset is **label-imbalanced**.\n",
    "- **Queries are short** (~9.6 words), while **titles are longer** (~25.4 words), with a few extreme outliers.\n",
    "- **Jaccard similarity** shows weak correlation with relevance, indicating that **lexical overlap is not enough** to capture semantic relevance.\n",
    "\n",
    "#### Next Steps:\n",
    "- Address **label imbalance** using resampling or class-weighted loss.\n",
    "- Apply **padding/truncation** to standardize input lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. <a name='4'></a>Data Module\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.1 <a name='4.1'></a>Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.1.1 <a name='4.1.1'></a> Filtering out the Outliers\n",
    "\n",
    "According to our EDA results, we identified extreme outliers in both query and title lengths. Extremely long texts may not only increase computational cost but also lead to noise in training, as they may not represent typical search engine queries or titles.\n",
    "\n",
    "To mitigate this, we filtered out entries where:\n",
    "- The **query length** exceeds 30 characters.\n",
    "- The **title length** exceeds 50 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterout_length(data, max_len, column='text'):\n",
    "    filtered_data = []\n",
    "    for d in data:\n",
    "        if len(d[column]) <= max_len:\n",
    "            filtered_data.append(d)\n",
    "    return filtered_data\n",
    "\n",
    "for name, data in data_dict.items():\n",
    "    data_dict[name] = filterout_length(data, 50, 'title')\n",
    "    data_dict[name] = filterout_length(data_dict[name], 30, 'query')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.1.2 <a name='4.1.2'></a> Sampling of the training data via LLMs\n",
    "To balance the dataset, we use follow steps:\n",
    "\n",
    "1. Drop 50% of the data with *Lable1*;\n",
    "2. Use the LLM to rephrase all the data with *Label2* and add this sample to dataset.\n",
    "\n",
    "This both balances the class distribution and increases dataset diversity through LLM-generated paraphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71df06b3f8a743a49fb4dc5aa175e487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167097 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f086ddd8c1f84e48a71f148396e1c3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0cc3a957584b5ebac90a9c171faa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chat_with_ollama(sentence):\n",
    "    client = ollama.Client()\n",
    "\n",
    "    model_name = \"mistral\"\n",
    "\n",
    "    response = client.chat(model=model_name, messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Please rephrase this sentence:{sentence}\"\"\"}\n",
    "    ],\n",
    "    options={\n",
    "        \"num_predict\": 15  # This is the equivalent of max_tokens in Ollama\n",
    "    })\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "newdata_dict = {}\n",
    "for name, data in data_dict.items():\n",
    "    newdata_dict[name] = []\n",
    "    id = 0\n",
    "    for item in tqdm(data):\n",
    "        id += 1\n",
    "        item['id'] = id\n",
    "        if item['label'] == '1':\n",
    "            if random.random() < 0.5:\n",
    "                newdata_dict[name].append(item)\n",
    "        elif item['label'] == '2':\n",
    "            newdata_dict[name].append(item)\n",
    "            id += 1\n",
    "            newdata_dict[name].append({\n",
    "                \"id\": id,\n",
    "                \"query\": chat_with_ollama(item['query']),\n",
    "                \"label\": \"2\",\n",
    "                \"title\": item['title'],\n",
    "            })\n",
    "        else:\n",
    "            newdata_dict[name].append(item)\n",
    "\n",
    "# save new data\n",
    "for name, data in newdata_dict.items():\n",
    "    with open(os.path.join(dataset_path, f\"{name}_new.json\"), \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 <a name='4.2'></a> Overview of the New Dataset\n",
    "\n",
    "After comprehensive preprocessing, we finalized a clean and balanced dataset containing 154,592 samples. Each entry includes a query, a corresponding document title, and a relevance label (0–2). The dataset has been filtered to remove outlier lengths, with queries capped at 30 characters (mean ≈ 10.1) and titles at 50 (mean ≈ 22.8). Label imbalance was addressed via downsampling and LLM-based augmentation, resulting in a semantically diverse and model-ready corpus for training relevance models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#texts: 154592\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 154592 entries, 0 to 154591\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      154592 non-null  int64 \n",
      " 1   query   154592 non-null  object\n",
      " 2   title   154592 non-null  object\n",
      " 3   label   154592 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 4.7+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query length description: count    154592.000000\n",
      "mean         10.086563\n",
      "std           5.892387\n",
      "min           1.000000\n",
      "25%           6.000000\n",
      "50%           9.000000\n",
      "75%          13.000000\n",
      "max          30.000000\n",
      "Name: query_len, dtype: float64\n",
      "Title length description: count    154592.000000\n",
      "mean         22.766631\n",
      "std          10.286630\n",
      "min           1.000000\n",
      "25%          15.000000\n",
      "50%          22.000000\n",
      "75%          29.000000\n",
      "max          50.000000\n",
      "Name: title_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "files = ['train_new.json', 'dev_new.json', 'test_new.json']\n",
    "data_dict = {}\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data_dict[file] = json.load(f)\n",
    "\n",
    "records = []\n",
    "for name, data in data_dict.items():\n",
    "    records.extend(data)\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(f'#texts: {len(df)}')\n",
    "print(df.info())\n",
    "df.head()\n",
    "\n",
    "sns.countplot(x='label', data=df)\n",
    "plt.show()\n",
    "\n",
    "df['query_len'] = df['query'].apply(len)\n",
    "df['title_len'] = df['title'].apply(len)\n",
    "\n",
    "sns.histplot(df['query_len'], bins=50, kde=True, legend=True, label=\"Query\")\n",
    "sns.histplot(df['title_len'], bins=50, kde=True, legend=True, label=\"Title\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Length\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Query length description: {df['query_len'].describe()}\")\n",
    "print(f\"Title length description: {df['title_len'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. <a name='5'></a>Model Module\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.1 <a name='5.2'></a>MLP\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP Model Structure](img/mlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP model described consists of an embedding layer that transforms input tokens into dense vectors, followed by multiple fully-connected hidden layers with ReLU activations to learn nonlinear patterns, and concludes with an output layer that produces logits for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/content/drive/MyDrive/CS5242/dataset/\"\n",
    "\n",
    "def load_data(file):\n",
    "    with open(data_path + file, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_df = load_data('train.json')\n",
    "dev_df = load_data('dev.json')\n",
    "test_public_df = load_data('test_public.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Vocab and Embedding Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, max_vocab_size=10000):\n",
    "    counter = Counter()\n",
    "    for sent in sentences:\n",
    "        counter.update(sent.split())\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    for word, _ in counter.most_common(max_vocab_size - len(vocab)):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "all_sentences = pd.concat([train_df, dev_df])['query'] + \" \" + pd.concat([train_df, dev_df])['title']\n",
    "vocab = build_vocab(all_sentences)\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.random.normal(scale=0.6, size=(len(vocab), embedding_dim)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Encoding and Label Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(df, vocab):\n",
    "    encoded = [\n",
    "        torch.tensor([vocab.get(w, vocab['<unk>']) for w in (q + \" \" + t).split()])\n",
    "        for q, t in zip(df['query'], df['title'])\n",
    "    ]\n",
    "    return pad_sequence(encoded, batch_first=True, padding_value=0)\n",
    "\n",
    "X_train_ids = encode_text(train_df, vocab)\n",
    "y_train = torch.tensor(train_df['label'].astype(int).values)\n",
    "\n",
    "X_dev_ids = encode_text(dev_df, vocab)\n",
    "y_dev = torch.tensor(dev_df['label'].astype(int).values)\n",
    "\n",
    "X_test_ids = encode_text(test_public_df, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix))\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        input_dim = embedding_matrix.shape[1]\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            input_dim = hidden_dim\n",
    "        self.out_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids).mean(dim=1)\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Instantiation and Device Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLP(\n",
       "  (embedding): Embedding(10000, 100)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (out_layer): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MyMLP(embedding_matrix, hidden_dim=128, output_dim=3, num_layers=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_ids, y_train), batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(TensorDataset(X_dev_ids, y_dev), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test_ids), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4826    0.0198    0.0381      4894\n",
      "           1     0.6315    0.9922    0.7718     12592\n",
      "           2     0.6000    0.0036    0.0071      2514\n",
      "\n",
      "    accuracy                         0.6300     20000\n",
      "   macro avg     0.5714    0.3385    0.2723     20000\n",
      "weighted avg     0.5911    0.6300    0.4961     20000\n",
      "\n",
      "Epoch 2, Loss: 0.8859\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4868    0.0793    0.1364      4894\n",
      "           1     0.6365    0.9693    0.7684     12592\n",
      "           2     0.5769    0.0060    0.0118      2514\n",
      "\n",
      "    accuracy                         0.6304     20000\n",
      "   macro avg     0.5667    0.3515    0.3055     20000\n",
      "weighted avg     0.5924    0.6304    0.5186     20000\n",
      "\n",
      "Epoch 3, Loss: 0.8737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5833    0.0930    0.1604      4894\n",
      "           1     0.6401    0.9726    0.7721     12592\n",
      "           2     0.3678    0.0127    0.0246      2514\n",
      "\n",
      "    accuracy                         0.6367     20000\n",
      "   macro avg     0.5304    0.3594    0.3190     20000\n",
      "weighted avg     0.5920    0.6367    0.5284     20000\n",
      "\n",
      "Epoch 4, Loss: 0.8612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6469    0.1071    0.1837      4894\n",
      "           1     0.6427    0.9754    0.7748     12592\n",
      "           2     0.3797    0.0119    0.0231      2514\n",
      "\n",
      "    accuracy                         0.6418     20000\n",
      "   macro avg     0.5564    0.3648    0.3272     20000\n",
      "weighted avg     0.6107    0.6418    0.5357     20000\n",
      "\n",
      "Epoch 5, Loss: 0.8500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6120    0.1463    0.2361      4894\n",
      "           1     0.6477    0.9655    0.7753     12592\n",
      "           2     0.3770    0.0091    0.0179      2514\n",
      "\n",
      "    accuracy                         0.6448     20000\n",
      "   macro avg     0.5456    0.3736    0.3431     20000\n",
      "weighted avg     0.6049    0.6448    0.5482     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            truths.extend(labels.numpy())\n",
    "    print(classification_report(truths, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Predictions for the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": test_public_df[\"id\"],\n",
    "    \"label\": [str(x) for x in test_preds]\n",
    "})\n",
    "\n",
    "submission_df.to_json(\"test_public_predictions.json\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_pred\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5729,\n        \"min\": 2,\n        \"max\": 19997,\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          17752,\n          1974,\n          14928\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_pred"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ed19cd3b-68fe-4307-a097-1c8c8c68e6a7\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3189</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed19cd3b-68fe-4307-a097-1c8c8c68e6a7')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ed19cd3b-68fe-4307-a097-1c8c8c68e6a7 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ed19cd3b-68fe-4307-a097-1c8c8c68e6a7');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-957ffc41-d970-460a-a749-c74127d834ce\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-957ffc41-d970-460a-a749-c74127d834ce')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-957ffc41-d970-460a-a749-c74127d834ce button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      id  label\n",
       "0  13475      1\n",
       "1  19170      1\n",
       "2  15378      1\n",
       "3  11256      1\n",
       "4   3189      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pred = pd.read_json(\"test_public_predictions.json\", lines=True)\n",
    "\n",
    "df_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.646\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6166    0.1290    0.2134      1209\n",
      "           1     0.6484    0.9725    0.7780      3159\n",
      "           2     0.2222    0.0032    0.0062       632\n",
      "\n",
      "    accuracy                         0.6460      5000\n",
      "   macro avg     0.4957    0.3682    0.3326      5000\n",
      "weighted avg     0.5868    0.6460    0.5439      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df_true = load_data(\"test_public.json\")\n",
    "df_true = df_true.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "df_pred = pd.read_json(\"test_public_predictions.json\", lines=True)\n",
    "df_pred = df_pred.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "assert all(df_true[\"id\"].values == df_pred[\"id\"].values), \"The IDs do not match, please check!\"\n",
    "\n",
    "y_true = df_true[\"label\"].astype(int)\n",
    "y_pred = df_pred[\"label\"].astype(int)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"Test Accuracy:\", acc)\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.2 <a name='5.2'></a>CNN\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN Model Structure](img/cnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model described is a TextCNN designed for text classification, consisting of an embedding layer initialized with pre-generated embeddings, followed by parallel convolutional layers with varying kernel sizes to capture different n-gram features.  Each convolutional output is pooled and concatenated to form a unified feature representation, which then passes through a dropout layer to reduce overfitting, and finally a fully connected layer produces logits for classification.  The network is trained using cross-entropy loss and optimized with the Adam algorithm, demonstrating effectiveness in capturing local textual patterns for accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/content/drive/MyDrive/CS5242/dataset/'\n",
    "\n",
    "def load_jsonl(file):\n",
    "    with open(DATA_PATH + file, 'r', encoding='utf-8') as f:\n",
    "        return pd.DataFrame([json.loads(line) for line in f])\n",
    "\n",
    "train_df = load_jsonl('train.json')\n",
    "dev_df = load_jsonl('dev.json')\n",
    "test_public_df = load_jsonl('test_public.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Vocabulary and Encoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, max_size=10000):\n",
    "    counter = Counter()\n",
    "    for sent in sentences:\n",
    "        counter.update(sent.split())\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    for word, _ in counter.most_common(max_size - len(vocab)):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode_text(df, vocab, min_len=5):\n",
    "    df['query'] = df['query'].astype(str)\n",
    "    df['title'] = df['title'].astype(str)\n",
    "\n",
    "    encoded = [\n",
    "        torch.tensor([vocab.get(w, vocab['<unk>']) for w in (q + \" \" + t).split()])\n",
    "        for q, t in zip(df['query'], df['title'])\n",
    "    ]\n",
    "    padded = [seq if len(seq) >= min_len else F.pad(seq, (0, min_len - len(seq)), value=0) for seq in encoded]\n",
    "    return pad_sequence(padded, batch_first=True, padding_value=0)\n",
    "\n",
    "train_text = train_df['query'].astype(str) + \" \" + train_df['title'].astype(str)\n",
    "dev_text = dev_df['query'].astype(str) + \" \" + dev_df['title'].astype(str)\n",
    "all_text = pd.concat([train_text, dev_text])\n",
    "\n",
    "vocab = build_vocab(all_text)\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.random.normal(0, 0.1, size=(len(vocab), embedding_dim)).astype(np.float32)\n",
    "\n",
    "X_train = encode_text(train_df, vocab)\n",
    "y_train = torch.tensor(train_df['label'].astype(int).values)\n",
    "\n",
    "X_dev = encode_text(dev_df, vocab)\n",
    "y_dev = torch.tensor(dev_df['label'].astype(int).values)\n",
    "\n",
    "X_test = encode_text(test_public_df, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the TextCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_classes, kernel_nums=[100, 100, 100], kernel_sizes=[3, 4, 5]):\n",
    "        super(TextCNN, self).__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix))\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, kernel_nums[i], (kernel_sizes[i], embed_dim))\n",
    "            for i in range(len(kernel_sizes))\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(sum(kernel_nums), num_classes)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)         # [batch_size, kernel_num, seq_len - k + 1]\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)          # [batch_size, seq_len, embed_dim]\n",
    "        x = x.unsqueeze(1)                     # [batch_size, 1, seq_len, embed_dim]\n",
    "        x = torch.cat([self.conv_and_pool(x, conv) for conv in self.convs], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Validation of the TextCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2524.9415\n",
      "Validation Accuracy: 0.6296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000      4894\n",
      "           1     0.6296    1.0000    0.7727     12592\n",
      "           2     0.0000    0.0000    0.0000      2514\n",
      "\n",
      "    accuracy                         0.6296     20000\n",
      "   macro avg     0.2099    0.3333    0.2576     20000\n",
      "weighted avg     0.3964    0.6296    0.4865     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2515.7200\n",
      "Validation Accuracy: 0.6296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000      4894\n",
      "           1     0.6296    1.0000    0.7727     12592\n",
      "           2     0.0000    0.0000    0.0000      2514\n",
      "\n",
      "    accuracy                         0.6296     20000\n",
      "   macro avg     0.2099    0.3333    0.2576     20000\n",
      "weighted avg     0.3964    0.6296    0.4865     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2510.1681\n",
      "Validation Accuracy: 0.6296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.0002    0.0004      4894\n",
      "           1     0.6296    0.9999    0.7727     12592\n",
      "           2     0.0000    0.0000    0.0000      2514\n",
      "\n",
      "    accuracy                         0.6296     20000\n",
      "   macro avg     0.3765    0.3334    0.2577     20000\n",
      "weighted avg     0.5188    0.6296    0.4866     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2505.7881\n",
      "Validation Accuracy: 0.6301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.0025    0.0049      4894\n",
      "           1     0.6301    0.9994    0.7729     12592\n",
      "           2     0.8571    0.0024    0.0048      2514\n",
      "\n",
      "    accuracy                         0.6301     20000\n",
      "   macro avg     0.6957    0.3347    0.2608     20000\n",
      "weighted avg     0.6512    0.6301    0.4884     20000\n",
      "\n",
      "Epoch 5, Loss: 2502.9500\n",
      "Validation Accuracy: 0.62985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5769    0.0031    0.0061      4894\n",
      "           1     0.6303    0.9975    0.7725     12592\n",
      "           2     0.4565    0.0084    0.0164      2514\n",
      "\n",
      "    accuracy                         0.6299     20000\n",
      "   macro avg     0.5546    0.3363    0.2650     20000\n",
      "weighted avg     0.5954    0.6299    0.4899     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "dev_loader = DataLoader(TensorDataset(X_dev, y_dev), batch_size=64)\n",
    "test_loader = DataLoader(TensorDataset(X_test), batch_size=64)\n",
    "\n",
    "model = TextCNN(embedding_matrix, num_classes=3)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dev_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            logits = model(batch_X)\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            truths.extend(batch_y.numpy())\n",
    "    print(\"Validation Accuracy:\", accuracy_score(truths, preds))\n",
    "    print(classification_report(truths, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Test Set Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.632\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.0041    0.0082      1209\n",
      "           1     0.6322    0.9984    0.7742      3159\n",
      "           2     0.2000    0.0016    0.0031       632\n",
      "\n",
      "    accuracy                         0.6320      5000\n",
      "   macro avg     0.5552    0.3347    0.2618      5000\n",
      "weighted avg     0.6262    0.6320    0.4915      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df_true = load_jsonl(\"test_public.json\")\n",
    "df_true = df_true.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "df_pred = pd.read_json(\"test_public_predictions_cnn.json\", lines=True)\n",
    "df_pred = df_pred.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "assert all(df_true[\"id\"].values == df_pred[\"id\"].values), \"❌ The IDs do not match, please check!\"\n",
    "\n",
    "y_true = df_true[\"label\"].astype(int)\n",
    "y_pred = df_pred[\"label\"].astype(int)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"Test Accuracy:\", acc)\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.3 <a name='5.3'></a>RNN(Single Tower+Tow Tower)\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](img/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SingleTowerRNN adopts a \"single-tower\" architecture. It first combines the two input texts (e.g., query and title) into a single sequence, then processes this combined sequence using a single RNN network, and finally outputs the prediction result through a fully connected layer. This model captures interaction information between the query and title early within the RNN layers.\n",
    "\n",
    "The TwoTowerRNN uses a \"two-tower\" architecture. It builds separate, independent RNN \"towers\" for each input text (e.g., query and title) to generate their respective representation vectors. These towers share the embedding layer but have independent RNN layers. After each tower processes its input, their respective final representation vectors are concatenated, and then fed into a fully connected layer for the final relevance prediction. This approach focuses on independently understanding the semantics of each input first, before making an interactive judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Preprocessing <a name='Preprocessing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenizes Chinese text using jieba.\"\"\"\n",
    "    if text is None: return []\n",
    "    # Use cut_for_search for potentially better recall\n",
    "    return jieba.lcut_for_search(str(text)) # Ensure text is string\n",
    "\n",
    "def build_vocab(texts, max_size, min_freq):\n",
    "    \"\"\"Builds a vocabulary including <PAD>, <UNK>, <SEP>.\"\"\"\n",
    "    word_counts = Counter()\n",
    "    print(\"Building vocabulary...\")\n",
    "    for text in texts:\n",
    "        word_counts.update(text)\n",
    "\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "    max_real_words = max_size - 3 # Reserve space for PAD, UNK, SEP\n",
    "    vocab_words = [word for word, freq in sorted_words if freq >= min_freq]\n",
    "    if len(vocab_words) > max_real_words:\n",
    "        vocab_words = vocab_words[:max_real_words]\n",
    "\n",
    "    word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<SEP>': 2}\n",
    "    idx_counter = 3\n",
    "    for word in vocab_words:\n",
    "        if word not in word_to_idx:\n",
    "             word_to_idx[word] = idx_counter\n",
    "             idx_counter += 1\n",
    "\n",
    "    print(f\"Vocabulary built with {len(word_to_idx)} words.\")\n",
    "    return word_to_idx\n",
    "\n",
    "def tokens_to_indices(tokens, word_to_idx, max_len):\n",
    "     \"\"\"Converts tokens to indices, truncates.\"\"\"\n",
    "     indices = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in tokens]\n",
    "     return indices[:max_len]\n",
    "\n",
    "def pad_sequence(indices, max_len, pad_idx):\n",
    "    \"\"\"Pads sequence to max_len.\"\"\"\n",
    "    current_len = len(indices)\n",
    "    if current_len < max_len:\n",
    "        return indices + [pad_idx] * (max_len - current_len)\n",
    "    else:\n",
    "        return indices[:max_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Unified Dataset <a name='Unified_Dataset'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevanceDataset(data.Dataset):\n",
    "    def __init__(self, queries, titles, labels, word_to_idx, max_query_len, max_title_len, max_combined_len):\n",
    "        self.queries = queries\n",
    "        self.titles = titles\n",
    "        self.labels = labels\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_query_len = max_query_len\n",
    "        self.max_title_len = max_title_len\n",
    "        self.max_combined_len = max_combined_len\n",
    "        self.pad_idx = word_to_idx['<PAD>']\n",
    "        self.sep_idx = word_to_idx['<SEP>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        query = self.queries[index]\n",
    "        title = self.titles[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        query_tokens = tokenize(query)\n",
    "        title_tokens = tokenize(title)\n",
    "\n",
    "        # Indices before padding (for combining)\n",
    "        query_indices_unpadded = tokens_to_indices(query_tokens, self.word_to_idx, self.max_query_len)\n",
    "        title_indices_unpadded = tokens_to_indices(title_tokens, self.word_to_idx, self.max_title_len)\n",
    "\n",
    "        # --- Prepare data for BOTH architectures ---\n",
    "\n",
    "        # For Two-Tower Model (Separate Padded Sequences)\n",
    "        query_padded = pad_sequence(query_indices_unpadded, self.max_query_len, self.pad_idx)\n",
    "        title_padded = pad_sequence(title_indices_unpadded, self.max_title_len, self.pad_idx)\n",
    "\n",
    "        # For Single-Tower Model (Combined Padded Sequence)\n",
    "        combined_indices = query_indices_unpadded + [self.sep_idx] + title_indices_unpadded\n",
    "        combined_padded = pad_sequence(combined_indices, self.max_combined_len, self.pad_idx)\n",
    "\n",
    "        # Convert to tensors\n",
    "        query_tensor = torch.tensor(query_padded, dtype=torch.long)\n",
    "        title_tensor = torch.tensor(title_padded, dtype=torch.long)\n",
    "        combined_tensor = torch.tensor(combined_padded, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return query_tensor, title_tensor, combined_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model Definition <a name='Model_Definition'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_final_hidden(hidden_state, num_layers, num_directions, batch_size):\n",
    "    # hidden_state shape: (num_layers * num_directions, batch_size, rnn_hidden_dim)\n",
    "    # Reshape to view layers and directions separately\n",
    "    hidden = hidden_state.view(num_layers, num_directions, batch_size, -1)\n",
    "    # Get the hidden state of the last layer (forward and backward if bidirectional)\n",
    "    # Shape: (num_directions, batch_size, rnn_hidden_dim)\n",
    "    hidden_last_layer = hidden[-1]\n",
    "    # Permute and reshape to (batch_size, num_directions * rnn_hidden_dim)\n",
    "    final_hidden = hidden_last_layer.permute(1, 0, 2).reshape(batch_size, -1)\n",
    "    return final_hidden\n",
    "\n",
    "class SingleTowerRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,\n",
    "                 rnn_layers, dropout_prob, bidirectional, rnn_nonlinearity, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        rnn_hidden_dim = hidden_dim // 2 if bidirectional else hidden_dim\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.rnn = nn.RNN(embedding_dim, rnn_hidden_dim, num_layers=rnn_layers,\n",
    "                          nonlinearity=rnn_nonlinearity, batch_first=True,\n",
    "                          dropout=dropout_prob if rnn_layers > 1 else 0,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes) # Input is hidden_dim (num_directions * rnn_hidden_dim)\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.num_directions = num_directions\n",
    "\n",
    "    def forward(self, combined_indices):\n",
    "        # combined_indices: (batch_size, max_combined_len)\n",
    "        embedded = self.dropout(self.embedding(combined_indices))\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # hidden: (num_layers * num_directions, batch_size, rnn_hidden_dim)\n",
    "        final_hidden = _extract_final_hidden(hidden, self.rnn_layers, self.num_directions, combined_indices.size(0))\n",
    "        # final_hidden: (batch_size, hidden_dim)\n",
    "        logits = self.fc(self.dropout(final_hidden))\n",
    "        return logits\n",
    "\n",
    "class TwoTowerRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,\n",
    "                 rnn_layers, dropout_prob, bidirectional, rnn_nonlinearity, pad_idx):\n",
    "        super().__init__()\n",
    "        # Share the embedding layer between towers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        rnn_hidden_dim = hidden_dim // 2 if bidirectional else hidden_dim\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.rnn_query = nn.RNN(embedding_dim, rnn_hidden_dim, num_layers=rnn_layers,\n",
    "                                nonlinearity=rnn_nonlinearity, batch_first=True,\n",
    "                                dropout=dropout_prob if rnn_layers > 1 else 0,\n",
    "                                bidirectional=bidirectional)\n",
    "        self.rnn_title = nn.RNN(embedding_dim, rnn_hidden_dim, num_layers=rnn_layers,\n",
    "                                nonlinearity=rnn_nonlinearity, batch_first=True,\n",
    "                                dropout=dropout_prob if rnn_layers > 1 else 0,\n",
    "                                bidirectional=bidirectional)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Input to FC layer is concatenation of final hidden states from both towers\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.num_directions = num_directions\n",
    "\n",
    "    def forward(self, query_indices, title_indices):\n",
    "        # query_indices: (batch_size, max_query_len)\n",
    "        # title_indices: (batch_size, max_title_len)\n",
    "\n",
    "        query_embedded = self.dropout(self.embedding(query_indices))\n",
    "        title_embedded = self.dropout(self.embedding(title_indices))\n",
    "\n",
    "        _, query_hidden = self.rnn_query(query_embedded)\n",
    "        _, title_hidden = self.rnn_title(title_embedded)\n",
    "        # hidden shapes: (num_layers * num_directions, batch_size, rnn_hidden_dim)\n",
    "\n",
    "        query_final_hidden = _extract_final_hidden(query_hidden, self.rnn_layers, self.num_directions, query_indices.size(0))\n",
    "        title_final_hidden = _extract_final_hidden(title_hidden, self.rnn_layers, self.num_directions, title_indices.size(0))\n",
    "        # final hidden shapes: (batch_size, hidden_dim)\n",
    "\n",
    "        # Concatenate the final states\n",
    "        combined = torch.cat((query_final_hidden, title_final_hidden), dim=1)\n",
    "        # combined: (batch_size, hidden_dim * 2)\n",
    "\n",
    "        logits = self.fc(self.dropout(combined))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Training Function <a name='Training_Function'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_architecture, train_loader, val_loader, optimizer, criterion, num_epochs, device, save_path):\n",
    "    print(f\"\\n--- Starting Training ({model_architecture.upper()}) ---\")\n",
    "    best_val_metric = -1.0 # Initialize with a value lower than any possible F1 score\n",
    "    start_train_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        # --- Training Loop with TQDM ---\n",
    "        train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", leave=False)\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            query_batch, title_batch, combined_batch, labels_batch = batch\n",
    "            labels_batch = labels_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if model_architecture == 'single':\n",
    "                model_input = combined_batch.to(device)\n",
    "                logits = model(model_input)\n",
    "            elif model_architecture == 'two':\n",
    "                query_input = query_batch.to(device)\n",
    "                title_input = title_batch.to(device)\n",
    "                logits = model(query_input, title_input)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model_architecture specified\")\n",
    "\n",
    "            loss = criterion(logits, labels_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item() * labels_batch.size(0)\n",
    "            total_train_samples += labels_batch.size(0)\n",
    "\n",
    "            # Update TQDM postfix with current batch loss\n",
    "            train_iterator.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        # --- End Training Loop ---\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / total_train_samples\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Training: loss={avg_train_loss:.4f}\") # Print average loss after epoch\n",
    "\n",
    "        # --- Validation Step with TQDM ---\n",
    "        model.eval()\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "\n",
    "        val_iterator = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Evaluating\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_iterator:\n",
    "                query_batch, title_batch, combined_batch, labels_batch = batch\n",
    "                labels_batch = labels_batch.to(device)\n",
    "                all_val_labels.extend(labels_batch.cpu().numpy()) # Collect labels\n",
    "\n",
    "                if model_architecture == 'single':\n",
    "                    model_input = combined_batch.to(device)\n",
    "                    logits = model(model_input)\n",
    "                elif model_architecture == 'two':\n",
    "                    query_input = query_batch.to(device)\n",
    "                    title_input = title_batch.to(device)\n",
    "                    logits = model(query_input, title_input)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid model_architecture specified\")\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_val_preds.extend(preds.cpu().numpy()) # Collect predictions\n",
    "        # --- End Validation Loop ---\n",
    "\n",
    "        # Calculate Validation Metrics\n",
    "        val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "        # Calculate weighted F1 score\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "        # Print Classification Report for Validation\n",
    "        target_names = ['Relevance 0', 'Relevance 1', 'Relevance 2']\n",
    "        try:\n",
    "            unique_labels_in_data = sorted(list(set(all_val_labels) | set(all_val_preds)))\n",
    "            current_target_names = [target_names[i] for i in unique_labels_in_data if i < len(target_names)]\n",
    "            report = classification_report(all_val_labels, all_val_preds, target_names=current_target_names, labels=unique_labels_in_data, digits=4, zero_division=0)\n",
    "            print(report)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate full classification report (validation): {e}\")\n",
    "            print(classification_report(all_val_labels, all_val_preds, digits=4, zero_division=0))\n",
    "\n",
    "\n",
    "        # Save best model based on weighted F1 score\n",
    "        if val_f1 > best_val_metric:\n",
    "            best_val_metric = val_f1\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"save best model to {save_path} (Val F1: {best_val_metric:.4f})\") # Indicate saving\n",
    "\n",
    "    total_train_duration = time.time() - start_train_time\n",
    "    print(f\"--- Training Finished ({model_architecture.upper()}) in {total_train_duration:.2f}s ---\")\n",
    "    print(f\"Best Validation F1 Score ({model_architecture.upper()}): {best_val_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Evaluation Function <a name='Evaluation_Function'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_architecture, test_loader, criterion, device):\n",
    "    print(f\"\\n--- Starting Evaluation on Test Set ({model_architecture.upper()}) ---\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # --- Evaluation Loop with TQDM ---\n",
    "    test_iterator = tqdm(test_loader, desc=\"Evaluating Test Set\", leave=True) # leave=True to keep bar after completion\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iterator:\n",
    "            query_batch, title_batch, combined_batch, labels_batch = batch\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            all_labels.extend(labels_batch.cpu().numpy()) # Collect labels\n",
    "\n",
    "            if model_architecture == 'single':\n",
    "                model_input = combined_batch.to(device)\n",
    "                logits = model(model_input)\n",
    "            elif model_architecture == 'two':\n",
    "                query_input = query_batch.to(device)\n",
    "                title_input = title_batch.to(device)\n",
    "                logits = model(query_input, title_input)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model_architecture specified\")\n",
    "\n",
    "            # loss = criterion(logits, labels_batch)\n",
    "            # total_loss += loss.item() * labels_batch.size(0)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy()) # Collect predictions\n",
    "    # --- End Evaluation Loop ---\n",
    "\n",
    "\n",
    "    if not all_labels:\n",
    "        print(\"Error: No samples found in the test loader for evaluation.\")\n",
    "        return 0.0, 0.0, [], [] # Return zeros or handle appropriately\n",
    "\n",
    "    # Calculate Test Metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0) # Weighted F1\n",
    "\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Set F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    # Print Classification Report for Test Set\n",
    "    target_names = ['Relevance 0', 'Relevance 1', 'Relevance 2']\n",
    "    try:\n",
    "        unique_labels_in_data = sorted(list(set(all_labels) | set(all_preds)))\n",
    "        current_target_names = [target_names[i] for i in unique_labels_in_data if i < len(target_names)]\n",
    "        report = classification_report(all_labels, all_preds, target_names=current_target_names, labels=unique_labels_in_data, digits=4, zero_division=0)\n",
    "        print(report)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate full classification report (test): {e}\")\n",
    "        print(classification_report(all_labels, all_preds, digits=4, zero_division=0))\n",
    "\n",
    "    # print(\"\\nConfusion Matrix:\")\n",
    "    # cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2])\n",
    "    # print(cm)\n",
    "\n",
    "    print(f\"--- Evaluation Finished ({model_architecture.upper()}) ---\")\n",
    "    return accuracy, test_f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Main Execution <a name='Main_Execution'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary from Training Data Only\n",
    "print(\"\\nTokenizing training data for vocabulary...\")\n",
    "train_query_tokens = [tokenize(q) for q in train_df['query']]\n",
    "train_title_tokens = [tokenize(t) for t in train_df['title']]\n",
    "word_to_idx = build_vocab(train_query_tokens + train_title_tokens,\n",
    "                          CONFIG[\"vocab_max_size\"], CONFIG[\"vocab_min_freq\"])\n",
    "vocab_size = len(word_to_idx)\n",
    "pad_idx = word_to_idx['<PAD>'] # Get pad index for models\n",
    "\n",
    "# Create Datasets and DataLoaders (Unified Dataset)\n",
    "train_dataset = RelevanceDataset(train_df['query'].tolist(), train_df['title'].tolist(), train_df['label'].tolist(),\n",
    "                                  word_to_idx, CONFIG[\"max_query_len\"], CONFIG[\"max_title_len\"], CONFIG[\"max_combined_len\"])\n",
    "val_dataset = RelevanceDataset(val_df['query'].tolist(), val_df['title'].tolist(), val_df['label'].tolist(),\n",
    "                                word_to_idx, CONFIG[\"max_query_len\"], CONFIG[\"max_title_len\"], CONFIG[\"max_combined_len\"])\n",
    "test_dataset = RelevanceDataset(test_df['query'].tolist(), test_df['title'].tolist(), test_df['label'].tolist(),\n",
    "                                word_to_idx, CONFIG[\"max_query_len\"], CONFIG[\"max_title_len\"], CONFIG[\"max_combined_len\"])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "# --- Shared Components ---\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SINGLE TOWER RNN <a name='SINGLE_TOWER_RNN'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== SINGLE TOWER RNN ====================\n",
      "\n",
      "--- Starting Training (SINGLE) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47abbcb047544e8ca4ae0ad703e592bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training: loss=0.8758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50eb7cfc3f5d421abed48c78995a0320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6317\n",
      "Validation F1 Score: 0.4961\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.6025    0.0198    0.0384      4894\n",
      " Relevance 1     0.6319    0.9956    0.7731     12592\n",
      " Relevance 2     0.0000    0.0000    0.0000      2514\n",
      "\n",
      "    accuracy                         0.6317     20000\n",
      "   macro avg     0.4115    0.3385    0.2705     20000\n",
      "weighted avg     0.5453    0.6317    0.4961     20000\n",
      "\n",
      "save best model to best_single_tower_rnn.pth (Val F1: 0.4961)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6568bd521054a209da220ab2f74226e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training: loss=0.8417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923e2143dc0a4be0bc352c88086c9387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6408\n",
      "Validation F1 Score: 0.5610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5219    0.2260    0.3154      4894\n",
      " Relevance 1     0.6548    0.9299    0.7685     12592\n",
      " Relevance 2     0.0000    0.0000    0.0000      2514\n",
      "\n",
      "    accuracy                         0.6408     20000\n",
      "   macro avg     0.3923    0.3853    0.3613     20000\n",
      "weighted avg     0.5400    0.6408    0.5610     20000\n",
      "\n",
      "save best model to best_single_tower_rnn.pth (Val F1: 0.5610)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d862be9ee04b9b9201f856b13d3664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training: loss=0.8206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501a885c872344cd9fe2a14101ca6ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6475\n",
      "Validation F1 Score: 0.5732\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5456    0.2638    0.3556      4894\n",
      " Relevance 1     0.6614    0.9256    0.7715     12592\n",
      " Relevance 2     0.4545    0.0020    0.0040      2514\n",
      "\n",
      "    accuracy                         0.6475     20000\n",
      "   macro avg     0.5538    0.3971    0.3770     20000\n",
      "weighted avg     0.6070    0.6475    0.5732     20000\n",
      "\n",
      "save best model to best_single_tower_rnn.pth (Val F1: 0.5732)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b16562a60c4e8ebdaf70e791b5ec81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training: loss=0.8060\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66f75a522bd418691f38066c8f853ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6358\n",
      "Validation F1 Score: 0.5846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.4818    0.3897    0.4309      4894\n",
      " Relevance 1     0.6749    0.8549    0.7543     12592\n",
      " Relevance 2     0.4835    0.0175    0.0338      2514\n",
      "\n",
      "    accuracy                         0.6358     20000\n",
      "   macro avg     0.5467    0.4207    0.4063     20000\n",
      "weighted avg     0.6036    0.6358    0.5846     20000\n",
      "\n",
      "save best model to best_single_tower_rnn.pth (Val F1: 0.5846)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4403280a1f4122a61f5ee6295c443a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Training: loss=0.7935\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e1ad6a83244a3681f34c6b66b0b4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6430\n",
      "Validation F1 Score: 0.5952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5314    0.2540    0.3437      4894\n",
      " Relevance 1     0.6696    0.8899    0.7642     12592\n",
      " Relevance 2     0.4428    0.1631    0.2384      2514\n",
      "\n",
      "    accuracy                         0.6430     20000\n",
      "   macro avg     0.5479    0.4357    0.4488     20000\n",
      "weighted avg     0.6073    0.6430    0.5952     20000\n",
      "\n",
      "save best model to best_single_tower_rnn.pth (Val F1: 0.5952)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10efec64f3e047a8a5a6beb0a4608651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Training: loss=0.7862\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a350e57b262e4be692aea11290d16da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6462\n",
      "Validation F1 Score: 0.6030\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5257    0.3222    0.3995      4894\n",
      " Relevance 1     0.6757    0.8761    0.7630     12592\n",
      " Relevance 2     0.4674    0.1253    0.1976      2514\n",
      "\n",
      "    accuracy                         0.6462     20000\n",
      "   macro avg     0.5563    0.4412    0.4534     20000\n",
      "weighted avg     0.6128    0.6462    0.6030     20000\n",
      "\n",
      "save best model to best_single_tower_rnn.pth (Val F1: 0.6030)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d785fda1a91b488893467db259fad0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Training: loss=0.7795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144a6ea3c60a45de9a8880020ef57821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6412\n",
      "Validation F1 Score: 0.6082\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5071    0.3784    0.4334      4894\n",
      " Relevance 1     0.6845    0.8437    0.7558     12592\n",
      " Relevance 2     0.4215    0.1388    0.2089      2514\n",
      "\n",
      "    accuracy                         0.6412     20000\n",
      "   macro avg     0.5377    0.4537    0.4660     20000\n",
      "weighted avg     0.6081    0.6412    0.6082     20000\n",
      "\n",
      "save best model to best_single_tower_rnn.pth (Val F1: 0.6082)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a294dbfc694ab39e91e1c8305f906f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Training: loss=0.7710\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9e83124d6042fb83f7363735c0e1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6448\n",
      "Validation F1 Score: 0.6138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5146    0.3782    0.4360      4894\n",
      " Relevance 1     0.6874    0.8443    0.7578     12592\n",
      " Relevance 2     0.4402    0.1639    0.2388      2514\n",
      "\n",
      "    accuracy                         0.6448     20000\n",
      "   macro avg     0.5474    0.4621    0.4776     20000\n",
      "weighted avg     0.6140    0.6448    0.6138     20000\n",
      "\n",
      "save best model to best_single_tower_rnn.pth (Val F1: 0.6138)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b95499d5c7470e94530aa02caed498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Training: loss=0.7650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb76e7d350b24247bdeba890b28d0868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6462\n",
      "Validation F1 Score: 0.6168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5073    0.4003    0.4475      4894\n",
      " Relevance 1     0.6911    0.8385    0.7577     12592\n",
      " Relevance 2     0.4733    0.1619    0.2413      2514\n",
      "\n",
      "    accuracy                         0.6462     20000\n",
      "   macro avg     0.5572    0.4669    0.4821     20000\n",
      "weighted avg     0.6187    0.6462    0.6168     20000\n",
      "\n",
      "save best model to best_single_tower_rnn.pth (Val F1: 0.6168)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894131194df24d91a0c373db5ff87407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Training: loss=0.7583\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f17fac8ac44844b3d1f26053f04c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6379\n",
      "Validation F1 Score: 0.6156\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.4999    0.4142    0.4530      4894\n",
      " Relevance 1     0.6949    0.8135    0.7496     12592\n",
      " Relevance 2     0.4037    0.1933    0.2614      2514\n",
      "\n",
      "    accuracy                         0.6379     20000\n",
      "   macro avg     0.5328    0.4737    0.4880     20000\n",
      "weighted avg     0.6106    0.6379    0.6156     20000\n",
      "\n",
      "--- Training Finished (SINGLE) in 602.04s ---\n",
      "Best Validation F1 Score (SINGLE): 0.6168\n",
      "\n",
      "Loading best single-tower model for final evaluation...\n",
      "\n",
      "--- Starting Evaluation on Test Set (SINGLE) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b31dca56414d8a8327641f77ac14b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.6440\n",
      "Test Set F1 Score: 0.6167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5082    0.4119    0.4550      1209\n",
      " Relevance 1     0.6938    0.8297    0.7557      3159\n",
      " Relevance 2     0.4174    0.1598    0.2311       632\n",
      "\n",
      "    accuracy                         0.6440      5000\n",
      "   macro avg     0.5398    0.4671    0.4806      5000\n",
      "weighted avg     0.6139    0.6440    0.6167      5000\n",
      "\n",
      "--- Evaluation Finished (SINGLE) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*20 + \" SINGLE TOWER RNN \" + \"=\"*20)\n",
    "single_tower_model = SingleTowerRNN(\n",
    "    vocab_size=vocab_size, embedding_dim=CONFIG[\"embedding_dim\"], hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "    num_classes=CONFIG[\"num_classes\"], rnn_layers=CONFIG[\"rnn_layers\"], dropout_prob=CONFIG[\"dropout_prob\"],\n",
    "    bidirectional=CONFIG[\"rnn_bidirectional\"], rnn_nonlinearity=CONFIG[\"rnn_nonlinearity\"], pad_idx=pad_idx\n",
    ").to(CONFIG[\"device\"])\n",
    "optimizer_single = optim.Adam(single_tower_model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "# Train Single Tower\n",
    "train_model(single_tower_model, 'single', train_loader, val_loader, optimizer_single, criterion,\n",
    "            CONFIG[\"num_epochs\"], CONFIG[\"device\"], CONFIG[\"save_path_single\"])\n",
    "\n",
    "# Evaluate Single Tower\n",
    "print(\"\\nLoading best single-tower model for final evaluation...\")\n",
    "try:\n",
    "    single_tower_model.load_state_dict(torch.load(CONFIG[\"save_path_single\"], map_location=CONFIG[\"device\"]))\n",
    "except Exception as e:\n",
    "    print(f\"Could not load saved single-tower model state: {e}. Evaluating model state after last epoch.\")\n",
    "accuracy_rnn_single, f1_rnn_single, _, _ = evaluate_model(single_tower_model, 'single', test_loader, criterion, CONFIG[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TWO TOWER RNN <a name='TWO_TOWER_RNN'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*20 + \" TWO TOWER RNN \" + \"=\"*20)\n",
    "two_tower_model = TwoTowerRNN(\n",
    "    vocab_size=vocab_size, embedding_dim=CONFIG[\"embedding_dim\"], hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "    num_classes=CONFIG[\"num_classes\"], rnn_layers=CONFIG[\"rnn_layers\"], dropout_prob=CONFIG[\"dropout_prob\"],\n",
    "    bidirectional=CONFIG[\"rnn_bidirectional\"], rnn_nonlinearity=CONFIG[\"rnn_nonlinearity\"], pad_idx=pad_idx\n",
    ").to(CONFIG[\"device\"])\n",
    "optimizer_two = optim.Adam(two_tower_model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "# Train Two Tower\n",
    "train_model(two_tower_model, 'two', train_loader, val_loader, optimizer_two, criterion,\n",
    "            CONFIG[\"num_epochs\"], CONFIG[\"device\"], CONFIG[\"save_path_two\"])\n",
    "\n",
    "# Evaluate Two Tower\n",
    "print(\"\\nLoading best two-tower model for final evaluation...\")\n",
    "try:\n",
    "    two_tower_model.load_state_dict(torch.load(CONFIG[\"save_path_two\"], map_location=CONFIG[\"device\"]))\n",
    "except Exception as e:\n",
    "      print(f\"Could not load saved two-tower model state: {e}. Evaluating model state after last epoch.\")\n",
    "accuracy_rnn_two, f1_rnn_two, _, _ = evaluate_model(two_tower_model, 'two', test_loader, criterion, CONFIG[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2524.9415\n",
      "Validation Accuracy: 0.6296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000      4894\n",
      "           1     0.6296    1.0000    0.7727     12592\n",
      "           2     0.0000    0.0000    0.0000      2514\n",
      "\n",
      "    accuracy                         0.6296     20000\n",
      "   macro avg     0.2099    0.3333    0.2576     20000\n",
      "weighted avg     0.3964    0.6296    0.4865     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2515.7200\n",
      "Validation Accuracy: 0.6296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000      4894\n",
      "           1     0.6296    1.0000    0.7727     12592\n",
      "           2     0.0000    0.0000    0.0000      2514\n",
      "\n",
      "    accuracy                         0.6296     20000\n",
      "   macro avg     0.2099    0.3333    0.2576     20000\n",
      "weighted avg     0.3964    0.6296    0.4865     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2510.1681\n",
      "Validation Accuracy: 0.6296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.0002    0.0004      4894\n",
      "           1     0.6296    0.9999    0.7727     12592\n",
      "           2     0.0000    0.0000    0.0000      2514\n",
      "\n",
      "    accuracy                         0.6296     20000\n",
      "   macro avg     0.3765    0.3334    0.2577     20000\n",
      "weighted avg     0.5188    0.6296    0.4866     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2505.7881\n",
      "Validation Accuracy: 0.6301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.0025    0.0049      4894\n",
      "           1     0.6301    0.9994    0.7729     12592\n",
      "           2     0.8571    0.0024    0.0048      2514\n",
      "\n",
      "    accuracy                         0.6301     20000\n",
      "   macro avg     0.6957    0.3347    0.2608     20000\n",
      "weighted avg     0.6512    0.6301    0.4884     20000\n",
      "\n",
      "Epoch 5, Loss: 2502.9500\n",
      "Validation Accuracy: 0.62985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5769    0.0031    0.0061      4894\n",
      "           1     0.6303    0.9975    0.7725     12592\n",
      "           2     0.4565    0.0084    0.0164      2514\n",
      "\n",
      "    accuracy                         0.6299     20000\n",
      "   macro avg     0.5546    0.3363    0.2650     20000\n",
      "weighted avg     0.5954    0.6299    0.4899     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "dev_loader = DataLoader(TensorDataset(X_dev, y_dev), batch_size=64)\n",
    "test_loader = DataLoader(TensorDataset(X_test), batch_size=64)\n",
    "\n",
    "model = TextCNN(embedding_matrix, num_classes=3)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dev_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            logits = model(batch_X)\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            truths.extend(batch_y.numpy())\n",
    "    print(\"Validation Accuracy:\", accuracy_score(truths, preds))\n",
    "    print(classification_report(truths, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.4 <a name='5.4'></a>LSTM(Single Tower+Tow Tower)\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](img/lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to basic RNN models, models using LSTM are generally more effective at handling longer text sequences and capturing long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model Definition <a name='Model_Definition'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTowerLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,\n",
    "                 rnn_layers, dropout_prob, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        lstm_hidden_dim = hidden_dim // 2 if bidirectional else hidden_dim\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.rnn = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=rnn_layers,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout_prob if rnn_layers > 1 else 0,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # FC layer input dim is: total hidden_dim (num_directions * lstm_hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.num_directions = num_directions\n",
    "\n",
    "    def forward(self, combined_indices):\n",
    "        # combined_indices: (batch_size, max_combined_len)\n",
    "        embedded = self.dropout(self.embedding(combined_indices))\n",
    "        # output shape: (batch_size, seq_len, num_directions * lstm_hidden_dim)\n",
    "        # hidden_n shape: (num_layers * num_directions, batch_size, lstm_hidden_dim)\n",
    "        # cell_n shape:   (num_layers * num_directions, batch_size, lstm_hidden_dim)\n",
    "        output, (hidden_n, cell_n) = self.rnn(embedded)\n",
    "        final_hidden = _extract_final_hidden(hidden_n, self.rnn_layers, self.num_directions, combined_indices.size(0))\n",
    "        # final_hidden: (batch_size, hidden_dim)\n",
    "\n",
    "        logits = self.fc(self.dropout(final_hidden))\n",
    "        return logits\n",
    "\n",
    "class TwoTowerLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,\n",
    "                 rnn_layers, dropout_prob, bidirectional, pad_idx): # Removed rnn_nonlinearity\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        lstm_hidden_dim = hidden_dim // 2 if bidirectional else hidden_dim\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # Use LSTM layers for query and title\n",
    "        self.rnn_query = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=rnn_layers,\n",
    "                                 batch_first=True,\n",
    "                                 dropout=dropout_prob if rnn_layers > 1 else 0,\n",
    "                                 bidirectional=bidirectional)\n",
    "        self.rnn_title = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=rnn_layers,\n",
    "                                 batch_first=True,\n",
    "                                 dropout=dropout_prob if rnn_layers > 1 else 0,\n",
    "                                 bidirectional=bidirectional)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Input to FC layer is concatenation of final hidden states (h_n) from both towers\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes) # hidden_dim * 2 because of two towers\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.num_directions = num_directions\n",
    "\n",
    "    def forward(self, query_indices, title_indices):\n",
    "        # query_indices: (batch_size, max_query_len)\n",
    "        # title_indices: (batch_size, max_title_len)\n",
    "\n",
    "        query_embedded = self.dropout(self.embedding(query_indices))\n",
    "        title_embedded = self.dropout(self.embedding(title_indices))\n",
    "\n",
    "        _, (query_hidden_n, query_cell_n) = self.rnn_query(query_embedded)\n",
    "        _, (title_hidden_n, title_cell_n) = self.rnn_title(title_embedded)\n",
    "        # hidden_n shapes: (num_layers * num_directions, batch_size, lstm_hidden_dim)\n",
    "\n",
    "        query_final_hidden = _extract_final_hidden(query_hidden_n, self.rnn_layers, self.num_directions, query_indices.size(0))\n",
    "        title_final_hidden = _extract_final_hidden(title_hidden_n, self.rnn_layers, self.num_directions, title_indices.size(0))\n",
    "        # final hidden shapes: (batch_size, hidden_dim)\n",
    "\n",
    "        # Concatenate the final hidden states\n",
    "        combined = torch.cat((query_final_hidden, title_final_hidden), dim=1)\n",
    "        # combined: (batch_size, hidden_dim * 2)\n",
    "\n",
    "        logits = self.fc(self.dropout(combined))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SINGLE TOWER LSTM <a name='SINGLE_TOWER_LSTM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== SINGLE TOWER LSTM ====================\n",
      "\n",
      "--- Starting Training (SINGLE) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdd12449cf748f282e6db1d3bf6c141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training: loss=0.8401\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011ea2476d0b48e6a020f3db4cb9a159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6462\n",
      "Validation F1 Score: 0.5908\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5175    0.3020    0.3814      4894\n",
      " Relevance 1     0.6701    0.8956    0.7666     12592\n",
      " Relevance 2     0.5335    0.0664    0.1181      2514\n",
      "\n",
      "    accuracy                         0.6462     20000\n",
      "   macro avg     0.5737    0.4214    0.4221     20000\n",
      "weighted avg     0.6156    0.6462    0.5908     20000\n",
      "\n",
      "save best model to best_single_tower_lstm.pth (Val F1: 0.5908)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62043cdc81f84ad89efed8d2bb0a48fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training: loss=0.7912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ce7a7a4d3548178ee9346285a2b7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6513\n",
      "Validation F1 Score: 0.6079\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5271    0.3441    0.4164      4894\n",
      " Relevance 1     0.6810    0.8783    0.7672     12592\n",
      " Relevance 2     0.4991    0.1122    0.1832      2514\n",
      "\n",
      "    accuracy                         0.6513     20000\n",
      "   macro avg     0.5691    0.4449    0.4556     20000\n",
      "weighted avg     0.6205    0.6513    0.6079     20000\n",
      "\n",
      "save best model to best_single_tower_lstm.pth (Val F1: 0.6079)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608b72385e4245228b67355872a6415f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training: loss=0.7660\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa225359ce947f0a27592cd46db83b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6558\n",
      "Validation F1 Score: 0.6201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5571    0.2989    0.3891      4894\n",
      " Relevance 1     0.6844    0.8769    0.7688     12592\n",
      " Relevance 2     0.4915    0.2426    0.3249      2514\n",
      "\n",
      "    accuracy                         0.6558     20000\n",
      "   macro avg     0.5777    0.4728    0.4943     20000\n",
      "weighted avg     0.6290    0.6558    0.6201     20000\n",
      "\n",
      "save best model to best_single_tower_lstm.pth (Val F1: 0.6201)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fe9d59d1a64ab699e7fc974590721a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training: loss=0.7474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5361d79589774456a3adeee022c842ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6585\n",
      "Validation F1 Score: 0.6192\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5711    0.2846    0.3799      4894\n",
      " Relevance 1     0.6829    0.8888    0.7724     12592\n",
      " Relevance 2     0.4996    0.2331    0.3179      2514\n",
      "\n",
      "    accuracy                         0.6585     20000\n",
      "   macro avg     0.5845    0.4688    0.4901     20000\n",
      "weighted avg     0.6325    0.6585    0.6192     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3c3fc13d26495c923d8a4acb021e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Training: loss=0.7313\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b605d894ee4410a050f70396686313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6590\n",
      "Validation F1 Score: 0.6349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5474    0.3682    0.4403      4894\n",
      " Relevance 1     0.6985    0.8475    0.7658     12592\n",
      " Relevance 2     0.4941    0.2808    0.3581      2514\n",
      "\n",
      "    accuracy                         0.6590     20000\n",
      "   macro avg     0.5800    0.4989    0.5214     20000\n",
      "weighted avg     0.6358    0.6590    0.6349     20000\n",
      "\n",
      "save best model to best_single_tower_lstm.pth (Val F1: 0.6349)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215d7ed84a01451cbe18a10c371ae7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Training: loss=0.7174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b089a5151b174bedbed91a4b74d33a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6585\n",
      "Validation F1 Score: 0.6294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5512    0.3396    0.4203      4894\n",
      " Relevance 1     0.6928    0.8621    0.7682     12592\n",
      " Relevance 2     0.4966    0.2601    0.3414      2514\n",
      "\n",
      "    accuracy                         0.6585     20000\n",
      "   macro avg     0.5802    0.4873    0.5100     20000\n",
      "weighted avg     0.6335    0.6585    0.6294     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f88d5ba90c64bc28f8a556ada5acd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Training: loss=0.7051\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df328871de643d28242d1fb2baba1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6562\n",
      "Validation F1 Score: 0.6378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5266    0.4422    0.4807      4894\n",
      " Relevance 1     0.7068    0.8194    0.7590     12592\n",
      " Relevance 2     0.4957    0.2550    0.3367      2514\n",
      "\n",
      "    accuracy                         0.6562     20000\n",
      "   macro avg     0.5764    0.5055    0.5255     20000\n",
      "weighted avg     0.6362    0.6562    0.6378     20000\n",
      "\n",
      "save best model to best_single_tower_lstm.pth (Val F1: 0.6378)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5954365c4f94ff882a0ebbf4e93966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Training: loss=0.6939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b3b60a9161472f81166311f9f55544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6567\n",
      "Validation F1 Score: 0.6360\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5326    0.4119    0.4646      4894\n",
      " Relevance 1     0.7031    0.8312    0.7618     12592\n",
      " Relevance 2     0.4910    0.2593    0.3394      2514\n",
      "\n",
      "    accuracy                         0.6567     20000\n",
      "   macro avg     0.5756    0.5008    0.5219     20000\n",
      "weighted avg     0.6347    0.6567    0.6360     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82fcaa9d0c64cefbb8fc87df37a0346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Training: loss=0.6851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf903aaed804ba1b298918f902e4459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6541\n",
      "Validation F1 Score: 0.6375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5197    0.4555    0.4855      4894\n",
      " Relevance 1     0.7080    0.8102    0.7557     12592\n",
      " Relevance 2     0.5000    0.2589    0.3412      2514\n",
      "\n",
      "    accuracy                         0.6541     20000\n",
      "   macro avg     0.5759    0.5082    0.5274     20000\n",
      "weighted avg     0.6358    0.6541    0.6375     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9629fdac64a46a88679b206eeba112f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Training: loss=0.6759\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecae6b261a943978e467f996d0f1dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6560\n",
      "Validation F1 Score: 0.6406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5286    0.4397    0.4801      4894\n",
      " Relevance 1     0.7104    0.8123    0.7580     12592\n",
      " Relevance 2     0.4824    0.2936    0.3650      2514\n",
      "\n",
      "    accuracy                         0.6560     20000\n",
      "   macro avg     0.5738    0.5152    0.5343     20000\n",
      "weighted avg     0.6373    0.6560    0.6406     20000\n",
      "\n",
      "save best model to best_single_tower_lstm.pth (Val F1: 0.6406)\n",
      "--- Training Finished (SINGLE) in 635.59s ---\n",
      "Best Validation F1 Score (SINGLE): 0.6406\n",
      "\n",
      "Loading best single-tower LSTM model for final evaluation...\n",
      "\n",
      "--- Starting Evaluation on Test Set (SINGLE) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a27bf8ba41495babd714b167fb6b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.6634\n",
      "Test Set F1 Score: 0.6487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5357    0.4467    0.4871      1209\n",
      " Relevance 1     0.7185    0.8177    0.7649      3159\n",
      " Relevance 2     0.4887    0.3070    0.3771       632\n",
      "\n",
      "    accuracy                         0.6634      5000\n",
      "   macro avg     0.5810    0.5238    0.5430      5000\n",
      "weighted avg     0.6452    0.6634    0.6487      5000\n",
      "\n",
      "--- Evaluation Finished (SINGLE) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*20 + \" SINGLE TOWER LSTM \" + \"=\"*20)\n",
    "single_tower_lstm_model = SingleTowerLSTM(\n",
    "    vocab_size=vocab_size, embedding_dim=CONFIG[\"embedding_dim\"], hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "    num_classes=CONFIG[\"num_classes\"], rnn_layers=CONFIG[\"rnn_layers\"], dropout_prob=CONFIG[\"dropout_prob\"],\n",
    "    bidirectional=CONFIG[\"rnn_bidirectional\"],\n",
    "    pad_idx=pad_idx\n",
    ").to(CONFIG[\"device\"])\n",
    "optimizer_single_lstm = optim.Adam(single_tower_lstm_model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "# Train Single Tower LSTM\n",
    "train_model(single_tower_lstm_model, 'single', train_loader, val_loader, optimizer_single_lstm, criterion,\n",
    "            CONFIG[\"num_epochs\"], CONFIG[\"device\"], CONFIG[\"save_path_single_lstm\"])\n",
    "\n",
    "# Evaluate Single Tower LSTM\n",
    "print(\"\\nLoading best single-tower LSTM model for final evaluation...\")\n",
    "try:\n",
    "    single_tower_lstm_model.load_state_dict(torch.load(CONFIG[\"save_path_single_lstm\"], map_location=CONFIG[\"device\"]))\n",
    "except Exception as e:\n",
    "    print(f\"Could not load saved single-tower LSTM model state: {e}. Evaluating model state after last epoch.\")\n",
    "accuracy_lstm_single, f1_lstm_single, _, _ = evaluate_model(single_tower_lstm_model, 'single', test_loader, criterion, CONFIG[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TWO TOWER LSTM <a name='TWO_TOWER_LSTM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TWO TOWER LSTM ====================\n",
      "\n",
      "--- Starting Training (TWO) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bcb1afed554d8296195b2372a5155c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training: loss=0.8342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483a78cda2de472a968adadcc8495bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6482\n",
      "Validation F1 Score: 0.6041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5530    0.2366    0.3314      4894\n",
      " Relevance 1     0.6758    0.8901    0.7683     12592\n",
      " Relevance 2     0.4531    0.2383    0.3123      2514\n",
      "\n",
      "    accuracy                         0.6482     20000\n",
      "   macro avg     0.5606    0.4550    0.4707     20000\n",
      "weighted avg     0.6178    0.6482    0.6041     20000\n",
      "\n",
      "save best model to best_two_tower_lstm.pth (Val F1: 0.6041)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccafc84024cc4fdb943df83019dd60c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training: loss=0.7886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9767ad7ec6d4705a4d552e116f89207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6515\n",
      "Validation F1 Score: 0.6171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5614    0.2738    0.3681      4894\n",
      " Relevance 1     0.6862    0.8715    0.7678     12592\n",
      " Relevance 2     0.4426    0.2852    0.3469      2514\n",
      "\n",
      "    accuracy                         0.6515     20000\n",
      "   macro avg     0.5634    0.4768    0.4943     20000\n",
      "weighted avg     0.6250    0.6515    0.6171     20000\n",
      "\n",
      "save best model to best_two_tower_lstm.pth (Val F1: 0.6171)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c941521f800d4e349afcdb6c7f091fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training: loss=0.7632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d60e10bc647aa87665bd1f249a373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6618\n",
      "Validation F1 Score: 0.6260\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5755    0.3202    0.4114      4894\n",
      " Relevance 1     0.6891    0.8829    0.7741     12592\n",
      " Relevance 2     0.4821    0.2196    0.3017      2514\n",
      "\n",
      "    accuracy                         0.6618     20000\n",
      "   macro avg     0.5822    0.4742    0.4957     20000\n",
      "weighted avg     0.6353    0.6618    0.6260     20000\n",
      "\n",
      "save best model to best_two_tower_lstm.pth (Val F1: 0.6260)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc4731d9e0c444f869f52cd04a5dc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training: loss=0.7428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2df5a0541444547a142058bb03e8737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6604\n",
      "Validation F1 Score: 0.6338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5584    0.3555    0.4345      4894\n",
      " Relevance 1     0.6980    0.8574    0.7695     12592\n",
      " Relevance 2     0.4746    0.2677    0.3423      2514\n",
      "\n",
      "    accuracy                         0.6604     20000\n",
      "   macro avg     0.5770    0.4935    0.5154     20000\n",
      "weighted avg     0.6358    0.6604    0.6338     20000\n",
      "\n",
      "save best model to best_two_tower_lstm.pth (Val F1: 0.6338)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e229b4b1f7740bbb5f20864f1b21fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Training: loss=0.7254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f7c15702ae4d7f82c4d7bd3dac5344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6635\n",
      "Validation F1 Score: 0.6306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5769    0.3304    0.4202      4894\n",
      " Relevance 1     0.6928    0.8779    0.7744     12592\n",
      " Relevance 2     0.4835    0.2387    0.3196      2514\n",
      "\n",
      "    accuracy                         0.6635     20000\n",
      "   macro avg     0.5844    0.4823    0.5047     20000\n",
      "weighted avg     0.6381    0.6635    0.6306     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd0ce6634bd4c1098e76cd8c4e60c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Training: loss=0.7109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a231020e61430b9106afb48a29dc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6634\n",
      "Validation F1 Score: 0.6425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5525    0.3948    0.4605      4894\n",
      " Relevance 1     0.7063    0.8416    0.7680     12592\n",
      " Relevance 2     0.4927    0.2940    0.3682      2514\n",
      "\n",
      "    accuracy                         0.6634     20000\n",
      "   macro avg     0.5838    0.5101    0.5322     20000\n",
      "weighted avg     0.6418    0.6634    0.6425     20000\n",
      "\n",
      "save best model to best_two_tower_lstm.pth (Val F1: 0.6425)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bcca36b034403e98a89fe35dbc7e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Training: loss=0.6975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48b68c004934c83b60e0726242ef7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6636\n",
      "Validation F1 Score: 0.6404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5826    0.3562    0.4420      4894\n",
      " Relevance 1     0.7031    0.8524    0.7706     12592\n",
      " Relevance 2     0.4575    0.3170    0.3745      2514\n",
      "\n",
      "    accuracy                         0.6636     20000\n",
      "   macro avg     0.5810    0.5085    0.5290     20000\n",
      "weighted avg     0.6427    0.6636    0.6404     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57266f137ec64b048f3953f9a73dd164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Training: loss=0.6864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c59aeee8a74d26b70c4d314d27e829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6667\n",
      "Validation F1 Score: 0.6453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5695    0.3954    0.4667      4894\n",
      " Relevance 1     0.7080    0.8472    0.7714     12592\n",
      " Relevance 2     0.4762    0.2908    0.3611      2514\n",
      "\n",
      "    accuracy                         0.6667     20000\n",
      "   macro avg     0.5846    0.5111    0.5331     20000\n",
      "weighted avg     0.6450    0.6667    0.6453     20000\n",
      "\n",
      "save best model to best_two_tower_lstm.pth (Val F1: 0.6453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4622c007774aebb3b366a728716147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Training: loss=0.6748\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43c9e06203e4461967926094ce31dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6626\n",
      "Validation F1 Score: 0.6371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5627    0.3833    0.4560      4894\n",
      " Relevance 1     0.7007    0.8540    0.7698     12592\n",
      " Relevance 2     0.4716    0.2474    0.3245      2514\n",
      "\n",
      "    accuracy                         0.6626     20000\n",
      "   macro avg     0.5783    0.4949    0.5168     20000\n",
      "weighted avg     0.6381    0.6626    0.6371     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4847aae1d50b4a599c2a5ed6a08c8e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Training: loss=0.6660\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57509d0620c48b48af8fea5ab86d9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6643\n",
      "Validation F1 Score: 0.6445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5712    0.3911    0.4643      4894\n",
      " Relevance 1     0.7085    0.8407    0.7689     12592\n",
      " Relevance 2     0.4599    0.3123    0.3719      2514\n",
      "\n",
      "    accuracy                         0.6643     20000\n",
      "   macro avg     0.5798    0.5147    0.5351     20000\n",
      "weighted avg     0.6436    0.6643    0.6445     20000\n",
      "\n",
      "--- Training Finished (TWO) in 670.58s ---\n",
      "Best Validation F1 Score (TWO): 0.6453\n",
      "\n",
      "Loading best two-tower LSTM model for final evaluation...\n",
      "\n",
      "--- Starting Evaluation on Test Set (TWO) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eca87b6af7d49a695d6eb03647a5eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.6646\n",
      "Test Set F1 Score: 0.6434\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5707    0.3937    0.4660      1209\n",
      " Relevance 1     0.7057    0.8433    0.7684      3159\n",
      " Relevance 2     0.4680    0.2896    0.3578       632\n",
      "\n",
      "    accuracy                         0.6646      5000\n",
      "   macro avg     0.5815    0.5089    0.5307      5000\n",
      "weighted avg     0.6430    0.6646    0.6434      5000\n",
      "\n",
      "--- Evaluation Finished (TWO) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*20 + \" TWO TOWER LSTM \" + \"=\"*20)\n",
    "two_tower_lstm_model = TwoTowerLSTM(\n",
    "    vocab_size=vocab_size, embedding_dim=CONFIG[\"embedding_dim\"], hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "    num_classes=CONFIG[\"num_classes\"], rnn_layers=CONFIG[\"rnn_layers\"], dropout_prob=CONFIG[\"dropout_prob\"],\n",
    "    bidirectional=CONFIG[\"rnn_bidirectional\"],\n",
    "    pad_idx=pad_idx\n",
    ").to(CONFIG[\"device\"])\n",
    "optimizer_two_lstm = optim.Adam(two_tower_lstm_model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "# Train Two Tower LSTM\n",
    "train_model(two_tower_lstm_model, 'two', train_loader, val_loader, optimizer_two_lstm, criterion,\n",
    "            CONFIG[\"num_epochs\"], CONFIG[\"device\"], CONFIG[\"save_path_two_lstm\"])\n",
    "\n",
    "# Evaluate Two Tower LSTM\n",
    "print(\"\\nLoading best two-tower LSTM model for final evaluation...\")\n",
    "try:\n",
    "    two_tower_lstm_model.load_state_dict(torch.load(CONFIG[\"save_path_two_lstm\"], map_location=CONFIG[\"device\"]))\n",
    "except Exception as e:\n",
    "      print(f\"Could not load saved two-tower LSTM model state: {e}. Evaluating model state after last epoch.\")\n",
    "accuracy_lstm_two, f1_lstm_two, _, _ = evaluate_model(two_tower_lstm_model, 'two', test_loader, criterion, CONFIG[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.5 <a name='5.3'></a>GRU(Single Tower+Tow Tower)\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GRU](img/gru.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU's performance is often comparable to LSTM, but due to its simpler structure and fewer parameters, it might offer advantages in training speed and memory consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model Definition <a name='Model_Definition'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTowerGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,\n",
    "                 rnn_layers, dropout_prob, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        gru_hidden_dim = hidden_dim // 2 if bidirectional else hidden_dim\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        # Use nn.GRU layer\n",
    "        self.rnn = nn.GRU(embedding_dim, gru_hidden_dim, num_layers=rnn_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout_prob if rnn_layers > 1 else 0,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes) # Input dim is total hidden_dim\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.num_directions = num_directions\n",
    "\n",
    "    def forward(self, combined_indices):\n",
    "        # combined_indices: (batch_size, max_combined_len)\n",
    "        embedded = self.dropout(self.embedding(combined_indices))\n",
    "        # output shape: (batch_size, seq_len, num_directions * gru_hidden_dim)\n",
    "        # hidden_n shape: (num_layers * num_directions, batch_size, gru_hidden_dim)\n",
    "        output, hidden_n = self.rnn(embedded)\n",
    "        final_hidden = _extract_final_hidden(hidden_n, self.rnn_layers, self.num_directions, combined_indices.size(0))\n",
    "        # final_hidden: (batch_size, hidden_dim)\n",
    "\n",
    "        logits = self.fc(self.dropout(final_hidden))\n",
    "        return logits\n",
    "\n",
    "class TwoTowerGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,\n",
    "                 rnn_layers, dropout_prob, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        gru_hidden_dim = hidden_dim // 2 if bidirectional else hidden_dim\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # Use GRU layers for query and title\n",
    "        self.rnn_query = nn.GRU(embedding_dim, gru_hidden_dim, num_layers=rnn_layers,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout_prob if rnn_layers > 1 else 0,\n",
    "                                bidirectional=bidirectional)\n",
    "        self.rnn_title = nn.GRU(embedding_dim, gru_hidden_dim, num_layers=rnn_layers,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout_prob if rnn_layers > 1 else 0,\n",
    "                                bidirectional=bidirectional)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Input to FC layer is concatenation of final hidden states (h_n) from both towers\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.num_directions = num_directions\n",
    "\n",
    "    def forward(self, query_indices, title_indices):\n",
    "        # query_indices: (batch_size, max_query_len)\n",
    "        # title_indices: (batch_size, max_title_len)\n",
    "\n",
    "        query_embedded = self.dropout(self.embedding(query_indices))\n",
    "        title_embedded = self.dropout(self.embedding(title_indices))\n",
    "\n",
    "        _, query_hidden_n = self.rnn_query(query_embedded)\n",
    "        _, title_hidden_n = self.rnn_title(title_embedded)\n",
    "        # hidden_n shapes: (num_layers * num_directions, batch_size, gru_hidden_dim)\n",
    "\n",
    "        query_final_hidden = _extract_final_hidden(query_hidden_n, self.rnn_layers, self.num_directions, query_indices.size(0))\n",
    "        title_final_hidden = _extract_final_hidden(title_hidden_n, self.rnn_layers, self.num_directions, title_indices.size(0))\n",
    "        # final hidden shapes: (batch_size, hidden_dim)\n",
    "\n",
    "        # Concatenate the final hidden states\n",
    "        combined = torch.cat((query_final_hidden, title_final_hidden), dim=1)\n",
    "        # combined: (batch_size, hidden_dim * 2)\n",
    "\n",
    "        logits = self.fc(self.dropout(combined))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SINGLE TOWER GRU <a name='SINGLE_TOWER_GRU'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== SINGLE TOWER GRU ====================\n",
      "\n",
      "--- Starting Training (SINGLE) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ce69c94dde44b1afd3c3f8b50488d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training: loss=0.8395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38556039be26431e977e6735a23a87e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6505\n",
      "Validation F1 Score: 0.6004\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5572    0.2327    0.3283      4894\n",
      " Relevance 1     0.6717    0.9042    0.7708     12592\n",
      " Relevance 2     0.4831    0.1933    0.2761      2514\n",
      "\n",
      "    accuracy                         0.6505     20000\n",
      "   macro avg     0.5707    0.4434    0.4584     20000\n",
      "weighted avg     0.6200    0.6505    0.6004     20000\n",
      "\n",
      "save best model to best_single_tower_gru.pth (Val F1: 0.6004)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdde88dbf144be1994adb8148b4b831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training: loss=0.7910\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1257915a31d4ca59b63d1f939dbaa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6506\n",
      "Validation F1 Score: 0.6152\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5279    0.3327    0.4081      4894\n",
      " Relevance 1     0.6839    0.8671    0.7646     12592\n",
      " Relevance 2     0.4911    0.1858    0.2696      2514\n",
      "\n",
      "    accuracy                         0.6506     20000\n",
      "   macro avg     0.5676    0.4618    0.4808     20000\n",
      "weighted avg     0.6215    0.6506    0.6152     20000\n",
      "\n",
      "save best model to best_single_tower_gru.pth (Val F1: 0.6152)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a81216df084b568f848bedc3d30ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training: loss=0.7687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7576ad44fe064dbab9cbdeb240711088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6550\n",
      "Validation F1 Score: 0.6206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5562    0.2942    0.3849      4894\n",
      " Relevance 1     0.6852    0.8735    0.7680     12592\n",
      " Relevance 2     0.4867    0.2629    0.3414      2514\n",
      "\n",
      "    accuracy                         0.6550     20000\n",
      "   macro avg     0.5760    0.4769    0.4981     20000\n",
      "weighted avg     0.6287    0.6550    0.6206     20000\n",
      "\n",
      "save best model to best_single_tower_gru.pth (Val F1: 0.6206)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b959c230e11431ebf66ba8fbf995fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training: loss=0.7509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf0b84f759a48a996ca8f64f3533e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6612\n",
      "Validation F1 Score: 0.6255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5809    0.2934    0.3899      4894\n",
      " Relevance 1     0.6872    0.8833    0.7730     12592\n",
      " Relevance 2     0.4952    0.2645    0.3448      2514\n",
      "\n",
      "    accuracy                         0.6612     20000\n",
      "   macro avg     0.5878    0.4804    0.5026     20000\n",
      "weighted avg     0.6371    0.6612    0.6255     20000\n",
      "\n",
      "save best model to best_single_tower_gru.pth (Val F1: 0.6255)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d200b5c6f6454d6795160dce1b482c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Training: loss=0.7359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6358938005d449eab554edaff437fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6600\n",
      "Validation F1 Score: 0.6341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5482    0.3670    0.4397      4894\n",
      " Relevance 1     0.6977    0.8534    0.7677     12592\n",
      " Relevance 2     0.4977    0.2617    0.3431      2514\n",
      "\n",
      "    accuracy                         0.6600     20000\n",
      "   macro avg     0.5812    0.4940    0.5168     20000\n",
      "weighted avg     0.6360    0.6600    0.6341     20000\n",
      "\n",
      "save best model to best_single_tower_gru.pth (Val F1: 0.6341)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e1f619b4484a328a9b2ff0cc895013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Training: loss=0.7243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c110ca9cf04da1a7f256a68228e57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6622\n",
      "Validation F1 Score: 0.6293\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5833    0.2983    0.3948      4894\n",
      " Relevance 1     0.6911    0.8777    0.7733     12592\n",
      " Relevance 2     0.4864    0.2912    0.3643      2514\n",
      "\n",
      "    accuracy                         0.6622     20000\n",
      "   macro avg     0.5869    0.4891    0.5108     20000\n",
      "weighted avg     0.6390    0.6622    0.6293     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fdd7d874d942d0a0054c3540a52735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Training: loss=0.7134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd35bd696d554d78a592c252961080fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6596\n",
      "Validation F1 Score: 0.6412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5430    0.4130    0.4691      4894\n",
      " Relevance 1     0.7072    0.8284    0.7630     12592\n",
      " Relevance 2     0.4840    0.2944    0.3661      2514\n",
      "\n",
      "    accuracy                         0.6596     20000\n",
      "   macro avg     0.5781    0.5119    0.5327     20000\n",
      "weighted avg     0.6390    0.6596    0.6412     20000\n",
      "\n",
      "save best model to best_single_tower_gru.pth (Val F1: 0.6412)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba1030b6d274814a6ad698ca2b5337e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Training: loss=0.7027\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82098d69d8a943968ea35591e03c2a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6589\n",
      "Validation F1 Score: 0.6423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5564    0.3880    0.4572      4894\n",
      " Relevance 1     0.7092    0.8254    0.7629     12592\n",
      " Relevance 2     0.4584    0.3524    0.3985      2514\n",
      "\n",
      "    accuracy                         0.6589     20000\n",
      "   macro avg     0.5747    0.5219    0.5395     20000\n",
      "weighted avg     0.6403    0.6589    0.6423     20000\n",
      "\n",
      "save best model to best_single_tower_gru.pth (Val F1: 0.6423)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9631bf0b943c42a5aea88d79f73c6354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Training: loss=0.6946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4f00ebf8444e65a3cec24ce76fd64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6602\n",
      "Validation F1 Score: 0.6455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5391    0.4324    0.4799      4894\n",
      " Relevance 1     0.7145    0.8166    0.7621     12592\n",
      " Relevance 2     0.4780    0.3202    0.3835      2514\n",
      "\n",
      "    accuracy                         0.6602     20000\n",
      "   macro avg     0.5772    0.5230    0.5418     20000\n",
      "weighted avg     0.6418    0.6602    0.6455     20000\n",
      "\n",
      "save best model to best_single_tower_gru.pth (Val F1: 0.6455)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b763c803b94598aac3a886d4314eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Training: loss=0.6857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b394f1c054494f89997504fab48f086b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6607\n",
      "Validation F1 Score: 0.6398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5533    0.3915    0.4585      4894\n",
      " Relevance 1     0.7046    0.8395    0.7662     12592\n",
      " Relevance 2     0.4743    0.2896    0.3596      2514\n",
      "\n",
      "    accuracy                         0.6607     20000\n",
      "   macro avg     0.5774    0.5069    0.5281     20000\n",
      "weighted avg     0.6386    0.6607    0.6398     20000\n",
      "\n",
      "--- Training Finished (SINGLE) in 638.16s ---\n",
      "Best Validation F1 Score (SINGLE): 0.6455\n",
      "\n",
      "Loading best single-tower GRU model for final evaluation...\n",
      "\n",
      "--- Starting Evaluation on Test Set (SINGLE) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42b3446bc954b9c883cb1f47bdd9d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.6640\n",
      "Test Set F1 Score: 0.6497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5445    0.4351    0.4837      1209\n",
      " Relevance 1     0.7197    0.8192    0.7662      3159\n",
      " Relevance 2     0.4703    0.3259    0.3850       632\n",
      "\n",
      "    accuracy                         0.6640      5000\n",
      "   macro avg     0.5782    0.5268    0.5450      5000\n",
      "weighted avg     0.6458    0.6640    0.6497      5000\n",
      "\n",
      "--- Evaluation Finished (SINGLE) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*20 + \" SINGLE TOWER GRU \" + \"=\"*20)\n",
    "single_tower_gru_model = SingleTowerGRU(\n",
    "    vocab_size=vocab_size, embedding_dim=CONFIG[\"embedding_dim\"], hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "    num_classes=CONFIG[\"num_classes\"], rnn_layers=CONFIG[\"rnn_layers\"], dropout_prob=CONFIG[\"dropout_prob\"],\n",
    "    bidirectional=CONFIG[\"rnn_bidirectional\"],\n",
    "    pad_idx=pad_idx\n",
    ").to(CONFIG[\"device\"])\n",
    "optimizer_single_gru = optim.Adam(single_tower_gru_model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "# Train Single Tower GRU\n",
    "train_model(single_tower_gru_model, 'single', train_loader, val_loader, optimizer_single_gru, criterion,\n",
    "            CONFIG[\"num_epochs\"], CONFIG[\"device\"], CONFIG[\"save_path_single_gru\"])\n",
    "\n",
    "# Evaluate Single Tower GRU\n",
    "print(\"\\nLoading best single-tower GRU model for final evaluation...\")\n",
    "try:\n",
    "    single_tower_gru_model.load_state_dict(torch.load(CONFIG[\"save_path_single_gru\"], map_location=CONFIG[\"device\"]))\n",
    "except Exception as e:\n",
    "    print(f\"Could not load saved single-tower GRU model state: {e}. Evaluating model state after last epoch.\")\n",
    "accuracy_gru_single, f1_gru_single, _, _ = evaluate_model(single_tower_gru_model, 'single', test_loader, criterion, CONFIG[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TWO TOWER GRU <a name='TWO_TOWER_GRU'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TWO TOWER GRU ====================\n",
      "\n",
      "--- Starting Training (TWO) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318fde5682a14907b6d677a545576645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training: loss=0.8347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4669f2e7b1dd40b1a77e00d86f973899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6507\n",
      "Validation F1 Score: 0.5941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5598    0.2131    0.3087      4894\n",
      " Relevance 1     0.6693    0.9171    0.7739     12592\n",
      " Relevance 2     0.4785    0.1683    0.2490      2514\n",
      "\n",
      "    accuracy                         0.6507     20000\n",
      "   macro avg     0.5692    0.4328    0.4439     20000\n",
      "weighted avg     0.6186    0.6507    0.5941     20000\n",
      "\n",
      "save best model to best_two_tower_gru.pth (Val F1: 0.5941)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef734eba0bd04721b84a47dcadd1a180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training: loss=0.7873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adc42834b4c4fa78548b6543caac645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6550\n",
      "Validation F1 Score: 0.6231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5555    0.2934    0.3840      4894\n",
      " Relevance 1     0.6895    0.8677    0.7684     12592\n",
      " Relevance 2     0.4700    0.2932    0.3611      2514\n",
      "\n",
      "    accuracy                         0.6550     20000\n",
      "   macro avg     0.5717    0.4848    0.5045     20000\n",
      "weighted avg     0.6291    0.6550    0.6231     20000\n",
      "\n",
      "save best model to best_two_tower_gru.pth (Val F1: 0.6231)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae61ba7f0354a738438c0244fb2987f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training: loss=0.7628\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ecd49ed84647dca543836938d25197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6584\n",
      "Validation F1 Score: 0.6263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5595    0.3163    0.4041      4894\n",
      " Relevance 1     0.6901    0.8710    0.7701     12592\n",
      " Relevance 2     0.4877    0.2597    0.3390      2514\n",
      "\n",
      "    accuracy                         0.6584     20000\n",
      "   macro avg     0.5791    0.4824    0.5044     20000\n",
      "weighted avg     0.6327    0.6584    0.6263     20000\n",
      "\n",
      "save best model to best_two_tower_gru.pth (Val F1: 0.6263)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297952cbbcd44c3f8ed191a999bdc29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training: loss=0.7443\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ba4f41694d44be8046961ede03ef24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6622\n",
      "Validation F1 Score: 0.6304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5891    0.2924    0.3908      4894\n",
      " Relevance 1     0.6938    0.8750    0.7739     12592\n",
      " Relevance 2     0.4698    0.3158    0.3777      2514\n",
      "\n",
      "    accuracy                         0.6622     20000\n",
      "   macro avg     0.5842    0.4944    0.5142     20000\n",
      "weighted avg     0.6400    0.6622    0.6304     20000\n",
      "\n",
      "save best model to best_two_tower_gru.pth (Val F1: 0.6304)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e47a2c305a74968b79cdbdcf677ee9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Training: loss=0.7270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c365c492a97c49eaab02740590c55d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6635\n",
      "Validation F1 Score: 0.6306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5661    0.3641    0.4432      4894\n",
      " Relevance 1     0.6938    0.8729    0.7731     12592\n",
      " Relevance 2     0.4921    0.1977    0.2821      2514\n",
      "\n",
      "    accuracy                         0.6635     20000\n",
      "   macro avg     0.5840    0.4782    0.4994     20000\n",
      "weighted avg     0.6372    0.6635    0.6306     20000\n",
      "\n",
      "save best model to best_two_tower_gru.pth (Val F1: 0.6306)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7813ab5a667847f8941b47f804f89e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Training: loss=0.7129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841f470deb39444591c5a1ecbbf408a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6637\n",
      "Validation F1 Score: 0.6422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5665    0.3754    0.4515      4894\n",
      " Relevance 1     0.7049    0.8459    0.7690     12592\n",
      " Relevance 2     0.4778    0.3126    0.3780      2514\n",
      "\n",
      "    accuracy                         0.6637     20000\n",
      "   macro avg     0.5830    0.5113    0.5328     20000\n",
      "weighted avg     0.6425    0.6637    0.6422     20000\n",
      "\n",
      "save best model to best_two_tower_gru.pth (Val F1: 0.6422)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf9b6c9c612445682553f9b435a1245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Training: loss=0.7009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715225fa852f49cda6c46a9a686ef6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6635\n",
      "Validation F1 Score: 0.6433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5544    0.4291    0.4838      4894\n",
      " Relevance 1     0.7098    0.8359    0.7677     12592\n",
      " Relevance 2     0.4657    0.2562    0.3305      2514\n",
      "\n",
      "    accuracy                         0.6635     20000\n",
      "   macro avg     0.5766    0.5071    0.5273     20000\n",
      "weighted avg     0.6411    0.6635    0.6433     20000\n",
      "\n",
      "save best model to best_two_tower_gru.pth (Val F1: 0.6433)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bdf95153e542d283e18901112bcdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Training: loss=0.6886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51e13b5ba96407bac52d6f847dc085a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6641\n",
      "Validation F1 Score: 0.6466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5604    0.4150    0.4769      4894\n",
      " Relevance 1     0.7116    0.8314    0.7669     12592\n",
      " Relevance 2     0.4697    0.3111    0.3743      2514\n",
      "\n",
      "    accuracy                         0.6641     20000\n",
      "   macro avg     0.5806    0.5192    0.5393     20000\n",
      "weighted avg     0.6442    0.6641    0.6466     20000\n",
      "\n",
      "save best model to best_two_tower_gru.pth (Val F1: 0.6466)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d038c2ef0d084a5589af2e7d51359a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Training: loss=0.6803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46ed2b7ff824c798438195382976632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6677\n",
      "Validation F1 Score: 0.6435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5911    0.3625    0.4494      4894\n",
      " Relevance 1     0.7029    0.8586    0.7730     12592\n",
      " Relevance 2     0.4753    0.3059    0.3722      2514\n",
      "\n",
      "    accuracy                         0.6677     20000\n",
      "   macro avg     0.5898    0.5090    0.5315     20000\n",
      "weighted avg     0.6470    0.6677    0.6435     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc23f101d6440f5916d07038fb26dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Training:   0%|          | 0/2813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Training: loss=0.6704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1247eede9e934a27b67cd09e17a7be14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6658\n",
      "Validation F1 Score: 0.6419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5866    0.3792    0.4607      4894\n",
      " Relevance 1     0.7031    0.8544    0.7714     12592\n",
      " Relevance 2     0.4573    0.2788    0.3464      2514\n",
      "\n",
      "    accuracy                         0.6658     20000\n",
      "   macro avg     0.5823    0.5042    0.5262     20000\n",
      "weighted avg     0.6437    0.6658    0.6419     20000\n",
      "\n",
      "--- Training Finished (TWO) in 661.87s ---\n",
      "Best Validation F1 Score (TWO): 0.6466\n",
      "\n",
      "Loading best two-tower GRU model for final evaluation...\n",
      "\n",
      "--- Starting Evaluation on Test Set (TWO) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fda56353434a26a00aab9ef7d16b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.6708\n",
      "Test Set F1 Score: 0.6533\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Relevance 0     0.5727    0.4202    0.4847      1209\n",
      " Relevance 1     0.7158    0.8370    0.7716      3159\n",
      " Relevance 2     0.4821    0.3196    0.3844       632\n",
      "\n",
      "    accuracy                         0.6708      5000\n",
      "   macro avg     0.5902    0.5256    0.5469      5000\n",
      "weighted avg     0.6516    0.6708    0.6533      5000\n",
      "\n",
      "--- Evaluation Finished (TWO) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*20 + \" TWO TOWER GRU \" + \"=\"*20)\n",
    "two_tower_gru_model = TwoTowerGRU(\n",
    "    vocab_size=vocab_size, embedding_dim=CONFIG[\"embedding_dim\"], hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "    num_classes=CONFIG[\"num_classes\"], rnn_layers=CONFIG[\"rnn_layers\"], dropout_prob=CONFIG[\"dropout_prob\"],\n",
    "    bidirectional=CONFIG[\"rnn_bidirectional\"],\n",
    "    pad_idx=pad_idx\n",
    ").to(CONFIG[\"device\"])\n",
    "optimizer_two_gru = optim.Adam(two_tower_gru_model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "# Train Two Tower GRU\n",
    "train_model(two_tower_gru_model, 'two', train_loader, val_loader, optimizer_two_gru, criterion,\n",
    "            CONFIG[\"num_epochs\"], CONFIG[\"device\"], CONFIG[\"save_path_two_gru\"])\n",
    "\n",
    "# Evaluate Two Tower GRU\n",
    "print(\"\\nLoading best two-tower GRU model for final evaluation...\")\n",
    "try:\n",
    "    two_tower_gru_model.load_state_dict(torch.load(CONFIG[\"save_path_two_gru\"], map_location=CONFIG[\"device\"]))\n",
    "except Exception as e:\n",
    "      print(f\"Could not load saved two-tower GRU model state: {e}. Evaluating model state after last epoch.\")\n",
    "accuracy_gru_two, f1_gru_two, _, _ = evaluate_model(two_tower_gru_model, 'two', test_loader, criterion, CONFIG[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.6 <a name='5.6'></a>Transformer Model\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "![Transformer](img/transformer.png)\n",
    "\n",
    "#### 1. Input Representation\n",
    "\n",
    "The model adopts a **character-level tokenization** strategy. Each input sequence, composed of a `query` and a `title`, is tokenized into individual characters and mapped to token IDs based on a constructed vocabulary. Two types of embeddings are employed:\n",
    "\n",
    "- **Word Embeddings**: Token IDs are projected into dense vectors of a fixed dimension (`embed_dim = 128`).\n",
    "- **Positional Embeddings**: Positional information is encoded via learnable embeddings, allowing the model to capture the sequential structure of the input.\n",
    "\n",
    "The final input embeddings are obtained by summing the word and positional embeddings.\n",
    "\n",
    "#### 2. Transformer Encoder\n",
    "\n",
    "The core of the model consists of a **Transformer encoder** comprising three stacked **TransformerEncoderLayers** (`num_layers = 3`). Each layer operates with:\n",
    "\n",
    "- A multi-head self-attention mechanism (`num_heads = 8`),\n",
    "- A feed-forward network with an intermediate dimension of `4 × embed_dim`,\n",
    "- Layer normalization and residual connections,\n",
    "- Support for an optional source key padding mask to handle variable-length inputs.\n",
    "\n",
    "This encoder processes the sequence embeddings and generates contextualized token representations.\n",
    "\n",
    "#### 3. Classification Head\n",
    "\n",
    "For classification, only the hidden state corresponding to the first token (analogous to the [CLS] token) is extracted. This representation is passed through a feed-forward network consisting of:\n",
    "\n",
    "- A linear transformation,\n",
    "- A ReLU activation function,\n",
    "- A dropout layer (`dropout rate = 0.1`) for regularization,\n",
    "- A final linear layer projecting to three output classes.\n",
    "\n",
    "The model is trained using the **cross-entropy loss** function when labels are provided.\n",
    "\n",
    "#### Training and Optimization\n",
    "\n",
    "The model is optimized using the **Adam optimizer** with an initial learning rate of 5e-4. A **linear learning rate scheduler with warmup** is utilized to improve training stability. During training, model selection is performed based on the best F1 score evaluated on the development set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMatchingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=8, num_layers=3, num_classes=3, max_length=256):\n",
    "        super(TransformerMatchingModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        word_embeddings = self.embedding(input_ids)\n",
    "        position_embeddings = self.position_embedding(positions)\n",
    "        embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "        # src_key_padding_mask: shape [batch_size, seq_len], where True indicates padding\n",
    "        src_key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "\n",
    "        transformer_output = self.transformer(embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "        cls_output = transformer_output[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_model():\n",
    "    all_text = list(train_df['query']) + list(train_df['title'])\n",
    "    all_text = [str(text) for text in all_text]\n",
    "\n",
    "    all_tokens = []\n",
    "    for text in all_text:\n",
    "        tokens = list(text)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    vocab = {token: idx + 1 for idx, token in enumerate(set(all_tokens))}\n",
    "    vocab['<PAD>'] = 0  # pad\n",
    "    vocab['<UNK>'] = len(vocab)  # unkown\n",
    "\n",
    "    class SimpleTokenizer:\n",
    "        def __init__(self, vocab, max_length=128):\n",
    "            self.vocab = vocab\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __call__(self, query, title, add_special_tokens=True, max_length=None, padding='max_length', truncation=True, return_tensors='pt'):\n",
    "            if max_length is None:\n",
    "                max_length = self.max_length\n",
    "\n",
    "            query_tokens = list(query)\n",
    "            title_tokens = list(title)\n",
    "\n",
    "            query_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in query_tokens]\n",
    "            title_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in title_tokens]\n",
    "\n",
    "            if add_special_tokens:\n",
    "                input_ids = [self.vocab['<UNK>']] + query_ids + [self.vocab['<UNK>']] + title_ids + [self.vocab['<UNK>']]\n",
    "                token_type_ids = [0] * (len(query_ids) + 2) + [1] * (len(title_ids) + 1)\n",
    "            else:\n",
    "                input_ids = query_ids + title_ids\n",
    "                token_type_ids = [0] * len(query_ids) + [1] * len(title_ids)\n",
    "\n",
    "            if truncation and len(input_ids) > max_length:\n",
    "                input_ids = input_ids[:max_length]\n",
    "                token_type_ids = token_type_ids[:max_length]\n",
    "\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            if padding == 'max_length':\n",
    "                pad_length = max_length - len(input_ids)\n",
    "                input_ids = input_ids + [self.vocab['<PAD>']] * pad_length\n",
    "                attention_mask = attention_mask + [0] * pad_length\n",
    "                token_type_ids = token_type_ids + [0] * pad_length\n",
    "\n",
    "            if return_tensors == 'pt':\n",
    "                return {\n",
    "                    'input_ids': torch.tensor([input_ids]),\n",
    "                    'attention_mask': torch.tensor([attention_mask]),\n",
    "                    'token_type_ids': torch.tensor([token_type_ids])\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'token_type_ids': token_type_ids\n",
    "                }\n",
    "\n",
    "    tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "    model = TransformerMatchingModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=128,\n",
    "        num_heads=8,\n",
    "        num_layers=3,\n",
    "        num_classes=3  # 0, 1, 2\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset = TextMatchingDataset(train_df, tokenizer)\n",
    "    dev_dataset = TextMatchingDataset(dev_df, tokenizer)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    epochs = 5\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.1 * total_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        train_loss = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"training loss: {train_loss:.4f}\")\n",
    "\n",
    "        accuracy, f1, report, _ = evaluate(model, dev_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_transformer_model.pt')\n",
    "            print(\"save best model\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_transformer_model.pt'))\n",
    "\n",
    "    test_public_dataset = TextMatchingDataset(test_public_df, tokenizer)\n",
    "    test_public_dataloader = DataLoader(test_public_dataset, batch_size=batch_size)\n",
    "\n",
    "    accuracy, f1, report, _ = evaluate(model, test_public_dataloader, device)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "    print(report)\n",
    "\n",
    "    # predict\n",
    "    # test_dataset = TextMatchingDataset(test_df, tokenizer)\n",
    "    # test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # predictions = predict(model, test_dataloader, device)\n",
    "\n",
    "    # test_df['predicted_label'] = predictions\n",
    "    # test_df[['id', 'predicted_label']].to_csv('transformer_predictions.csv', index=False)\n",
    "    # print(\"result: transformer_predictions.csv\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Transformer model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:50<00:00, 25.52it/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.8529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6457\n",
      "Validation F1 Score: 0.5975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5540    0.2432    0.3380      4894\n",
      "           1     0.6693    0.8945    0.7657     12592\n",
      "           2     0.4498    0.1834    0.2605      2514\n",
      "\n",
      "    accuracy                         0.6457     20000\n",
      "   macro avg     0.5577    0.4403    0.4547     20000\n",
      "weighted avg     0.6135    0.6457    0.5975     20000\n",
      "\n",
      "save best model\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:52<00:00, 24.92it/s, loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.8035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6544\n",
      "Validation F1 Score: 0.5901\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5815    0.2303    0.3299      4894\n",
      "           1     0.6643    0.9299    0.7749     12592\n",
      "           2     0.5793    0.1002    0.1709      2514\n",
      "\n",
      "    accuracy                         0.6544     20000\n",
      "   macro avg     0.6084    0.4201    0.4253     20000\n",
      "weighted avg     0.6333    0.6544    0.5901     20000\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:50<00:00, 25.38it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6579\n",
      "Validation F1 Score: 0.6009\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5783    0.2460    0.3452      4894\n",
      "           1     0.6702    0.9221    0.7762     12592\n",
      "           2     0.5758    0.1360    0.2201      2514\n",
      "\n",
      "    accuracy                         0.6579     20000\n",
      "   macro avg     0.6081    0.4347    0.4472     20000\n",
      "weighted avg     0.6359    0.6579    0.6009     20000\n",
      "\n",
      "save best model\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:53<00:00, 24.88it/s, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6604\n",
      "Validation F1 Score: 0.6255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5624    0.3269    0.4135      4894\n",
      "           1     0.6883    0.8781    0.7717     12592\n",
      "           2     0.5050    0.2192    0.3057      2514\n",
      "\n",
      "    accuracy                         0.6604     20000\n",
      "   macro avg     0.5852    0.4747    0.4970     20000\n",
      "weighted avg     0.6345    0.6604    0.6255     20000\n",
      "\n",
      "save best model\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [01:49<00:00, 25.69it/s, loss=0.933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:08<00:00, 38.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6609\n",
      "Validation F1 Score: 0.6268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5642    0.3259    0.4132      4894\n",
      "           1     0.6895    0.8770    0.7720     12592\n",
      "           2     0.5009    0.2303    0.3155      2514\n",
      "\n",
      "    accuracy                         0.6609     20000\n",
      "   macro avg     0.5848    0.4777    0.5002     20000\n",
      "weighted avg     0.6351    0.6609    0.6268     20000\n",
      "\n",
      "save best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:02<00:00, 38.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.6588\n",
      "Test Set F1 Score: 0.6277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5637    0.3366    0.4215      1209\n",
      "           1     0.6912    0.8664    0.7689      3159\n",
      "           2     0.4717    0.2373    0.3158       632\n",
      "\n",
      "    accuracy                         0.6588      5000\n",
      "   macro avg     0.5755    0.4801    0.5021      5000\n",
      "weighted avg     0.6326    0.6588    0.6277      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ntrain Transformer model...\")\n",
    "transformer_model = train_transformer_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.7 <a name='5.7'></a>BERT-based Single Tower\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We fine-tune a pre-trained Single-Tower BERT (`bert-base-chinese`) to perform this **three-class text matching** task, where the objective is to predict the degree of relevance between a `query` and a `title`. \n",
    "\n",
    "The model architecture leverages the pre-trained **BERT encoder** for feature extraction and adds a simple classification head to adapt it to the specific task.\n",
    "\n",
    "#### Architecture\n",
    "![BERT_1](img/bert_1.png)\n",
    "#### 1. Input Encoding\n",
    "\n",
    "- The input consists of a `query` and a `title`, which are tokenized using the **BertTokenizer**.\n",
    "- Special tokens `[CLS]`, `[SEP]` are automatically inserted:\n",
    "  ```\n",
    "  [CLS] query tokens [SEP] title tokens [SEP]\n",
    "  ```\n",
    "- The tokenizer outputs:\n",
    "  - `input_ids`: Token IDs\n",
    "  - `attention_mask`: Binary mask indicating valid tokens\n",
    "  - `token_type_ids`: Segment IDs distinguishing query and title parts\n",
    "\n",
    "#### 2. Pre-trained BERT Encoder\n",
    "\n",
    "- The tokenized input is fed into **BERT** (`bert-base-chinese`), which produces contextualized embeddings for each token.\n",
    "- BERT’s output includes a special hidden state corresponding to the `[CLS]` token, which is used as the aggregate sequence representation.\n",
    "\n",
    "#### 3. Classification Head\n",
    "\n",
    "- A **single fully connected layer** is added on top of the `[CLS]` representation:\n",
    "  - Maps the `[CLS]` vector (size 768) to logits over 3 classes.\n",
    "- The model is trained using **cross-entropy loss**.\n",
    "\n",
    "#### 4. Training Setup\n",
    "\n",
    "- Optimizer: **AdamW** with a learning rate of 2e-5.\n",
    "- Learning Rate Scheduler: **Linear decay with warmup**.\n",
    "- Best model selection based on **validation F1 score**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model():\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-chinese',\n",
    "        num_labels=3  # 0, 1, 2\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset = TextMatchingDataset(train_df, tokenizer)\n",
    "    dev_dataset = TextMatchingDataset(dev_df, tokenizer)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    epochs = 3\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.1 * total_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        train_loss = train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"training loss: {train_loss:.4f}\")\n",
    "\n",
    "        # val\n",
    "        accuracy, f1, report, _ = evaluate(model, dev_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_bert_model.pt')\n",
    "            print(\"save best model\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_bert_model.pt'))\n",
    "\n",
    "    # test_public\n",
    "    test_public_dataset = TextMatchingDataset(test_public_df, tokenizer)\n",
    "    test_public_dataloader = DataLoader(test_public_dataset, batch_size=batch_size)\n",
    "\n",
    "    accuracy, f1, report, _ = evaluate(model, test_public_dataloader, device)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "    print(report)\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Transformer_bge model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:36<00:00,  8.08it/s, loss=0.621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:13<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6681\n",
      "Validation F1 Score: 0.5845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7288    0.2372    0.3579      4894\n",
      "           1     0.6626    0.9676    0.7866     12592\n",
      "           2     0.8500    0.0068    0.0134      2514\n",
      "\n",
      "    accuracy                         0.6681     20000\n",
      "   macro avg     0.7472    0.4039    0.3860     20000\n",
      "weighted avg     0.7024    0.6681    0.5845     20000\n",
      "\n",
      "save best model\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:35<00:00,  8.09it/s, loss=0.618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:12<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6916\n",
      "Validation F1 Score: 0.6571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6631    0.4142    0.5099      4894\n",
      "           1     0.7041    0.9009    0.7904     12592\n",
      "           2     0.5548    0.1834    0.2756      2514\n",
      "\n",
      "    accuracy                         0.6916     20000\n",
      "   macro avg     0.6406    0.4995    0.5253     20000\n",
      "weighted avg     0.6753    0.6916    0.6571     20000\n",
      "\n",
      "save best model\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:33<00:00,  8.11it/s, loss=0.737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.7124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:12<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6890\n",
      "Validation F1 Score: 0.6535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6723    0.3756    0.4819      4894\n",
      "           1     0.7004    0.9064    0.7902     12592\n",
      "           2     0.5443    0.2100    0.3031      2514\n",
      "\n",
      "    accuracy                         0.6890     20000\n",
      "   macro avg     0.6390    0.4973    0.5251     20000\n",
      "weighted avg     0.6739    0.6890    0.6535     20000\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:33<00:00,  8.11it/s, loss=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:13<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6955\n",
      "Validation F1 Score: 0.6668\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7067    0.3796    0.4940      4894\n",
      "           1     0.7077    0.8989    0.7919     12592\n",
      "           2     0.5320    0.2912    0.3763      2514\n",
      "\n",
      "    accuracy                         0.6955     20000\n",
      "   macro avg     0.6488    0.5232    0.5541     20000\n",
      "weighted avg     0.6853    0.6955    0.6668     20000\n",
      "\n",
      "save best model\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5625/5625 [11:35<00:00,  8.09it/s, loss=0.734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:12<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6979\n",
      "Validation F1 Score: 0.6786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6958    0.4285    0.5303      4894\n",
      "           1     0.7210    0.8722    0.7894     12592\n",
      "           2     0.5017    0.3496    0.4121      2514\n",
      "\n",
      "    accuracy                         0.6979     20000\n",
      "   macro avg     0.6395    0.5501    0.5773     20000\n",
      "weighted avg     0.6872    0.6979    0.6786     20000\n",
      "\n",
      "save best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [00:18<00:00,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.7000\n",
      "Test Set F1 Score: 0.6812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7020    0.4326    0.5353      1209\n",
      "           1     0.7245    0.8724    0.7916      3159\n",
      "           2     0.4900    0.3497    0.4081       632\n",
      "\n",
      "    accuracy                         0.7000      5000\n",
      "   macro avg     0.6388    0.5516    0.5784      5000\n",
      "weighted avg     0.6894    0.7000    0.6812      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ntrain Transformer_bge model...\")\n",
    "transformer_model = train_bge_transformer_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.8 <a name='5.8'></a>BERT-based Dual-Tower\n",
    "[Back to Table of Contents](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose a **Dual-Tower BERT architecture** to address the text matching task. In this design, the `query` and `title` are encoded independently by two parallel BERT encoders (weight-sharing), and their representations are concatenated for final classification.\n",
    "\n",
    "This dual-tower structure enables independent semantic encoding of inputs, allowing for flexible retrieval and matching scenarios.\n",
    "\n",
    "#### Architecture\n",
    "![BERT_2](img/bert_2.png)\n",
    "\n",
    "#### 1. Input Encoding\n",
    "\n",
    "- **Separate Encoding**:\n",
    "  - Each `query` and `title` is **tokenized independently** using the **BertTokenizer**.\n",
    "  - Special tokens `[CLS]`, `[SEP]` are inserted into each sequence separately.\n",
    "  - The tokenizer outputs for each:\n",
    "    - `input_ids`\n",
    "    - `attention_mask`\n",
    "\n",
    "- **No Cross-Attention** between the query and title during encoding.\n",
    "\n",
    "#### 2. BERT Encoders\n",
    "\n",
    "- **Shared Weights**:\n",
    "  - Both the query and the title are passed through the **same pre-trained BERT encoder** (`bert-base-chinese`).\n",
    "- **Pooler Output Extraction**:\n",
    "  - For each input, we extract the **[CLS] token’s pooler output** (a 768-dimensional vector).\n",
    "  - Output vectors:\n",
    "    - `query_repr` (768-d)\n",
    "    - `title_repr` (768-d)\n",
    "\n",
    "#### 3. Representation Combination\n",
    "\n",
    "- The two representations are **concatenated** to form a combined vector of size `768 × 2 = 1536`.\n",
    "- This combined feature vector captures information from both inputs while maintaining their independent encoding paths.\n",
    "\n",
    "#### 4. Classification Head\n",
    "\n",
    "- A **single fully connected (linear) layer** is applied:\n",
    "  - Input: 1536-dimensional combined vector\n",
    "  - Output: 3 logits corresponding to the three relevance classes.\n",
    "\n",
    "- The model is optimized with **cross-entropy loss**.\n",
    "\n",
    "#### 5. Training Setup\n",
    "\n",
    "- Optimizer: **AdamW** with a learning rate of 2e-5.\n",
    "- Learning Rate Scheduler: **Linear decay with warmup**.\n",
    "- Model selection is based on the **best validation F1 score**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMatchingDataset_Dual(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.queries = df['query'].tolist()\n",
    "        self.titles = df['title'].tolist()\n",
    "        self.labels = df['label'].tolist() if 'label' in df.columns else [0] * len(df)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.queries[idx]\n",
    "        title = self.titles[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        query_enc = self.tokenizer(query, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        title_enc = self.tokenizer(title, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids_1': query_enc['input_ids'].squeeze(0),\n",
    "            'attention_mask_1': query_enc['attention_mask'].squeeze(0),\n",
    "            'input_ids_2': title_enc['input_ids'].squeeze(0),\n",
    "            'attention_mask_2': title_enc['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualTowerBERTModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-chinese', hidden_size=768, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        output1 = self.bert(input_ids=input_ids_1, attention_mask=attention_mask_1).pooler_output\n",
    "        output2 = self.bert(input_ids=input_ids_2, attention_mask=attention_mask_2).pooler_output\n",
    "        combined = torch.cat([output1, output2], dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids_1 = batch['input_ids_1'].to(device)\n",
    "        attention_mask_1 = batch['attention_mask_1'].to(device)\n",
    "        input_ids_2 = batch['input_ids_2'].to(device)\n",
    "        attention_mask_2 = batch['attention_mask_2'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids_1 = batch['input_ids_1'].to(device)\n",
    "            attention_mask_1 = batch['attention_mask_1'].to(device)\n",
    "            input_ids_2 = batch['input_ids_2'].to(device)\n",
    "            attention_mask_2 = batch['attention_mask_2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    report = classification_report(all_labels, all_preds, digits=4)\n",
    "    return acc, f1, report, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model_Dual():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    model = DualTowerBERTModel().to(device)\n",
    "\n",
    "    train_dataset = TextMatchingDataset_Dual(train_df, tokenizer)\n",
    "    dev_dataset = TextMatchingDataset_Dual(dev_df, tokenizer)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    epochs = 3\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "        print(f\"training loss: {train_loss:.4f}\")\n",
    "\n",
    "        acc, f1, report, _ = evaluate(model, dev_loader, device)\n",
    "        print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "        print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_dualtower_model.pt')\n",
    "            print(\"save best model\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_dualtower_model.pt'))\n",
    "\n",
    "    test_public_dataset = TextMatchingDataset_Dual(test_public_df, tokenizer)\n",
    "    test_public_dataloader = DataLoader(test_public_dataset, batch_size=batch_size)\n",
    "\n",
    "    accuracy, f1, report, _ = evaluate(model, test_public_dataloader, device)\n",
    "    print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "    print(report)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [27:56<00:00,  1.68it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.6991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [01:09<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7220\n",
      "Validation F1 Score: 0.7145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6961    0.5922    0.6399      4894\n",
      "           1     0.7594    0.8362    0.7959     12592\n",
      "           2     0.5137    0.4029    0.4516      2514\n",
      "\n",
      "    accuracy                         0.7220     20000\n",
      "   macro avg     0.6564    0.6104    0.6292     20000\n",
      "weighted avg     0.7130    0.7220    0.7145     20000\n",
      "\n",
      "save best model\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [27:57<00:00,  1.68it/s, loss=0.593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.5784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [01:09<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7331\n",
      "Validation F1 Score: 0.7275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7228    0.5936    0.6519      4894\n",
      "           1     0.7710    0.8420    0.8049     12592\n",
      "           2     0.5184    0.4594    0.4871      2514\n",
      "\n",
      "    accuracy                         0.7331     20000\n",
      "   macro avg     0.6707    0.6317    0.6480     20000\n",
      "weighted avg     0.7274    0.7331    0.7275     20000\n",
      "\n",
      "save best model\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2813/2813 [27:57<00:00,  1.68it/s, loss=0.493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [01:09<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7308\n",
      "Validation F1 Score: 0.7274\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7166    0.5979    0.6519      4894\n",
      "           1     0.7759    0.8289    0.8015     12592\n",
      "           2     0.5077    0.4980    0.5028      2514\n",
      "\n",
      "    accuracy                         0.7308     20000\n",
      "   macro avg     0.6668    0.6416    0.6521     20000\n",
      "weighted avg     0.7277    0.7308    0.7274     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:17<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.7408\n",
      "Test Set F1 Score: 0.7345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7227    0.5906    0.6500      1209\n",
      "           1     0.7780    0.8531    0.8138      3159\n",
      "           2     0.5383    0.4668    0.5000       632\n",
      "\n",
      "    accuracy                         0.7408      5000\n",
      "   macro avg     0.6797    0.6368    0.6546      5000\n",
      "weighted avg     0.7343    0.7408    0.7345      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"train BERT model...\")\n",
    "bert_model = train_bert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 <a name='5.8'></a>BGE-based Single-Tower \n",
    "[Back to Table of Contents](#top)\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"img/RetroMAE.png\" alt=\"RetroMAE\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "**Removal of the NSP (Next Sentence Prediction) Task**\n",
    "BERT originally employed the **NSP** task to model relationships between sentences. However, subsequent research has shown that **NSP** provides limited benefits for downstream performance. BGE removes the **NSP** task entirely to avoid introducing unnecessary noise, allowing the model to focus more on fine-grained semantic representation learning.\n",
    "\n",
    "**Introduction of the MLLM (Masked Language Model for Matching) Pretraining Objective**\n",
    "BGE adopts a specially designed **MLLM** objective tailored for retrieval tasks. It combines masked language modeling with semantic matching, not only predicting masked tokens but also enhancing the modeling of relevance between paired texts.\n",
    "\n",
    "**Integration of RetroMAE as a Decoder Module**\n",
    "During pretraining, BGE incorporates **RetroMAE(Retrieval-oriented Masked AutoEncoder)**  as its decoder component to strengthen the model’s representation learning capability. By reconstructing the hidden representations of the original text, RetroMAE guides the encoder to learn more retrieval-friendly vector distributions, leading to significant improvements in retrieval-related tasks.\n",
    "\n",
    "**Incorporating Contrastive Learning**\n",
    "\n",
    "The BGE model introduces **contrastive learning** during pretraining to optimize sentence embeddings for semantic retrieval tasks.Unlike traditional language models like BERT, which are trained with **Masked Language Modeling** and **Next Sentence Prediction**, BGE focuses on learning embeddings where semantically similar sentences are close together in the vector space, and dissimilar ones are far apart.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"img/single_tower_bge.jpg\" alt=\"single_tower_bge\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Single Tower Relevance Modeling with BGE refers to a design where both the query and document are concatenated and jointly fed into a single encoder, typically a transformer-based language model. The single-tower approach enables deeper interaction between the two inputs. This allows the model to better capture fine-grained semantic relationships and contextual dependencies. \n",
    "In BGE, the input format usually follows **[CLS] Query [SEP] Document [SEP]**, and the relevance score is computed from the **[CLS]** token embedding using a simple classifier. Although single-tower models are computationally heavier during inference, they often yield higher performance in tasks requiring precise matching, such as question answering and passage ranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.optimizer = 'adamw'\n",
    "        self.batch_size = 16\n",
    "        self.num_epochs = 10\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 1e-4\n",
    "        self.lr_period = 4\n",
    "        self.lr_decay = 0.8\n",
    "        self.dropout = 0.1\n",
    "        self.embedding_dim = 300\n",
    "        self.hidden_dim = 300\n",
    "        self.num_class = 3\n",
    "        self.num_aspect = 6\n",
    "        self.max_seq_len = 500\n",
    "        self.num_warmup_steps = 200\n",
    "        self.pt_path = \"/content/drive/MyDrive/CS5242/ckpt\"\n",
    "        self.img_path = \"/content/drive/MyDrive/CS5242/images\"\n",
    "        self.learning_rates = []\n",
    "        self.monitor = 'loss'\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Defined in :numref:`sec_minibatch_sgd`\"\"\"\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        use_svg_display()\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "    def save(self, output_dir, fname):\n",
    "        plt.savefig(os.path.join(output_dir, fname), dpi=600)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        logits: [batch_size, num_classes]\n",
    "        targets: [batch_size] with class indices (0, 1, 2)\n",
    "        \"\"\"\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            at = self.alpha.gather(0, targets)\n",
    "            focal_loss = at * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    \"\"\"Compute the number of correct predictions.\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = torch.argmax(y_pred, axis=1)\n",
    "    cmp = (y_pred.type(y.dtype)) == y\n",
    "    return float(torch.sum(cmp.type(y.dtype)))\n",
    "\n",
    "def download_model(model_name,cache_dir):\n",
    "  if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "  model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "  total, trainable = count_parameters(model)\n",
    "  print(f\"Total Parameters: {total:,}, Trainable: {trainable:,}\")\n",
    "  print(model)\n",
    "\n",
    "def evaluate_loss_and_acc_gpu(net, data_iter, loss=None, device=None):\n",
    "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    if loss:\n",
    "        metric = Accumulator(3)\n",
    "    else:\n",
    "        metric = Accumulator(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            if isinstance(batch, dict):\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = batch[k].to(device)\n",
    "            else:\n",
    "                batch = batch.to(device)\n",
    "            y = batch['label']\n",
    "            y_pred = net(**batch)\n",
    "            if loss:\n",
    "                metric.add(loss(y_pred, y).sum(), accuracy(y_pred, y), torch.numel(y))\n",
    "            else:\n",
    "                metric.add(accuracy(y_pred, y), torch.numel(y))\n",
    "    return (metric[0] / metric[2], metric[1] / metric[2]) if loss else metric[0] / metric[1]\n",
    "\n",
    "def train_batch(net, batch, loss, trainer, devices):\n",
    "    \"\"\"Train for a minibatch \"\"\"\n",
    "    if isinstance(batch, dict):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = batch[k].to(devices[0])\n",
    "    else:\n",
    "        batch = batch.to(devices[0])\n",
    "    y = batch['label']\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(**batch)\n",
    "    l = loss(pred, y)\n",
    "    l.mean().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum().item()\n",
    "    train_acc_sum = accuracy(pred, y)\n",
    "    args.learning_rates.append(trainer.param_groups[0][\"lr\"])\n",
    "    return train_loss_sum, train_acc_sum\n",
    "\n",
    "def train_transformer(net,\n",
    "                      train_iter,\n",
    "                      valid_iter,\n",
    "                      num_epochs,\n",
    "                      optimizer,\n",
    "                      lr,\n",
    "                      wd,\n",
    "                      num_warmup_steps,\n",
    "                      devices,\n",
    "                      task_name):\n",
    "    if optimizer == 'sgd':\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    elif optimizer == 'adam':\n",
    "        trainer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    elif optimizer == 'adamw':\n",
    "        trainer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    model_name = str(net.__class__.__name__)\n",
    "    scheduler = get_cosine_schedule_with_warmup(trainer, num_warmup_steps=num_warmup_steps, num_training_steps=num_epochs*len(train_iter))\n",
    "\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    timer = Timer()\n",
    "    num_batches = len(train_iter)\n",
    "    legend = ['train loss', 'train acc']\n",
    "    if valid_iter is not None:\n",
    "        legend += ['valid loss', 'valid acc']\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], legend=legend, figsize=(7, 5))\n",
    "\n",
    "    if args.monitor == 'loss':\n",
    "        monitor_val = math.inf\n",
    "    else:\n",
    "        monitor_val = -math.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        metric = Accumulator(4)\n",
    "        for i, batch in enumerate(tqdm(train_iter)):\n",
    "            timer.start()\n",
    "            labels = batch['label']\n",
    "            output = train_batch(net, batch, loss, trainer, devices)\n",
    "            scheduler.step()\n",
    "            metric.add(output[0], output[1], labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[2], None, None))\n",
    "\n",
    "        if valid_iter is not None:\n",
    "            valid_loss, valid_acc = evaluate_loss_and_acc_gpu(net, valid_iter, loss, devices[0])\n",
    "            animator.add(epoch + 1, (None, None, valid_loss, valid_acc))\n",
    "            if args.monitor == 'loss':\n",
    "                if valid_loss < monitor_val:\n",
    "                    filename = task_name + f\"-epoch{epoch}-val_loss{valid_loss :.2f}.pt\"\n",
    "                    best_model_state = copy.deepcopy(net.state_dict())\n",
    "                    monitor_val = valid_loss\n",
    "            else:\n",
    "                if valid_acc > monitor_val:\n",
    "                    filename = task_name + f\"-epoch{epoch}-val_acc{valid_acc :.2f}.pt\"\n",
    "                    best_model_state = copy.deepcopy(net.state_dict())\n",
    "                    monitor_val = valid_acc\n",
    "\n",
    "\n",
    "    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n",
    "                f'train acc {metric[1] / metric[2]:.3f}')\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid loss {valid_loss:.3f}'\n",
    "        measures += f', valid acc {valid_acc:.3f}'\n",
    "    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')\n",
    "    args.pt_path = os.path.join(args.pt_path, filename)\n",
    "    torch.save(best_model_state, args.pt_path)\n",
    "    animator.save(args.img_path, filename + f\"-train_loss{metric[0] / metric[2]:.2f}.png\")\n",
    "\n",
    "def test(net, data_iter, output_dir=None, device=None):\n",
    "    net.eval()\n",
    "    full_logits=[]\n",
    "    full_label_ids=[]\n",
    "    for i, batch in tqdm(enumerate(data_iter)):\n",
    "        with torch.no_grad():\n",
    "            if isinstance(batch, dict):\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = batch[k].to(device)\n",
    "            else:\n",
    "                batch = batch.to(device)\n",
    "\n",
    "            logits = net(**batch)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            y = batch['label'].cpu().numpy()\n",
    "\n",
    "            full_logits.extend(logits.tolist())\n",
    "            full_label_ids.extend(y.tolist())\n",
    "\n",
    "    y_pred = [np.argmax(logit) for logit in full_logits]\n",
    "    y_true = full_label_ids\n",
    "    f1 =sklearn.metrics.f1_score(y_true, y_pred, average='macro')\n",
    "    acc =sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    print(f\"test on final epoch -- acc: {acc*100 :.2f}, f1-macro: {f1*100 :.2f}\")\n",
    "    if output_dir:\n",
    "        output_eval_json = os.path.join(output_dir, str(net.__class__.__name__) + \"_predictions.json\")\n",
    "        with open(output_eval_json, \"w\") as fw:\n",
    "            json.dump({\"logits\": full_logits, \"label_ids\": full_label_ids}, fw)\n",
    "\n",
    "def test_on_checkpoint(net, file_path, data_iter, output_dir=None, device=None):\n",
    "    state_dict = torch.load(file_path, map_location=device)\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    full_logits=[]\n",
    "    full_label_ids=[]\n",
    "    full_aspect_ids = []\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        with torch.no_grad():\n",
    "            if isinstance(batch, dict):\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = batch[k].to(device)\n",
    "            else:\n",
    "                batch = batch.to(device)\n",
    "\n",
    "            logits = net(**batch)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            y = batch['label'].cpu().numpy()\n",
    "\n",
    "            full_logits.extend(logits.tolist())\n",
    "            full_label_ids.extend(y.tolist())\n",
    "\n",
    "\n",
    "    y_pred = [np.argmax(logit) for logit in full_logits]\n",
    "    y_true = full_label_ids\n",
    "    f1 =sklearn.metrics.f1_score(y_true, y_pred, average='macro')\n",
    "    acc =sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    print(f\"test on {file_path} -- acc: {acc*100 :.2f}, f1-macro: {f1*100 :.2f}\")\n",
    "    if output_dir:\n",
    "        output_eval_json = os.path.join(output_dir, str(net.__class__.__name__) + \"_predictions.json\")\n",
    "        with open(output_eval_json, \"w\") as fw:\n",
    "            json.dump({\"logits\": full_logits, \"label_ids\": full_label_ids}, fw)\n",
    "\n",
    "class BGEProcessor(object):\n",
    "    def __init__(self, data_dir, data_name):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_name = data_name\n",
    "\n",
    "    def get_examples(self):\n",
    "        \"\"\"Read data from json file.\n",
    "        Args:\n",
    "            data_dir (string):\n",
    "                Path.\n",
    "            mode (string):\n",
    "                Indicate train/dev/test.\n",
    "        Returns:\n",
    "            examples for different modes.\n",
    "        \"\"\"\n",
    "        return self._create_examples(self._read_json(os.path.join(self.data_dir, self.data_name)))\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Return labels for specific dataset\"\"\"\n",
    "        return [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "    def _create_examples(self, datas):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for idx,data in enumerate(datas):\n",
    "            data['label'] = int(data[\"label\"])\n",
    "            examples.append(data)\n",
    "        return examples\n",
    "\n",
    "    def _read_json(self, input_file):\n",
    "        \"\"\"Reads a json file for tasks in sentiment analysis.\"\"\"\n",
    "        datas = []\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "          for line in f:\n",
    "              obj = json.loads(line)\n",
    "              datas.append(obj)\n",
    "        return datas\n",
    "\n",
    "class SingleTowerBGEDataset(Dataset):\n",
    "    \"\"\"Build dataset and convert examples to features\"\"\"\n",
    "    def __init__(self, model_name, cache_dir, samples):\n",
    "        self.model_name = model_name\n",
    "        self.samples = samples\n",
    "        print(len(self.samples))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "        self.convert_data_to_features()\n",
    "\n",
    "    def convert_data_to_features(self):\n",
    "        \"\"\"convert examples to features\"\"\"\n",
    "        label_map = {0: 0, 1: 1, 2: 2}\n",
    "\n",
    "        output = []\n",
    "        for (i, sample) in tqdm(enumerate(self.samples)):\n",
    "            feature = {}\n",
    "            query = sample['query']\n",
    "            document = sample['title']\n",
    "            inputs = self.tokenizer(query,\n",
    "                                    document,\n",
    "                                    return_tensors=\"pt\",\n",
    "                                    padding=True,\n",
    "                                    truncation=True)\n",
    "\n",
    "            input_ids = inputs[\"input_ids\"][0]\n",
    "            token_type_ids  = inputs[\"token_type_ids\"][0]\n",
    "            attention_mask = inputs[\"attention_mask\"][0]\n",
    "            label_id = label_map[sample['label']]\n",
    "            feature = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"label_id\": label_id\n",
    "            }\n",
    "            output += [feature]\n",
    "        self.data = output\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def build_bge_collate_fn(tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        input_ids = [item['input_ids'] for item in batch]\n",
    "        attention_mask = [item['attention_mask'] for item in batch]\n",
    "        token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "        label_ids = torch.tensor([item['label_id'] for item in batch])\n",
    "\n",
    "        padded = tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            },\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": padded[\"input_ids\"],\n",
    "            \"attention_mask\": padded[\"attention_mask\"],\n",
    "            \"token_type_ids\": padded[\"token_type_ids\"],\n",
    "            \"label\": label_ids\n",
    "        }\n",
    "    return collate_fn\n",
    "\n",
    "class DualTowerBGEDataset(Dataset):\n",
    "    \"\"\"Build dataset for dual tower model and convert examples to features separately\"\"\"\n",
    "    def __init__(self, model_name, cache_dir, samples):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "        self.samples = samples\n",
    "        self.data = self.convert_data_to_features(samples)\n",
    "\n",
    "    def convert_data_to_features(self, samples):\n",
    "        label_map = {0: 0, 1: 1, 2: 2}\n",
    "        features = []\n",
    "\n",
    "        for sample in tqdm(samples, desc=\"Converting data\"):\n",
    "            query = sample['query']\n",
    "            document = sample['title']\n",
    "            label = label_map[sample['label']]\n",
    "\n",
    "            query_inputs = self.tokenizer(query,\n",
    "                                          return_tensors=\"pt\",\n",
    "                                          padding=True,\n",
    "                                          truncation=True)\n",
    "            doc_inputs = self.tokenizer(  document,\n",
    "                                          return_tensors=\"pt\",\n",
    "                                          padding=True,\n",
    "                                          truncation=True)\n",
    "\n",
    "            feature = {\n",
    "                \"query_input_ids\": query_inputs[\"input_ids\"].squeeze(0),\n",
    "                \"query_attention_mask\": query_inputs[\"attention_mask\"].squeeze(0),\n",
    "                \"query_token_type_ids\": query_inputs[\"token_type_ids\"].squeeze(0),\n",
    "\n",
    "                \"doc_input_ids\": doc_inputs[\"input_ids\"].squeeze(0),\n",
    "                \"doc_attention_mask\": doc_inputs[\"attention_mask\"].squeeze(0),\n",
    "                \"doc_token_type_ids\": doc_inputs[\"token_type_ids\"].squeeze(0),\n",
    "\n",
    "                \"label\": label\n",
    "            }\n",
    "            features.append(feature)\n",
    "        return features\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def build_dual_bge_collate_fn(tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        query_input_ids = [item['query_input_ids'] for item in batch]\n",
    "        query_attention_mask = [item['query_attention_mask'] for item in batch]\n",
    "        query_token_type_ids = [item['query_token_type_ids'] for item in batch]\n",
    "\n",
    "        doc_input_ids = [item['doc_input_ids'] for item in batch]\n",
    "        doc_attention_mask = [item['doc_attention_mask'] for item in batch]\n",
    "        doc_token_type_ids = [item['doc_token_type_ids'] for item in batch]\n",
    "\n",
    "        label_ids = torch.tensor([item['label'] for item in batch])\n",
    "\n",
    "        # Padding query\n",
    "        query_batch = tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": query_input_ids,\n",
    "                \"attention_mask\": query_attention_mask,\n",
    "                \"token_type_ids\": query_token_type_ids,\n",
    "            },\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        doc_batch = tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": doc_input_ids,\n",
    "                \"attention_mask\": doc_attention_mask,\n",
    "                \"token_type_ids\": doc_token_type_ids,\n",
    "            },\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"query_input_ids\": query_batch[\"input_ids\"],\n",
    "            \"query_attention_mask\": query_batch[\"attention_mask\"],\n",
    "            \"query_token_type_ids\": query_batch[\"token_type_ids\"],\n",
    "            \"doc_input_ids\": doc_batch[\"input_ids\"],\n",
    "            \"doc_attention_mask\": doc_batch[\"attention_mask\"],\n",
    "            \"doc_token_type_ids\": doc_batch[\"token_type_ids\"],\n",
    "            \"label\": label_ids\n",
    "        }\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 102,267,648, Trainable: 102,267,648\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/content/drive/MyDrive/CS5242/\"\n",
    "model_name = \"BAAI/bge-base-zh-v1.5\" ; cache_dir = \"/content/drive/MyDrive/CS5242/bge-base-zh-v1.5\"\n",
    "test_data_name = \"test_public.json\" ; train_data_name = \"train.json\"\n",
    "task_name = \"SingleTowerBGE\"\n",
    "args = Args()\n",
    "args.lr = 3e-5\n",
    "args.num_epochs=5\n",
    "args.batch_size=64\n",
    "args.monitor = 'accuracy'\n",
    "devices = [torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")]\n",
    "download_model(model_name,cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTowerBGE(nn.Module):\n",
    "    \"\"\" Transformer encoder\"\"\"\n",
    "    def __init__(self, model_name,cache_dir,output_dim):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.cache_dir = cache_dir\n",
    "        self.bge = AutoModel.from_pretrained(self.model_name,cache_dir=self.cache_dir)\n",
    "        self.d_model = self.bge.config.hidden_size\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(self.d_model, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, **kwarg):\n",
    "        outputs = self.bge(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "        ).last_hidden_state\n",
    "        logits = self.clf(outputs[:,0,:])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processor = BGEProcessor(data_dir,train_data_name)\n",
    "train_examples = train_processor.get_examples()[:45000]\n",
    "test_processor = BGEProcessor(data_dir,test_data_name)\n",
    "test_examples = test_processor.get_examples()[:5000]\n",
    "train_examples, dev_examples = train_test_split(train_examples, test_size=1/9, random_state=42)\n",
    "\n",
    "train_data = SingleTowerBGEDataset(model_name, cache_dir,train_examples)\n",
    "dev_data = SingleTowerBGEDataset(model_name, cache_dir,dev_examples)\n",
    "test_data = SingleTowerBGEDataset(model_name, cache_dir,test_examples)\n",
    "\n",
    "train_iter = DataLoader(dataset=train_data,\n",
    "                        batch_size = args.batch_size,\n",
    "                        collate_fn=build_bge_collate_fn(\n",
    "                              AutoTokenizer.from_pretrained(\n",
    "                                  model_name,\n",
    "                                  cache_dir = cache_dir)),\n",
    "                        shuffle=False)\n",
    "dev_iter = DataLoader(dataset=dev_data,\n",
    "                        batch_size = args.batch_size,\n",
    "                        collate_fn=build_bge_collate_fn(\n",
    "                              AutoTokenizer.from_pretrained(\n",
    "                                  model_name,\n",
    "                                  cache_dir = cache_dir)),\n",
    "                        shuffle=False)\n",
    "test_iter = DataLoader(dataset=test_data,\n",
    "                       batch_size=args.batch_size,\n",
    "                       collate_fn=build_bge_collate_fn(\n",
    "                              AutoTokenizer.from_pretrained(\n",
    "                                  model_name,\n",
    "                                  cache_dir = cache_dir)),\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16000it [00:05, 2688.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:01, 1955.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 2373.35it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = SingleTowerBGEDataset(model_name, cache_dir,train_examples)\n",
    "dev_data = SingleTowerBGEDataset(model_name, cache_dir,dev_examples)\n",
    "test_data = SingleTowerBGEDataset(model_name, cache_dir,test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.238, train acc 0.913, valid loss 0.990, valid acc 0.679\n",
      "175.9 examples/sec on [device(type='cuda')]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"435.854688pt\" height=\"321.95625pt\" viewBox=\"0 0 435.854688 321.95625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-04-26T07:54:47.930962</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 321.95625 \n",
       "L 435.854688 321.95625 \n",
       "L 435.854688 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 284.4 \n",
       "L 420.703125 284.4 \n",
       "L 420.703125 7.2 \n",
       "L 30.103125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 30.103125 284.4 \n",
       "L 30.103125 7.2 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m14a0c43bb0\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14a0c43bb0\" x=\"30.103125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(22.151563 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 78.928125 284.4 \n",
       "L 78.928125 7.2 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14a0c43bb0\" x=\"78.928125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(70.976563 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 127.753125 284.4 \n",
       "L 127.753125 7.2 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14a0c43bb0\" x=\"127.753125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(119.801563 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 176.578125 284.4 \n",
       "L 176.578125 7.2 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14a0c43bb0\" x=\"176.578125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 2.5 -->\n",
       "      <g transform=\"translate(168.626563 298.998438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 225.403125 284.4 \n",
       "L 225.403125 7.2 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14a0c43bb0\" x=\"225.403125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 3.0 -->\n",
       "      <g transform=\"translate(217.451563 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 274.228125 284.4 \n",
       "L 274.228125 7.2 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14a0c43bb0\" x=\"274.228125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 3.5 -->\n",
       "      <g transform=\"translate(266.276563 298.998438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 323.053125 284.4 \n",
       "L 323.053125 7.2 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14a0c43bb0\" x=\"323.053125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 4.0 -->\n",
       "      <g transform=\"translate(315.101562 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 371.878125 284.4 \n",
       "L 371.878125 7.2 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14a0c43bb0\" x=\"371.878125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 4.5 -->\n",
       "      <g transform=\"translate(363.926562 298.998438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 420.703125 284.4 \n",
       "L 420.703125 7.2 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m14a0c43bb0\" x=\"420.703125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 5.0 -->\n",
       "      <g transform=\"translate(412.751562 298.998438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(210.175 312.676562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(61.523438 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(125 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(186.181641 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(241.162109 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 30.103125 251.046671 \n",
       "L 420.703125 251.046671 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <defs>\n",
       "       <path id=\"m4c4016ba45\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4c4016ba45\" x=\"30.103125\" y=\"251.046671\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.3 -->\n",
       "      <g transform=\"translate(7.2 254.84589) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 30.103125 217.54693 \n",
       "L 420.703125 217.54693 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4c4016ba45\" x=\"30.103125\" y=\"217.54693\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(7.2 221.346148) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 30.103125 184.047188 \n",
       "L 420.703125 184.047188 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4c4016ba45\" x=\"30.103125\" y=\"184.047188\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(7.2 187.846407) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 30.103125 150.547446 \n",
       "L 420.703125 150.547446 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4c4016ba45\" x=\"30.103125\" y=\"150.547446\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(7.2 154.346665) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path d=\"M 30.103125 117.047705 \n",
       "L 420.703125 117.047705 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4c4016ba45\" x=\"30.103125\" y=\"117.047705\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.7 -->\n",
       "      <g transform=\"translate(7.2 120.846924) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <path d=\"M 30.103125 83.547963 \n",
       "L 420.703125 83.547963 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_30\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4c4016ba45\" x=\"30.103125\" y=\"83.547963\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(7.2 87.347182) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_31\">\n",
       "      <path d=\"M 30.103125 50.048222 \n",
       "L 420.703125 50.048222 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_32\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4c4016ba45\" x=\"30.103125\" y=\"50.048222\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 0.9 -->\n",
       "      <g transform=\"translate(7.2 53.84744) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_33\">\n",
       "      <path d=\"M 30.103125 16.54848 \n",
       "L 420.703125 16.54848 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_34\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m4c4016ba45\" x=\"30.103125\" y=\"16.54848\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(7.2 20.347699) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_35\">\n",
       "    <path d=\"M -1 86.666756 \n",
       "L 10.573125 91.51803 \n",
       "L 30.103125 96.649176 \n",
       "L 49.633125 122.711187 \n",
       "L 69.163125 135.026604 \n",
       "L 88.693125 141.890957 \n",
       "L 108.223125 147.47729 \n",
       "L 127.753125 150.376404 \n",
       "L 147.283125 165.224063 \n",
       "L 166.813125 182.351369 \n",
       "L 186.343125 191.238502 \n",
       "L 205.873125 197.491108 \n",
       "L 225.403125 201.481447 \n",
       "L 244.933125 214.438531 \n",
       "L 264.463125 230.999085 \n",
       "L 283.993125 239.159909 \n",
       "L 303.523125 243.65065 \n",
       "L 323.053125 246.014772 \n",
       "L 342.583125 252.692574 \n",
       "L 362.113125 263.616104 \n",
       "L 381.643125 268.876255 \n",
       "L 401.173125 271.580845 \n",
       "L 420.703125 271.8 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_36\">\n",
       "    <path d=\"M -1 135.577072 \n",
       "L 10.573125 132.886801 \n",
       "L 30.103125 130.640225 \n",
       "L 49.633125 116.042713 \n",
       "L 69.163125 109.887135 \n",
       "L 88.693125 106.579036 \n",
       "L 108.223125 104.10843 \n",
       "L 127.753125 103.43006 \n",
       "L 147.283125 95.565996 \n",
       "L 166.813125 87.211997 \n",
       "L 186.343125 82.529013 \n",
       "L 205.873125 79.5594 \n",
       "L 225.403125 77.819507 \n",
       "L 244.933125 71.906803 \n",
       "L 264.463125 63.448118 \n",
       "L 283.993125 59.749188 \n",
       "L 303.523125 58.035816 \n",
       "L 323.053125 57.032918 \n",
       "L 342.583125 55.450055 \n",
       "L 362.113125 50.006347 \n",
       "L 381.643125 47.521783 \n",
       "L 401.173125 46.195751 \n",
       "L 420.703125 45.844004 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_37\">\n",
       "    <path d=\"M 30.103125 123.29103 \n",
       "L 127.753125 107.402533 \n",
       "L 225.403125 70.217615 \n",
       "L 323.053125 37.50893 \n",
       "L 420.703125 19.8 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_38\">\n",
       "    <path d=\"M 30.103125 114.769722 \n",
       "L 127.753125 117.382702 \n",
       "L 225.403125 119.794684 \n",
       "L 323.053125 127.231626 \n",
       "L 420.703125 124.082651 \n",
       "\" clip-path=\"url(#p5fd9924327)\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 284.4 \n",
       "L 30.103125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 420.703125 284.4 \n",
       "L 420.703125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 284.4 \n",
       "L 420.703125 284.4 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 7.2 \n",
       "L 420.703125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 37.103125 73.9125 \n",
       "L 115.548438 73.9125 \n",
       "Q 117.548438 73.9125 117.548438 71.9125 \n",
       "L 117.548438 14.2 \n",
       "Q 117.548438 12.2 115.548438 12.2 \n",
       "L 37.103125 12.2 \n",
       "Q 35.103125 12.2 35.103125 14.2 \n",
       "L 35.103125 71.9125 \n",
       "Q 35.103125 73.9125 37.103125 73.9125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_39\">\n",
       "     <path d=\"M 39.103125 20.298437 \n",
       "L 49.103125 20.298437 \n",
       "L 59.103125 20.298437 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- train loss -->\n",
       "     <g transform=\"translate(67.103125 23.798437) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(80.322266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(141.601562 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(169.384766 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(232.763672 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(264.550781 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(292.333984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(353.515625 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(405.615234 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_40\">\n",
       "     <path d=\"M 39.103125 34.976562 \n",
       "L 49.103125 34.976562 \n",
       "L 59.103125 34.976562 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- train acc -->\n",
       "     <g transform=\"translate(67.103125 38.476562) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(80.322266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(141.601562 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(169.384766 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(232.763672 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(264.550781 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(325.830078 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(380.810547 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_41\">\n",
       "     <path d=\"M 39.103125 49.654687 \n",
       "L 49.103125 49.654687 \n",
       "L 59.103125 49.654687 \n",
       "\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_21\">\n",
       "     <!-- valid loss -->\n",
       "     <g transform=\"translate(67.103125 53.154687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(59.179688 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(120.458984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(148.242188 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(176.025391 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(239.501953 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(271.289062 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(299.072266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(360.253906 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(412.353516 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_42\">\n",
       "     <path d=\"M 39.103125 64.332812 \n",
       "L 49.103125 64.332812 \n",
       "L 59.103125 64.332812 \n",
       "\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_22\">\n",
       "     <!-- valid acc -->\n",
       "     <g transform=\"translate(67.103125 67.832812) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(59.179688 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(120.458984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(148.242188 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(176.025391 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(239.501953 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(271.289062 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(332.568359 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(387.548828 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5fd9924327\">\n",
       "   <rect x=\"30.103125\" y=\"7.2\" width=\"390.6\" height=\"277.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = SingleTowerBGE(model_name,cache_dir,args.num_class).to(devices[0])\n",
    "train_transformer(net,\n",
    "                  train_iter,\n",
    "                  dev_iter,\n",
    "                  args.num_epochs,\n",
    "                  args.optimizer,\n",
    "                  args.lr,\n",
    "                  args.weight_decay,\n",
    "                  args.num_warmup_steps,\n",
    "                  devices,\n",
    "                  task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "79it [00:09,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on final epoch -- acc: 67.60, f1-macro: 59.46\n",
      "test on /content/drive/MyDrive/CS5242/ckpt/SingleTowerBGE-epoch0-val_acc0.71.pt -- acc: 70.20, f1-macro: 55.78\n"
     ]
    }
   ],
   "source": [
    "test(net,test_iter,None,device = devices[0])\n",
    "test_on_checkpoint(net, args.pt_path, test_iter, None, device=devices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.10 <a name='5.8'></a> Dual Tower Relevance Modeling with BGE\n",
    "[Back to Table of Contents](#top)\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"img/dual_tower_bge.jpg\" alt=\"dual_tower_bge\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Dual Tower Relevance Modeling with BGE refers to a design where the query and document are processed separately through two independent encoders, typically based on transformer architectures. Each encoder generates a dense embedding for its respective input (query or document). The relevance score is then computed based on the similarity (e.g., dot product or cosine similarity) between the two embeddings.\n",
    "\n",
    "Compared to single-tower models, the dual-tower approach enables much faster inference because query and document embeddings can be pre-computed and stored for efficient retrieval. Although dual-tower models might sacrifice some fine-grained interaction between the query and document, they are highly scalable and particularly effective for large-scale retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 23,953,920, Trainable: 23,953,920\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 512, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 512)\n",
      "    (token_type_embeddings): Embedding(2, 512)\n",
      "    (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-3): 4 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/content/drive/MyDrive/CS5242/\"\n",
    "model_name = \"BAAI/bge-small-zh-v1.5\" ; cache_dir = \"/content/drive/MyDrive/CS5242/bge-small-zh-v1.5\"\n",
    "test_data_name = \"test_public.json\" ; train_data_name = \"train.json\"\n",
    "args = Args()\n",
    "args.lr = 3e-5\n",
    "args.num_epochs=5\n",
    "args.batch_size=64\n",
    "args.monitor = 'accuracy'\n",
    "devices = [torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")]\n",
    "task_name = \"DualTowerBGE\"\n",
    "download_model(model_name,cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting data: 100%|██████████| 40000/40000 [00:16<00:00, 2410.45it/s]\n",
      "Converting data: 100%|██████████| 5000/5000 [00:02<00:00, 1892.07it/s]\n",
      "Converting data: 100%|██████████| 5000/5000 [00:02<00:00, 2494.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train_processor = BGEProcessor(data_dir,train_data_name)\n",
    "train_examples = train_processor.get_examples()[:45000]\n",
    "test_processor = BGEProcessor(data_dir,test_data_name)\n",
    "test_examples = test_processor.get_examples()[:5000]\n",
    "train_examples, dev_examples = train_test_split(train_examples, test_size=1/9, random_state=42)\n",
    "\n",
    "train_data = DualTowerBGEDataset(model_name, cache_dir,train_examples)\n",
    "dev_data = DualTowerBGEDataset(model_name, cache_dir,dev_examples)\n",
    "test_data = DualTowerBGEDataset(model_name, cache_dir,test_examples)\n",
    "\n",
    "train_iter = DataLoader(dataset=train_data,\n",
    "                        batch_size = args.batch_size,\n",
    "                        collate_fn = build_dual_bge_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                        shuffle=False)\n",
    "dev_iter = DataLoader(dataset=dev_data,\n",
    "                        batch_size = args.batch_size,\n",
    "                        collate_fn = build_dual_bge_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                        shuffle=False)\n",
    "test_iter = DataLoader(dataset=test_data,\n",
    "                       batch_size=args.batch_size,\n",
    "                       collate_fn = build_dual_bge_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualTowerBGE(nn.Module):\n",
    "    \"\"\" Dual Tower Transformer Encoder \"\"\"\n",
    "    def __init__(self, model_name, cache_dir, output_dim):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.cache_dir = cache_dir\n",
    "        self.query_encoder = AutoModel.from_pretrained(self.model_name, cache_dir=self.cache_dir)\n",
    "        self.doc_encoder = AutoModel.from_pretrained(self.model_name, cache_dir=self.cache_dir)\n",
    "        self.d_model = self.query_encoder.config.hidden_size\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(self.d_model * 2, self.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(self.d_model, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, query_input_ids,\n",
    "                      query_attention_mask,\n",
    "                      query_token_type_ids,\n",
    "                      doc_input_ids,\n",
    "                      doc_attention_mask,\n",
    "                      doc_token_type_ids,\n",
    "                      **kwargs):\n",
    "        '''\n",
    "        query_inputs: dict, doc_inputs: dict\n",
    "        Including input_ids, attention_mask, token_type_ids\n",
    "        '''\n",
    "        query_inputs = {\"input_ids\":query_input_ids,\n",
    "                        \"attention_mask\":query_attention_mask,\n",
    "                        \"token_type_ids\":query_token_type_ids}\n",
    "        doc_inputs   = {\"input_ids\":doc_input_ids,\n",
    "                        \"attention_mask\":doc_attention_mask,\n",
    "                        \"token_type_ids\":doc_token_type_ids}\n",
    "        query_output = self.query_encoder(**query_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "        doc_output = self.doc_encoder(**doc_inputs).last_hidden_state[:, 0, :]        # [CLS]\n",
    "        concat = torch.cat([query_output, doc_output], dim=-1)  # [batch, 2*d_model]\n",
    "        logits = self.clf(concat)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.499, train acc 0.796, valid loss 0.864, valid acc 0.663\n",
      "814.2 examples/sec on [device(type='cuda')]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"435.854688pt\" height=\"321.95625pt\" viewBox=\"0 0 435.854688 321.95625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-04-26T08:01:50.336303</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 321.95625 \n",
       "L 435.854688 321.95625 \n",
       "L 435.854688 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 284.4 \n",
       "L 420.703125 284.4 \n",
       "L 420.703125 7.2 \n",
       "L 30.103125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 30.103125 284.4 \n",
       "L 30.103125 7.2 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mb444dfbb9b\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb444dfbb9b\" x=\"30.103125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(22.151563 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 78.928125 284.4 \n",
       "L 78.928125 7.2 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb444dfbb9b\" x=\"78.928125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(70.976563 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 127.753125 284.4 \n",
       "L 127.753125 7.2 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb444dfbb9b\" x=\"127.753125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(119.801563 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 176.578125 284.4 \n",
       "L 176.578125 7.2 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb444dfbb9b\" x=\"176.578125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 2.5 -->\n",
       "      <g transform=\"translate(168.626563 298.998438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 225.403125 284.4 \n",
       "L 225.403125 7.2 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb444dfbb9b\" x=\"225.403125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 3.0 -->\n",
       "      <g transform=\"translate(217.451563 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 274.228125 284.4 \n",
       "L 274.228125 7.2 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb444dfbb9b\" x=\"274.228125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 3.5 -->\n",
       "      <g transform=\"translate(266.276563 298.998438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 323.053125 284.4 \n",
       "L 323.053125 7.2 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb444dfbb9b\" x=\"323.053125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 4.0 -->\n",
       "      <g transform=\"translate(315.101562 298.998438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 371.878125 284.4 \n",
       "L 371.878125 7.2 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb444dfbb9b\" x=\"371.878125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 4.5 -->\n",
       "      <g transform=\"translate(363.926562 298.998438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 420.703125 284.4 \n",
       "L 420.703125 7.2 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb444dfbb9b\" x=\"420.703125\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 5.0 -->\n",
       "      <g transform=\"translate(412.751562 298.998438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(210.175 312.676562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(61.523438 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(125 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(186.181641 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(241.162109 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 30.103125 270.488187 \n",
       "L 420.703125 270.488187 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <defs>\n",
       "       <path id=\"mb34ad4dc30\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb34ad4dc30\" x=\"30.103125\" y=\"270.488187\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(7.2 274.287406) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 30.103125 216.720978 \n",
       "L 420.703125 216.720978 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb34ad4dc30\" x=\"30.103125\" y=\"216.720978\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(7.2 220.520197) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 30.103125 162.953769 \n",
       "L 420.703125 162.953769 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb34ad4dc30\" x=\"30.103125\" y=\"162.953769\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.7 -->\n",
       "      <g transform=\"translate(7.2 166.752988) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 30.103125 109.18656 \n",
       "L 420.703125 109.18656 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb34ad4dc30\" x=\"30.103125\" y=\"109.18656\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(7.2 112.985779) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path d=\"M 30.103125 55.419351 \n",
       "L 420.703125 55.419351 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb34ad4dc30\" x=\"30.103125\" y=\"55.419351\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.9 -->\n",
       "      <g transform=\"translate(7.2 59.21857) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_29\">\n",
       "    <path d=\"M -1 77.615319 \n",
       "L 10.573125 83.543851 \n",
       "L 30.103125 90.125219 \n",
       "L 49.633125 122.339215 \n",
       "L 69.163125 130.142567 \n",
       "L 88.693125 140.434384 \n",
       "L 108.223125 144.901823 \n",
       "L 127.753125 147.508174 \n",
       "L 147.283125 163.864414 \n",
       "L 166.813125 173.222176 \n",
       "L 186.343125 188.248744 \n",
       "L 205.873125 194.185119 \n",
       "L 225.403125 197.585862 \n",
       "L 244.933125 211.99531 \n",
       "L 264.463125 221.238985 \n",
       "L 283.993125 236.52558 \n",
       "L 303.523125 243.125317 \n",
       "L 323.053125 244.964488 \n",
       "L 342.583125 246.546829 \n",
       "L 362.113125 252.806168 \n",
       "L 381.643125 266.932524 \n",
       "L 401.173125 271.8 \n",
       "L 420.703125 270.870628 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_30\">\n",
       "    <path d=\"M -1 207.755571 \n",
       "L 10.573125 204.556147 \n",
       "L 30.103125 201.316673 \n",
       "L 49.633125 183.116473 \n",
       "L 69.163125 180.999389 \n",
       "L 88.693125 176.261154 \n",
       "L 108.223125 173.47198 \n",
       "L 127.753125 173.075446 \n",
       "L 147.283125 165.373294 \n",
       "L 166.813125 160.836685 \n",
       "L 186.343125 153.813344 \n",
       "L 205.873125 150.570509 \n",
       "L 225.403125 149.216247 \n",
       "L 244.933125 139.901078 \n",
       "L 264.463125 134.725985 \n",
       "L 283.993125 127.75865 \n",
       "L 303.523125 124.493413 \n",
       "L 323.053125 123.838125 \n",
       "L 342.583125 122.493945 \n",
       "L 362.113125 119.704771 \n",
       "L 381.643125 112.524608 \n",
       "L 401.173125 110.329113 \n",
       "L 420.703125 111.256598 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_31\">\n",
       "    <path d=\"M 30.103125 130.114438 \n",
       "L 127.753125 130.760294 \n",
       "L 225.403125 112.055445 \n",
       "L 323.053125 81.24973 \n",
       "L 420.703125 74.88189 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_32\">\n",
       "    <path d=\"M 30.103125 182.955171 \n",
       "L 127.753125 178.868863 \n",
       "L 225.403125 182.309965 \n",
       "L 323.053125 182.094896 \n",
       "L 420.703125 183.062705 \n",
       "\" clip-path=\"url(#p3f05c9dc2f)\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 284.4 \n",
       "L 30.103125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 420.703125 284.4 \n",
       "L 420.703125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 284.4 \n",
       "L 420.703125 284.4 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 7.2 \n",
       "L 420.703125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 335.257812 73.9125 \n",
       "L 413.703125 73.9125 \n",
       "Q 415.703125 73.9125 415.703125 71.9125 \n",
       "L 415.703125 14.2 \n",
       "Q 415.703125 12.2 413.703125 12.2 \n",
       "L 335.257812 12.2 \n",
       "Q 333.257812 12.2 333.257812 14.2 \n",
       "L 333.257812 71.9125 \n",
       "Q 333.257812 73.9125 335.257812 73.9125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_33\">\n",
       "     <path d=\"M 337.257812 20.298437 \n",
       "L 347.257812 20.298437 \n",
       "L 357.257812 20.298437 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- train loss -->\n",
       "     <g transform=\"translate(365.257812 23.798437) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(80.322266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(141.601562 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(169.384766 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(232.763672 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(264.550781 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(292.333984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(353.515625 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(405.615234 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_34\">\n",
       "     <path d=\"M 337.257812 34.976562 \n",
       "L 347.257812 34.976562 \n",
       "L 357.257812 34.976562 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- train acc -->\n",
       "     <g transform=\"translate(365.257812 38.476562) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(80.322266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(141.601562 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(169.384766 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(232.763672 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(264.550781 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(325.830078 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(380.810547 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_35\">\n",
       "     <path d=\"M 337.257812 49.654687 \n",
       "L 347.257812 49.654687 \n",
       "L 357.257812 49.654687 \n",
       "\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_18\">\n",
       "     <!-- valid loss -->\n",
       "     <g transform=\"translate(365.257812 53.154687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(59.179688 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(120.458984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(148.242188 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(176.025391 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(239.501953 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(271.289062 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(299.072266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(360.253906 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(412.353516 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_36\">\n",
       "     <path d=\"M 337.257812 64.332812 \n",
       "L 347.257812 64.332812 \n",
       "L 357.257812 64.332812 \n",
       "\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- valid acc -->\n",
       "     <g transform=\"translate(365.257812 67.832812) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(59.179688 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(120.458984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(148.242188 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(176.025391 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(239.501953 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(271.289062 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(332.568359 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(387.548828 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p3f05c9dc2f\">\n",
       "   <rect x=\"30.103125\" y=\"7.2\" width=\"390.6\" height=\"277.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = DualTowerBGE(model_name,cache_dir,args.num_class).to(devices[0])\n",
    "train_transformer(net,\n",
    "                  train_iter,\n",
    "                  dev_iter,\n",
    "                  args.num_epochs,\n",
    "                  args.optimizer,\n",
    "                  args.lr,\n",
    "                  args.weight_decay,\n",
    "                  args.num_warmup_steps,\n",
    "                  devices,\n",
    "                  task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "79it [00:02, 34.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on final epoch -- acc: 66.08, f1-macro: 54.36\n",
      "test on /content/drive/MyDrive/CS5242/ckpt/DualTowerBGE-epoch1-val_acc0.67.pt -- acc: 66.84, f1-macro: 50.86\n"
     ]
    }
   ],
   "source": [
    "test(net,test_iter,None,device = devices[0])\n",
    "test_on_checkpoint(net, args.pt_path, test_iter, None, device=devices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.11 <a name='5.8'></a> Knowledge Distillation\n",
    "[Back to Table of Contents](#top)\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"img/knowledge_dis.jpg\" alt=\"knowledge_dis\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "A powerful single-tower model is used as the **teacher** to train a dual-tower **student** through knowledge distillation.\n",
    "\n",
    "The **single-tower teacher** computes high-quality similarity scores by jointly encoding the query and document.\n",
    "The **dual-tower student** encodes query and document separately and tries to match the teacher's scores.\n",
    "The student learns to approximate the teacher’s behavior while maintaining efficiency.\n",
    "\n",
    "Dual-tower is much faster at inference (suitable for large-scale retrieval).Single-tower provides more accurate supervision thanks to full input interaction.Knowledge Distillation Combine the performance of a strong teacher with the speed of a lightweight student.\n",
    "\n",
    "**KL divergence loss:**\n",
    "$$\n",
    "\\mathcal{L}_{\\text{KD}} = T^2 \\times \\sum_{i} p^t_i \\log\\left( \\frac{p^t_i}{p^s_i} \\right)\n",
    "$$\n",
    "\n",
    "**Total loss combining hard and soft targets:**\n",
    "$$\n",
    "\\mathcal{L} = \\alpha \\times \\mathcal{L}_{\\text{CE}}(y, p^s) + (1 - \\alpha) \\times \\mathcal{L}_{\\text{KD}}\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- Loss between teacher and student outputs: $ \\mathcal{L}_{\\text{KD}} $\n",
    "- Cross-entropy loss between true labels and student predictions: $ \\mathcal{L}_{\\text{CE}} $\n",
    "- Teacher soft targets after temperature scaling:  $ p^t $\n",
    "- Student soft targets after temperature scaling:  $ p^s $\n",
    "- Temperature parameter: $ T $\n",
    "- Balance hyperparameter: $ \\alpha \\in [0, 1] $\n",
    "- Ground-truth labels: $ y $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_name = \"BAAI/bge-small-zh-v1.5\"\n",
    "student_cache_dir = \"/content/drive/MyDrive/CS5242/bge-small-zh-v1.5\"\n",
    "\n",
    "teacher_model_name = \"BAAI/bge-base-zh-v1.5\"\n",
    "teacher_cache_dir = \"/content/drive/MyDrive/CS5242/bge-base-zh-v1.5\"\n",
    "\n",
    "data_dir = \"/content/drive/MyDrive/CS5242/\"\n",
    "test_data_name = \"test_public.json\"\n",
    "train_data_name = \"train.json\"\n",
    "\n",
    "task_name = \"NDDualTowerBGE\"\n",
    "devices = [torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")]\n",
    "\n",
    "args = Args()\n",
    "args.lr = 3e-5\n",
    "args.num_epochs=5\n",
    "args.batch_size=64\n",
    "args.monitor = 'accuracy'\n",
    "\n",
    "models = os.listdir(\"/content/drive/MyDrive/CS5242/ckpt/\")\n",
    "for model in models:\n",
    "  if \"SingleTowerBGE-epoch0-val_acc0.71\" in model:\n",
    "    teacher_ckpt_path = os.path.join(\"/content/drive/MyDrive/CS5242/ckpt/\",model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting data: 100%|██████████| 40000/40000 [00:17<00:00, 2342.73it/s]\n",
      "Converting data: 100%|██████████| 5000/5000 [00:02<00:00, 2451.37it/s]\n",
      "Converting data: 100%|██████████| 5000/5000 [00:02<00:00, 2456.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40000it [00:13, 2864.55it/s]\n"
     ]
    }
   ],
   "source": [
    "teacher_net = SingleTowerBGE(teacher_model_name, teacher_cache_dir, args.num_class)\n",
    "teacher_net.load_state_dict(torch.load(teacher_ckpt_path))\n",
    "teacher_net.to(devices[0])\n",
    "teacher_net.eval()\n",
    "for param in teacher_net.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "student_net = DualTowerBGE(student_model_name, student_cache_dir, args.num_class)\n",
    "student_net.to(devices[0])\n",
    "\n",
    "train_processor = BGEProcessor(data_dir,train_data_name)\n",
    "train_examples = train_processor.get_examples()[:45000]\n",
    "test_processor = BGEProcessor(data_dir,test_data_name)\n",
    "test_examples = test_processor.get_examples()[:5000]\n",
    "train_examples, dev_examples = train_test_split(train_examples, test_size=1/9, random_state=42)\n",
    "\n",
    "student_train_data = DualTowerBGEDataset(student_model_name, student_cache_dir,train_examples)\n",
    "student_dev_data = DualTowerBGEDataset(student_model_name, student_cache_dir,dev_examples)\n",
    "student_test_data = DualTowerBGEDataset(student_model_name, student_cache_dir,test_examples)\n",
    "\n",
    "teacher_train_data = SingleTowerBGEDataset(teacher_model_name, teacher_cache_dir, train_examples)\n",
    "\n",
    "student_train_iter = DataLoader(dataset=student_train_data,\n",
    "                        batch_size = args.batch_size,\n",
    "                        collate_fn = build_dual_bge_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                        shuffle=False)\n",
    "student_dev_iter = DataLoader(dataset=student_dev_data,\n",
    "                        batch_size = args.batch_size,\n",
    "                        collate_fn = build_dual_bge_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                        shuffle=False)\n",
    "student_test_iter = DataLoader(dataset=student_test_data,\n",
    "                       batch_size=args.batch_size,\n",
    "                       collate_fn = build_dual_bge_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                       shuffle=False)\n",
    "teacher_train_iter = DataLoader(dataset=teacher_train_data,\n",
    "                       batch_size=args.batch_size,\n",
    "                       collate_fn = build_bge_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1)\n",
    "    ) * (T * T)\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "\n",
    "def train_ND_batch(teacher_net, teacher_batch, student_net, student_batch, loss_fn, trainer, devices):\n",
    "    teacher_net.eval()\n",
    "    student_net.train()\n",
    "\n",
    "    teacher_batch = {k: v.to(devices[0]) for k, v in teacher_batch.items()}\n",
    "    student_batch = {k: v.to(devices[0]) for k, v in student_batch.items()}\n",
    "    labels = student_batch['label']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher_net(**teacher_batch)\n",
    "\n",
    "    student_logits = student_net(**student_batch)\n",
    "\n",
    "    loss = loss_fn(student_logits, teacher_logits, labels)\n",
    "    trainer.zero_grad()\n",
    "    loss.backward()\n",
    "    trainer.step()\n",
    "    args.learning_rates.append(trainer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return loss.item() * labels.shape[0], accuracy(student_logits, labels)\n",
    "\n",
    "def train_ND(teacher_net,\n",
    "             student_net,\n",
    "             teacher_train_iter,\n",
    "             student_train_iter,\n",
    "             valid_iter,\n",
    "             num_epochs,\n",
    "             optimizer,\n",
    "             lr,\n",
    "             wd,\n",
    "             num_warmup_steps,\n",
    "             devices,\n",
    "             task_name):\n",
    "    if optimizer == 'sgd':\n",
    "        trainer = torch.optim.SGD(student_net.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    elif optimizer == 'adam':\n",
    "        trainer = torch.optim.Adam(student_net.parameters(), lr=lr, weight_decay=wd)\n",
    "    elif optimizer == 'adamw':\n",
    "        trainer = torch.optim.AdamW(student_net.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    model_name = str(net.__class__.__name__)\n",
    "    scheduler = get_cosine_schedule_with_warmup(trainer, num_warmup_steps=num_warmup_steps, num_training_steps=num_epochs*len(student_train_iter))\n",
    "\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    timer = Timer()\n",
    "    num_batches = len(train_iter)\n",
    "    legend = ['train loss', 'train acc']\n",
    "    if valid_iter is not None:\n",
    "        legend += ['valid loss', 'valid acc']\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], legend=legend, figsize=(7, 5))\n",
    "\n",
    "    if args.monitor == 'loss':\n",
    "        monitor_val = math.inf\n",
    "    else:\n",
    "        monitor_val = -math.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        metric = Accumulator(4)\n",
    "        for i, (teacher_batch, student_batch) in enumerate(tqdm(zip(teacher_train_iter, student_train_iter))):\n",
    "            output = train_ND_batch(teacher_net,\n",
    "                                    teacher_batch,\n",
    "                                    student_net,\n",
    "                                    student_batch,\n",
    "                                    distillation_loss,\n",
    "                                    trainer,\n",
    "                                    devices)\n",
    "            timer.start()\n",
    "            labels = student_batch['label']\n",
    "            scheduler.step()\n",
    "            metric.add(output[0], output[1], labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2]  , metric[1] / metric[2], None, None))\n",
    "\n",
    "        if valid_iter is not None:\n",
    "            valid_loss, valid_acc = evaluate_loss_and_acc_gpu(student_net, valid_iter, loss, devices[0])\n",
    "            animator.add(epoch + 1, (None, None, valid_loss, valid_acc))\n",
    "            if args.monitor == 'loss':\n",
    "                if valid_loss < monitor_val:\n",
    "                    filename = task_name + f\"-epoch{epoch}-val_loss{valid_loss :.2f}.pt\"\n",
    "                    best_model_state = copy.deepcopy(student_net.state_dict())\n",
    "                    monitor_val = valid_loss\n",
    "            else:\n",
    "                if valid_acc > monitor_val:\n",
    "                    filename = task_name + f\"-epoch{epoch}-val_acc{valid_acc :.2f}.pt\"\n",
    "                    best_model_state = copy.deepcopy(student_net.state_dict())\n",
    "                    monitor_val = valid_acc\n",
    "\n",
    "\n",
    "    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n",
    "                f'train acc {metric[1] / metric[2]:.3f}')\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid loss {valid_loss:.3f}'\n",
    "        measures += f', valid acc  {valid_acc: .3f}'\n",
    "    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')\n",
    "    args.pt_path = os.path.join(args.pt_path, filename)\n",
    "    torch.save(best_model_state, args.pt_path)\n",
    "    animator.save(args.img_path, filename + f\"-train_loss{metric[0] / metric[2]:.2f}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.363, train acc 0.758, valid loss 0.777, valid acc   0.675\n",
      "1191269.2 examples/sec on [device(type='cuda')]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"435.854688pt\" height=\"324.534926pt\" viewBox=\"0 0 435.854688 324.534926\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-04-26T08:47:18.331825</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 324.534926 \n",
       "L 435.854688 324.534926 \n",
       "L 435.854688 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 286.978676 \n",
       "L 420.703125 286.978676 \n",
       "L 420.703125 9.778676 \n",
       "L 30.103125 9.778676 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 30.103125 286.978676 \n",
       "L 30.103125 9.778676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mbb1c8e3ef6\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbb1c8e3ef6\" x=\"30.103125\" y=\"286.978676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(22.151563 301.577114) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 78.928125 286.978676 \n",
       "L 78.928125 9.778676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbb1c8e3ef6\" x=\"78.928125\" y=\"286.978676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(70.976563 301.577114) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 127.753125 286.978676 \n",
       "L 127.753125 9.778676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbb1c8e3ef6\" x=\"127.753125\" y=\"286.978676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(119.801563 301.577114) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 176.578125 286.978676 \n",
       "L 176.578125 9.778676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbb1c8e3ef6\" x=\"176.578125\" y=\"286.978676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 2.5 -->\n",
       "      <g transform=\"translate(168.626563 301.577114) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 225.403125 286.978676 \n",
       "L 225.403125 9.778676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbb1c8e3ef6\" x=\"225.403125\" y=\"286.978676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 3.0 -->\n",
       "      <g transform=\"translate(217.451563 301.577114) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 274.228125 286.978676 \n",
       "L 274.228125 9.778676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbb1c8e3ef6\" x=\"274.228125\" y=\"286.978676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 3.5 -->\n",
       "      <g transform=\"translate(266.276563 301.577114) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 323.053125 286.978676 \n",
       "L 323.053125 9.778676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbb1c8e3ef6\" x=\"323.053125\" y=\"286.978676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 4.0 -->\n",
       "      <g transform=\"translate(315.101562 301.577114) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 371.878125 286.978676 \n",
       "L 371.878125 9.778676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbb1c8e3ef6\" x=\"371.878125\" y=\"286.978676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 4.5 -->\n",
       "      <g transform=\"translate(363.926562 301.577114) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 420.703125 286.978676 \n",
       "L 420.703125 9.778676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mbb1c8e3ef6\" x=\"420.703125\" y=\"286.978676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 5.0 -->\n",
       "      <g transform=\"translate(412.751562 301.577114) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(210.175 315.255239) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(61.523438 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(125 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(186.181641 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(241.162109 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 30.103125 252.322351 \n",
       "L 420.703125 252.322351 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <defs>\n",
       "       <path id=\"maf5bde3bda\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#maf5bde3bda\" x=\"30.103125\" y=\"252.322351\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(7.2 256.12157) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 30.103125 191.991568 \n",
       "L 420.703125 191.991568 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#maf5bde3bda\" x=\"30.103125\" y=\"191.991568\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(7.2 195.790787) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 30.103125 131.660785 \n",
       "L 420.703125 131.660785 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#maf5bde3bda\" x=\"30.103125\" y=\"131.660785\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(7.2 135.460004) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 30.103125 71.330002 \n",
       "L 420.703125 71.330002 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#maf5bde3bda\" x=\"30.103125\" y=\"71.330002\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.7 -->\n",
       "      <g transform=\"translate(7.2 75.129221) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path d=\"M 30.103125 10.999219 \n",
       "L 420.703125 10.999219 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#maf5bde3bda\" x=\"30.103125\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(7.2 14.798438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_29\">\n",
       "    <path d=\"M -1 124.155778 \n",
       "L 10.573125 132.94467 \n",
       "L 30.103125 142.102286 \n",
       "L 49.633125 189.506628 \n",
       "L 69.163125 196.906606 \n",
       "L 88.693125 203.946499 \n",
       "L 108.223125 207.391992 \n",
       "L 127.753125 209.195936 \n",
       "L 147.283125 219.487572 \n",
       "L 166.813125 227.652527 \n",
       "L 186.343125 235.504563 \n",
       "L 205.873125 238.778579 \n",
       "L 225.403125 240.504613 \n",
       "L 244.933125 243.688747 \n",
       "L 264.463125 251.971409 \n",
       "L 283.993125 258.714528 \n",
       "L 303.523125 262.258722 \n",
       "L 323.053125 263.752413 \n",
       "L 342.583125 255.856679 \n",
       "L 362.113125 264.740079 \n",
       "L 381.643125 270.687002 \n",
       "L 401.173125 273.986712 \n",
       "L 420.703125 274.378676 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_30\">\n",
       "    <path d=\"M -1 110.295683 \n",
       "L 10.573125 107.848979 \n",
       "L 30.103125 106.729089 \n",
       "L 49.633125 94.632767 \n",
       "L 69.163125 91.729348 \n",
       "L 88.693125 85.306633 \n",
       "L 108.223125 82.28381 \n",
       "L 127.753125 81.661648 \n",
       "L 147.283125 78.117215 \n",
       "L 166.813125 74.308834 \n",
       "L 186.343125 66.704642 \n",
       "L 205.873125 63.637827 \n",
       "L 225.403125 62.989271 \n",
       "L 244.933125 58.811364 \n",
       "L 264.463125 56.624373 \n",
       "L 283.993125 50.113676 \n",
       "L 303.523125 46.066486 \n",
       "L 323.053125 44.603465 \n",
       "L 342.583125 49.761747 \n",
       "L 362.113125 45.123818 \n",
       "L 381.643125 38.600552 \n",
       "L 401.173125 35.640573 \n",
       "L 420.703125 36.217486 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_31\">\n",
       "    <path d=\"M 30.103125 31.858225 \n",
       "L 127.753125 35.549179 \n",
       "L 225.403125 33.435407 \n",
       "L 323.053125 22.378676 \n",
       "L 420.703125 24.863971 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_32\">\n",
       "    <path d=\"M 30.103125 90.635852 \n",
       "L 127.753125 87.498652 \n",
       "L 225.403125 86.412698 \n",
       "L 323.053125 85.326743 \n",
       "L 420.703125 86.171374 \n",
       "\" clip-path=\"url(#p5832274b67)\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 286.978676 \n",
       "L 30.103125 9.778676 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 420.703125 286.978676 \n",
       "L 420.703125 9.778676 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 286.978676 \n",
       "L 420.703125 286.978676 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 9.778676 \n",
       "L 420.703125 9.778676 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 37.103125 281.978676 \n",
       "L 115.548438 281.978676 \n",
       "Q 117.548438 281.978676 117.548438 279.978676 \n",
       "L 117.548438 222.266176 \n",
       "Q 117.548438 220.266176 115.548438 220.266176 \n",
       "L 37.103125 220.266176 \n",
       "Q 35.103125 220.266176 35.103125 222.266176 \n",
       "L 35.103125 279.978676 \n",
       "Q 35.103125 281.978676 37.103125 281.978676 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_33\">\n",
       "     <path d=\"M 39.103125 228.364614 \n",
       "L 49.103125 228.364614 \n",
       "L 59.103125 228.364614 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- train loss -->\n",
       "     <g transform=\"translate(67.103125 231.864614) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(80.322266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(141.601562 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(169.384766 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(232.763672 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(264.550781 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(292.333984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(353.515625 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(405.615234 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_34\">\n",
       "     <path d=\"M 39.103125 243.042739 \n",
       "L 49.103125 243.042739 \n",
       "L 59.103125 243.042739 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- train acc -->\n",
       "     <g transform=\"translate(67.103125 246.542739) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(80.322266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(141.601562 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(169.384766 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(232.763672 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(264.550781 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(325.830078 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(380.810547 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_35\">\n",
       "     <path d=\"M 39.103125 257.720864 \n",
       "L 49.103125 257.720864 \n",
       "L 59.103125 257.720864 \n",
       "\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_18\">\n",
       "     <!-- valid loss -->\n",
       "     <g transform=\"translate(67.103125 261.220864) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(59.179688 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(120.458984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(148.242188 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(176.025391 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(239.501953 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(271.289062 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(299.072266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(360.253906 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(412.353516 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_36\">\n",
       "     <path d=\"M 39.103125 272.398989 \n",
       "L 49.103125 272.398989 \n",
       "L 59.103125 272.398989 \n",
       "\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- valid acc -->\n",
       "     <g transform=\"translate(67.103125 275.898989) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(59.179688 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(120.458984 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(148.242188 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(176.025391 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(239.501953 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(271.289062 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(332.568359 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(387.548828 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5832274b67\">\n",
       "   <rect x=\"30.103125\" y=\"9.778676\" width=\"390.6\" height=\"277.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ND(\n",
    "          teacher_net,\n",
    "          student_net,\n",
    "          teacher_train_iter,\n",
    "          student_train_iter,\n",
    "          student_dev_iter,\n",
    "          args.num_epochs,\n",
    "          args.optimizer,\n",
    "          args.lr,\n",
    "          args.weight_decay,\n",
    "          args.num_warmup_steps,\n",
    "          devices,\n",
    "          task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:02, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on final epoch -- acc: 67.28, f1-macro: 53.53\n",
      "test on /content/drive/MyDrive/CS5242/ckpt/NDDualTowerBGE-epoch3-val_acc0.68.pt -- acc: 67.26, f1-macro: 53.68\n"
     ]
    }
   ],
   "source": [
    "test(student_net,test_iter,None,device = devices[0])\n",
    "test_on_checkpoint(student_net, args.pt_path, test_iter, None, device=devices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.12 <a name='5.8'></a> Relevance Modeling with Large Language Model\n",
    "[Back to Table of Contents](#top)\n",
    "\n",
    "**Data Preparation Phase**\n",
    "\n",
    "- **Data Conversion**: Convert the raw query, document, and label data into the input format required by the large model (ChatML conversation format). This step ensures that the data is structured appropriately for model consumption.\n",
    "- **Tokenization**: Once the data is converted, apply tokenization to split the content into smaller units (tokens). This will generate the following outputs:\n",
    "  - `input_ids`: Numerical representations of the tokens.\n",
    "  - `attention_mask`: A mask to indicate which tokens should be attended to by the model.\n",
    "  - `labels`: The ground truth labels that will be used for training the model, typically indicating the relevance score (0/1/2).\n",
    "\n",
    "**Model Training Phase**\n",
    "\n",
    "- **Model Selection**: Use a standard large language model (LLM) which is Qwen-0.5B, that is capable of processing the input query and document pairs.\n",
    "- **Input Structure**: The model receives a prompt consisting of the concatenated query and document. The model's goal is to predict the relevance score (0/1/2) for each query-document pair.\n",
    "- **Loss Function**: The training loss is computed using the standard language model loss, which is typically the cross-entropy loss applied to the token predictions. This loss guides the model to improve its predictions by adjusting the weights based on the error in token prediction.\n",
    "\n",
    "**Model Testing Phase**\n",
    "\n",
    "- **Input Structure**: During testing, the model is given the query and document as input but does not receive the correct answer label. The model must generate its own relevance score (0/1/2) based on its learned understanding.\n",
    "- **Generation and Evaluation**: The model's output is compared against the true label to assess its performance. The accuracy of the generated relevance score is measured to evaluate how well the model has learned to predict relevance.\n",
    "\n",
    "**Example**\n",
    "\n",
    "  System:\n",
    "\n",
    "  You are a search engine. You need to judge the relevance between a given user query and a document snippet.\n",
    "  Score according to the following rules, range 0–2:\n",
    "\n",
    "  0 – Not relevant;\n",
    "\n",
    "  1 – Somewhat relevant;\n",
    "\n",
    "  2 – Relevant.\n",
    "\n",
    "  User:\n",
    "\n",
    "  Query is \n",
    "  “From the date of filing for divorce, if either party does not agree to the divorce, they may apply to…”\n",
    "\n",
    "  Document is:\n",
    "\n",
    "  “According to the Civil Code of the People’s Republic of China, from the date the marriage registration authority receives a divorce registration application, if either party is unwilling to divorce, they may apply to the marriage registration authority for divorce registration.”\n",
    "\n",
    "  Please give the relevance score (just the number): \n",
    "  <span style=\"color:red;\">1 (label)</span>\n",
    "\n",
    "**The red 1 means that is what we want to predict which is the relevance label.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 494,032,768, Trainable: 494,032,768\n",
      "Qwen2Model(\n",
      "  (embed_tokens): Embedding(151936, 896)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x Qwen2DecoderLayer(\n",
      "      (self_attn): Qwen2Attention(\n",
      "        (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "        (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "        (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "        (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "        (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/content/drive/MyDrive/CS5242/\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "cache_dir = \"/content/drive/MyDrive/CS5242/Qwen2.5-0.5B\"\n",
    "\n",
    "test_data_name = \"test_public.json\"\n",
    "train_data_name = \"train.json\"\n",
    "\n",
    "args = Args()\n",
    "args.lr = 3e-4\n",
    "args.num_epochs=3\n",
    "args.batch_size=8\n",
    "args.monitor = 'accuracy'\n",
    "devices = [torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")]\n",
    "task_name = \"Relevance with LLM\"\n",
    "download_model(model_name,cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2LLMDataset(Dataset):\n",
    "    def __init__(self, model_name, cache_dir, samples):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, trust_remote_code=True\n",
    "        )\n",
    "        self.samples = samples\n",
    "        self.convert_data_to_features()\n",
    "\n",
    "    def convert_data_to_features(self):\n",
    "        self.data = []\n",
    "        system_prompt = \\\n",
    "        \"\"\"你是一个搜索引擎，需要判断给定的用户查询和文档片段之间的相关性。\n",
    "           请根据以下规则打分，范围 0 到 2：\\n\n",
    "           0-无关；1-一般相关；2-相关。\"\"\"\n",
    "        for sample in tqdm(self.samples, desc=\"Preparing ChatML prompts\"):\n",
    "            query = sample['query']\n",
    "            document = sample['title']\n",
    "            label = str(sample['label'])\n",
    "            messages = [\n",
    "                {\"role\": \"system\",    \"content\": system_prompt},\n",
    "                {\"role\": \"user\",      \"content\":\n",
    "                    f\"查询内容：{query}\\n\"\n",
    "                    f\"文档片段：{document}\\n\"\n",
    "                    \"请给出相关性评分（仅填写数字）：\"\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": label}\n",
    "            ]\n",
    "            # 先拼 prompt，再 tokenize\n",
    "            prompt = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False\n",
    "            )\n",
    "            encoded = self.tokenizer(\n",
    "                prompt,\n",
    "                truncation=False,\n",
    "                padding=False,\n",
    "                return_tensors=None\n",
    "            )\n",
    "            input_ids = encoded['input_ids'] + [self.tokenizer.eos_token_id]\n",
    "            attenttiom_mask = encoded['attention_mask'] + [1]\n",
    "            label_ids = input_ids[-4:]\n",
    "            labels = [-100] * (len(input_ids) - 4) + label_ids\n",
    "\n",
    "            self.data.append({\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\":attenttiom_mask,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def build_qwen2_collate_fn(tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        # 把 list-of-dicts 拆成三个 list-of-lists\n",
    "        input_ids_list     = [torch.tensor(ex[\"input_ids\"],     dtype=torch.long) for ex in batch]\n",
    "        attention_list     = [torch.tensor(ex[\"attention_mask\"], dtype=torch.long) for ex in batch]\n",
    "        labels_list        = [torch.tensor(ex[\"labels\"],        dtype=torch.long) for ex in batch]\n",
    "        from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "        # 手动 pad_sequence\n",
    "        input_ids_padded      = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        attention_padded      = pad_sequence(attention_list, batch_first=True, padding_value=0)\n",
    "        labels_padded         = pad_sequence(labels_list, batch_first=True, padding_value=-100)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":      input_ids_padded,\n",
    "            \"attention_mask\": attention_padded,\n",
    "            \"labels\":         labels_padded,\n",
    "        }\n",
    "    return collate_fn\n",
    "\n",
    "class Qwen2LLMTestDataset(Dataset):\n",
    "    def __init__(self, model_name, cache_dir, samples):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, trust_remote_code=True\n",
    "        )\n",
    "        self.samples = samples\n",
    "        self.convert_data_to_features()\n",
    "\n",
    "    def convert_data_to_features(self):\n",
    "        self.data = []\n",
    "        system_prompt = \\\n",
    "        \"\"\"你是一个搜索引擎，需要判断给定的用户查询和文档片段之间的相关性。\n",
    "           请根据以下规则打分，范围 0 到 2：\\n\n",
    "           0-无关；1-一般相关；2-相关。\"\"\"\n",
    "        for sample in tqdm(self.samples, desc=\"Preparing ChatML prompts\"):\n",
    "            query = sample['query']\n",
    "            document = sample['title']\n",
    "            label = str(sample['label'])  # e.g. \"0\",\"1\",\"2\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\",    \"content\": system_prompt},\n",
    "                {\"role\": \"user\",      \"content\":\n",
    "                    f\"查询内容：{query}\\n\"\n",
    "                    f\"文档片段：{document}\\n\"\n",
    "                    \"请给出相关性评分（仅填写数字）：\"\n",
    "                },\n",
    "            ]\n",
    "            # 先拼 prompt，再 tokenize\n",
    "            prompt = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=False\n",
    "            )\n",
    "            self.data.append({\"prompt\":prompt,\"label\":label})\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def build_qwen2_test_collate_fn(tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        prompts = [sample['prompt'] for sample in batch]\n",
    "        labels = [item['label'] for item in batch]\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(devices[0])\n",
    "        return {\n",
    "            \"input_ids\":      inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"labels\":         labels\n",
    "        }\n",
    "    return collate_fn\n",
    "\n",
    "def train_llm_batch(net, batch, optimizer, device):\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Move inputs and labels to device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch.get('attention_mask', None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = net(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_llm(\n",
    "    net,\n",
    "    train_loader,\n",
    "    valid_loader=None,\n",
    "    num_epochs=3,\n",
    "    lr=5e-5,\n",
    "    wd=0.0,\n",
    "    num_warmup_steps=0,\n",
    "    device=None,\n",
    "    monitor=\"loss\",\n",
    "    task_name=\"llm_task\",\n",
    "    output_dir=\".\"\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_metric = float('inf')\n",
    "    best_state = None\n",
    "    best_filename = None\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        net.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            loss = train_llm_batch(net, batch, optimizer, device)\n",
    "            batch_size = batch['input_ids'].size(0)\n",
    "            epoch_loss += loss * batch_size\n",
    "            num_samples += batch_size\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss = epoch_loss / num_samples\n",
    "        print(f\"Epoch {epoch}/{num_epochs} - train loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        if valid_loader is not None:\n",
    "            net.eval()\n",
    "            val_loss = 0.0\n",
    "            val_samples = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    # Reuse train_llm_batch for forward w/o optimizer updates\n",
    "                    batch_loss = train_llm_batch(net, batch, optimizer, device)\n",
    "                    bs = batch['input_ids'].size(0)\n",
    "                    val_loss += batch_loss * bs\n",
    "                    val_samples += bs\n",
    "            val_loss /= val_samples\n",
    "            print(f\"Epoch {epoch}/{num_epochs} - valid loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_metric:\n",
    "                best_metric = val_loss\n",
    "                best_state = copy.deepcopy(net.state_dict())\n",
    "                best_filename = f\"{task_name}-epoch{epoch}-val{val_loss:.4f}.pt\"\n",
    "\n",
    "    # Save the best model if available\n",
    "    if best_state is not None and best_filename is not None:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        save_path = os.path.join(output_dir, best_filename)\n",
    "        torch.save(best_state, save_path)\n",
    "        print(f\"Best model saved to {save_path} (valid loss: {best_metric:.4f})\")\n",
    "    else:\n",
    "        print(\"Training completed. No validation improvements to save.\")\n",
    "def test_llm(net,test_iter):\n",
    "  net.eval()\n",
    "  gt = []\n",
    "  with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "      outputs = net.generate(**batch)\n",
    "      generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "      gt += batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Information Counter({1: 25126, 0: 9757, 2: 5117})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing ChatML prompts: 100%|██████████| 40000/40000 [00:20<00:00, 1974.97it/s]\n",
      "Preparing ChatML prompts: 100%|██████████| 5000/5000 [00:02<00:00, 1977.82it/s]\n",
      "Preparing ChatML prompts: 100%|██████████| 5000/5000 [00:00<00:00, 20028.04it/s]\n"
     ]
    }
   ],
   "source": [
    "train_processor = BGEProcessor(data_dir,train_data_name)\n",
    "train_examples = train_processor.get_examples()[:45000]\n",
    "test_processor = BGEProcessor(data_dir,test_data_name)\n",
    "test_examples = test_processor.get_examples()[:5000]\n",
    "train_examples, dev_examples = train_test_split(train_examples, test_size=1/9, random_state=42)\n",
    "labels = [ex[\"label\"] for ex in train_examples]\n",
    "dist = Counter(labels)\n",
    "print(\"Data Information\",dist)\n",
    "train_data = Qwen2LLMDataset(model_name, cache_dir,train_examples)\n",
    "dev_data = Qwen2LLMDataset(model_name, cache_dir,dev_examples)\n",
    "test_data = Qwen2LLMTestDataset(model_name, cache_dir,test_examples)\n",
    "\n",
    "train_iter = DataLoader(dataset=train_data,\n",
    "                        batch_size = args.batch_size,\n",
    "                        collate_fn = build_qwen2_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                        shuffle=False)\n",
    "dev_iter = DataLoader(dataset=dev_data,\n",
    "                        batch_size = args.batch_size,\n",
    "                        collate_fn = build_qwen2_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                        shuffle=False)\n",
    "test_iter = DataLoader(dataset=test_data,\n",
    "                       batch_size=args.batch_size,\n",
    "                       collate_fn = build_qwen2_test_collate_fn(\n",
    "                                        AutoTokenizer.from_pretrained(\n",
    "                                        model_name,\n",
    "                                        cache_dir = cache_dir)),\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [17:45<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - train loss: 0.3569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [17:46<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - train loss: 0.2214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [17:46<00:00,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - train loss: 0.2112\n",
      "Training completed. No validation improvements to save.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "net = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "\n",
    "train_llm(\n",
    "          net,\n",
    "          train_iter,\n",
    "          None,\n",
    "          args.num_epochs,\n",
    "          args.lr,\n",
    "          args.weight_decay,\n",
    "          args.num_warmup_steps,\n",
    "          devices[0],\n",
    "          monitor=\"loss\",\n",
    "          task_name=task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "pred = []\n",
    "gt = []\n",
    "for test_batch in test_iter:\n",
    "  inputs = {\"input_ids\": test_batch[\"input_ids\"].to(devices[0]),\n",
    "            \"attention_mask\": test_batch[\"attention_mask\"].to(devices[0])}\n",
    "  generated_ids = net.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=30,\n",
    "    temperature=1,\n",
    "    top_k= 1,\n",
    "    top_p =0.9,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "  generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs[\"input_ids\"], generated_ids)\n",
    "]\n",
    "  generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "  for text in generated_texts:\n",
    "    pred.append(text)\n",
    "  gt += test_batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "79it [00:09,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on final epoch -- acc: 67.60, f1-macro: 59.46\n",
      "test on /content/drive/MyDrive/CS5242/ckpt/SingleTowerBGE-epoch0-val_acc0.71.pt -- acc: 70.20, f1-macro: 55.78\n"
     ]
    }
   ],
   "source": [
    "test(student_net,test_iter,None,device = devices[0])\n",
    "test_on_checkpoint(student_net, args.pt_path, test_iter, None, device=devices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6. <a name='6'></a>Results\n",
    "[Back to Table of Contents](#top)\n",
    "\n",
    "We report the performance of various models on the document relevance classification task across three relevance levels: Poor, Moderately, and Highly relevant. The evaluation metrics include Precision, Recall, and F1-score for each relevance category, along with the macro-averaged results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" cellspacing=\"0\" cellpadding=\"5\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th rowspan=\"2\">Model</th>\n",
    "      <th colspan=\"3\">Label 0</th>\n",
    "      <th colspan=\"3\">Label 1</th>\n",
    "      <th colspan=\"3\">Label 2</th>\n",
    "      <th colspan=\"3\">Macro Average</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Precision</th><th>Recall</th><th>F1-score</th>\n",
    "      <th>Precision</th><th>Recall</th><th>F1-score</th>\n",
    "      <th>Precision</th><th>Recall</th><th>F1-score</th>\n",
    "      <th>Precision</th><th>Recall</th><th>F1-score</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td>MLP</td><td>0.6166</td><td>0.1290</td><td>0.2134</td><td>0.6484</td><td>0.9725</td><td>0.7780</td><td>0.2222</td><td>0.0032</td><td>0.0062</td><td>0.4957</td><td>0.3682</td><td>0.3326</td></tr>\n",
    "    <tr><td>CNN</td><td>0.8333</td><td>0.0041</td><td>0.0082</td><td>0.6322</td><td>0.9984</td><td>0.7742</td><td>0.2000</td><td>0.0016</td><td>0.0031</td><td>0.5552</td><td>0.3347</td><td>0.2618</td></tr>\n",
    "    <tr><td>Transformer</td><td>0.5637</td><td>0.3366</td><td>0.4215</td><td>0.6912</td><td>0.8665</td><td>0.7689</td><td>0.4717</td><td>0.2373</td><td>0.3158</td><td>0.5755</td><td>0.4801</td><td>0.5021</td></tr>\n",
    "    <tr><td>BGE+Transformer</td><td>0.7020</td><td>0.4326</td><td>0.5353</td><td>0.7245</td><td>0.8724</td><td>0.7916</td><td>0.4900</td><td>0.3497</td><td>0.4081</td><td>0.6388</td><td>0.5516</td><td>0.5784</td></tr>\n",
    "    <tr><td>BERT(Single)</td><td>0.7227</td><td>0.5906</td><td>0.6500</td><td>0.7780</td><td>0.8531</td><td>0.8138</td><td>0.5383</td><td>0.4668</td><td>0.5000</td><td>0.6797</td><td>0.6368</td><td>0.6546</td></tr>\n",
    "    <tr><td>BERT(Double)</td><td>0.6518</td><td>0.4582</td><td>0.5381</td><td>0.7389</td><td>0.8446</td><td>0.7882</td><td>0.4842</td><td>0.4130</td><td>0.4458</td><td>0.6249</td><td>0.5719</td><td>0.5907</td></tr>\n",
    "    <tr><td>RNN(Single)</td><td>0.5082</td><td>0.4119</td><td>0.4550</td><td>0.6938</td><td>0.8297</td><td>0.7557</td><td>0.4174</td><td>0.1598</td><td>0.2311</td><td>0.5398</td><td>0.4671</td><td>0.4806</td></tr>\n",
    "    <tr><td>RNN(Double)</td><td>0.5679</td><td>0.3598</td><td>0.4405</td><td>0.6939</td><td>0.8563</td><td>0.7666</td><td>0.4196</td><td>0.2231</td><td>0.2931</td><td>0.5605</td><td>0.4797</td><td>0.4995</td></tr>\n",
    "    <tr><td>LSTM(Single)</td><td>0.5357</td><td>0.4467</td><td>0.4871</td><td>0.7185</td><td>0.8177</td><td>0.7649</td><td>0.4887</td><td>0.3070</td><td>0.3771</td><td>0.5810</td><td>0.5238</td><td>0.5430</td></tr>\n",
    "    <tr><td>LSTM(Double)</td><td>0.5707</td><td>0.3937</td><td>0.4770</td><td>0.7075</td><td>0.8433</td><td>0.7684</td><td>0.4680</td><td>0.2896</td><td>0.3578</td><td>0.5815</td><td>0.5089</td><td>0.5307</td></tr>\n",
    "    <tr><td>GRU(Single)</td><td>0.5445</td><td>0.4351</td><td>0.4837</td><td>0.7197</td><td>0.8192</td><td>0.7662</td><td>0.4703</td><td>0.3259</td><td>0.3850</td><td>0.5782</td><td>0.5268</td><td>0.5450</td></tr>\n",
    "    <tr><td>GRU(Double)</td><td>0.5727</td><td>0.4202</td><td>0.4847</td><td>0.7158</td><td>0.8370</td><td>0.7716</td><td>0.4821</td><td>0.3196</td><td>0.3844</td><td>0.5902</td><td>0.5256</td><td>0.5459</td></tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all models, **BERT (Single)** achieved the highest overall macro-averaged F1-score (0.6546), indicating its superior capability in handling document relevance classification tasks. This strong performance can be attributed to BERT's pre-trained contextual embeddings, which enable it to capture complex semantic relationships within the text, particularly benefiting \"Moderately relevant\" and \"Highly relevant\" categories. On the other hand, one of pre-train task of BERT is **\"Next Sentence Prediction\"**, which is a binary classification task that aims to predict whether the two sentences following each other are consecutive or not. This task is similiar to the \"Relevance\" task in our dataset.\n",
    "\n",
    "On the other hand, **MLP** and **CNN** performed relatively poorly, especially on \"Highly relevant\" instances, where their F1-scores are extremely low (0.0062 and 0.0031, respectively). This could be because simple MLP and CNN architectures lack the sequential modeling ability required to understand long document structures and fine-grained semantic differences. Specifically, CNNs tend to focus on local patterns but may struggle to capture the broader contextual information necessary for relevance prediction in lengthy documents.\n",
    "\n",
    "**Transformer** and **BGE+Transformer** models achieved better results than simple RNN or CNN models, showing the advantage of self-attention mechanisms in capturing global dependencies. Notably, integrating BGE with Transformer slightly improved the overall performance, suggesting that additional guidance from a pre-trained backbone enhances the model's robustness.\n",
    "\n",
    "Recurrent models (RNN, LSTM, GRU) with double-layer architectures slightly outperformed their single-layer counterparts, reflecting the importance of deeper modeling for complex sequence understanding. However, compared to Transformer-based models, their gains were still limited, possibly due to vanishing gradient issues or less effective long-range dependency modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7. <a name='7'></a>Conclusion & Future Development\n",
    "[Back to Table of Contents](#top)\n",
    "\n",
    "In this project, we systematically explored the effectiveness of various deep learning architectures for query-document relevance prediction on the QBQTC dataset. Our experiments covered a range of models, including traditional RNN-based methods (RNN, LSTM, GRU), Transformer architectures, and BERT variants.\n",
    "\n",
    "From the evaluation results, several key conclusions can be drawn:\n",
    "\n",
    "BERT-based models demonstrated the best overall performance, with the BERT (Single) model achieving the highest Macro Average F1-score (0.6546). This highlights the advantage of using pretrained contextual embeddings and deep bidirectional representations in relevance prediction tasks.\n",
    "Transformer and BGE+Transformer models also performed competitively, outperforming traditional RNN-based models. The self-attention mechanism effectively captures long-range dependencies, which is crucial in matching queries and document titles.\n",
    "RNN, LSTM, and GRU models showed moderate performance. Although GRU and LSTM models achieved relatively higher F1-scores among the RNN variants, they still lagged behind Transformer-based methods. This could be attributed to their limited ability to model complex interactions over long sequences compared to attention-based architectures.\n",
    "Simple models like MLP and CNN performed poorly, with the CNN model achieving the lowest Macro Average F1-score (0.2618). The limited expressiveness and inability to capture intricate semantic relationships between queries and documents likely contributed to their subpar performance.\n",
    "Several possible factors influenced these outcomes:\n",
    "\n",
    "Pretrained language models like BERT bring strong generalization capabilities from large corpora, providing a significant head start over models trained from scratch.\n",
    "Attention mechanisms, as used in Transformers, allow models to selectively focus on important parts of the input, which is especially important for short but semantically rich query-document pairs.\n",
    "Traditional RNNs, despite their sequential modeling strength, struggle with long-term dependencies and are less effective at highlighting the most relevant information for retrieval tasks.\n",
    "In summary, Transformer-based and pretrained models clearly offer substantial advantages for query-document relevance prediction in web search scenarios. Future work could explore larger model variants, domain-specific pretraining, or incorporating additional signals like authority and timeliness to further enhance retrieval performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
